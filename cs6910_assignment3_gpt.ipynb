{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Installing dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T08:10:40.954288Z",
     "iopub.status.busy": "2022-05-04T08:10:40.953804Z",
     "iopub.status.idle": "2022-05-04T08:10:54.04081Z",
     "shell.execute_reply": "2022-05-04T08:10:54.03998Z",
     "shell.execute_reply.started": "2022-05-04T08:10:40.954208Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T08:49:47.297874Z",
     "iopub.status.busy": "2022-05-04T08:49:47.297184Z",
     "iopub.status.idle": "2022-05-04T08:50:08.973309Z",
     "shell.execute_reply": "2022-05-04T08:50:08.972475Z",
     "shell.execute_reply.started": "2022-05-04T08:49:47.297821Z"
    }
   },
   "outputs": [],
   "source": [
    "! pip install transformers\n",
    "! pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Necessary Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T08:10:54.044561Z",
     "iopub.status.busy": "2022-05-04T08:10:54.04435Z",
     "iopub.status.idle": "2022-05-04T08:10:54.052805Z",
     "shell.execute_reply": "2022-05-04T08:10:54.052111Z",
     "shell.execute_reply.started": "2022-05-04T08:10:54.044534Z"
    }
   },
   "outputs": [],
   "source": [
    "import wget\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T08:10:54.055559Z",
     "iopub.status.busy": "2022-05-04T08:10:54.055364Z",
     "iopub.status.idle": "2022-05-04T08:10:54.062166Z",
     "shell.execute_reply": "2022-05-04T08:10:54.061512Z",
     "shell.execute_reply.started": "2022-05-04T08:10:54.055536Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T08:14:19.906551Z",
     "iopub.status.busy": "2022-05-04T08:14:19.906282Z",
     "iopub.status.idle": "2022-05-04T08:14:19.914188Z",
     "shell.execute_reply": "2022-05-04T08:14:19.913494Z",
     "shell.execute_reply.started": "2022-05-04T08:14:19.906523Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_data(filename,foldername,url):\n",
    "    #method to download and extract data from a url and place it in the folder all in the current working directory\n",
    "    '''\n",
    "    Arguments:\n",
    "    filename -- (string) -- filename of downloaded zip file\n",
    "    url --(string) -- download url from where the zipped file is fetched\n",
    "    foldername --(string) -- the files from the file are extracted in this folder\n",
    "    '''\n",
    "    if not os.path.exists(filename) and not os.path.exists(foldername):\n",
    "        filename_zip = wget.download(url,filename)\n",
    "        with ZipFile(filename_zip, 'r') as z:\n",
    "            z.printdir()\n",
    "            print('\\nExtracting files ....')\n",
    "            z.extractall(path=foldername)\n",
    "            print('Done')\n",
    "        os.remove(filename_zip)\n",
    "    elif not os.path.exists(foldername):\n",
    "        with ZipFile(filename, 'r') as z:\n",
    "            z.printdir()\n",
    "            print('\\nExtracting files ....')\n",
    "            z.extractall(path=foldername)\n",
    "            print('Done')\n",
    "        os.remove(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Datasets can be used for fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T08:15:03.021878Z",
     "iopub.status.busy": "2022-05-04T08:15:03.02121Z",
     "iopub.status.idle": "2022-05-04T08:15:03.026667Z",
     "shell.execute_reply": "2022-05-04T08:15:03.025757Z",
     "shell.execute_reply.started": "2022-05-04T08:15:03.021825Z"
    }
   },
   "outputs": [],
   "source": [
    "filename1 = 'datasets_small.zip'\n",
    "filename2 = 'datasets_big.zip'\n",
    "foldername1 = 'datasets_small'\n",
    "foldername2 = 'datasets_big'\n",
    "url1 = 'https://storage.googleapis.com/kaggle-data-sets/6776/81739/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20220503%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20220503T083043Z&X-Goog-Expires=259199&X-Goog-SignedHeaders=host&X-Goog-Signature=55f4aa710ff542d586888c9c86c3fe7fbc30b324eed1971fd3dfb7b44c0da033d64d8db594d368c814bde54174cea54e6e64c0ef8110453f56086bfff28b3e43622c1947e4e44fe2dd87fd421f43c844047c85e7ff5aaf47b77b9505501fe16769ebab99b085f04f7cc9622c6e0f3d3a995a9f10f407279fea01f8fe41b45c492666018074f4b1b9c20877aac198c0a8051193c2f493604591a5dbdb65971cd809f94c9305f405a1c31f291713b78051124ac88b544323c5c05c281dc8d7b3f55ee895b11c0130c1956afd2bec6832ad182f55210a9f282de30788e0d4673a314a703f7d713180c01df763729532c5b933c3381ccf6eb99f914018d0601b3851'\n",
    "url2 = 'https://storage.googleapis.com/kaggle-data-sets/118366/3314065/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20220503%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20220503T131835Z&X-Goog-Expires=259199&X-Goog-SignedHeaders=host&X-Goog-Signature=24b3ec919def9e90873f70af803df3847f3c2f4883fe93922440001c7f82613b974c994c942b94e40cadd69c3d807579662b970217cde17a3c35b29061ad5e53f975383fe5c6f1f7b6302245f05b6d94ac1e93a1e7d3e8b3f1cd5f851d98c50c77bb394fc863a3b937276eee68a0a69b34bb637238528b1d5cab8e2b27a3e64bee9d4a9e3882ed1f0d3ca3c7a0b97eef12849e408dee695220d13be4302f87081aebe6a2071c6541f769a204d5c07b68f7aeae84cfaea9215879b21fa4faf4d5ea4bdceba08eb23b644781278527429531c72d41ee82fa8e6bb946614a1482c6f0432369fe4946b4ad1b2936fa59c71d653e037d1ad52512baa26367a31d2cdd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T08:26:08.319577Z",
     "iopub.status.busy": "2022-05-04T08:26:08.319299Z",
     "iopub.status.idle": "2022-05-04T08:26:10.201187Z",
     "shell.execute_reply": "2022-05-04T08:26:10.200498Z",
     "shell.execute_reply.started": "2022-05-04T08:26:08.319543Z"
    }
   },
   "outputs": [],
   "source": [
    "read_data(filename1,foldername1,url1)\n",
    "read_data(filename2,foldername2,url2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T08:29:03.577823Z",
     "iopub.status.busy": "2022-05-04T08:29:03.577544Z",
     "iopub.status.idle": "2022-05-04T08:29:03.581628Z",
     "shell.execute_reply": "2022-05-04T08:29:03.580632Z",
     "shell.execute_reply.started": "2022-05-04T08:29:03.577796Z"
    }
   },
   "outputs": [],
   "source": [
    "lyrics_file = \"lyrics-data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T08:27:11.005109Z",
     "iopub.status.busy": "2022-05-04T08:27:11.004397Z",
     "iopub.status.idle": "2022-05-04T08:27:11.027078Z",
     "shell.execute_reply": "2022-05-04T08:27:11.026421Z",
     "shell.execute_reply.started": "2022-05-04T08:27:11.005059Z"
    }
   },
   "outputs": [],
   "source": [
    "#Only for Dataset 1 this preprocessing is done\n",
    "rel_data_path = foldername1\n",
    "data = []\n",
    "for root,_,files in os.walk(rel_data_path,'r'):\n",
    "    for file in files:\n",
    "        file_absolute_path = os.path.abspath(os.path.join(root,file))\n",
    "    #     print(file_absolute_path)\n",
    "        with open(file_absolute_path,'r') as f:\n",
    "#             data.append('<|title|>'+''.join([s.strip() for s in f.readlines()]))\n",
    "#             data = data + ['<|title|>'+s.strip() for s in f.readlines()]\n",
    "            data.append(f.read())\n",
    "#         os.remove(file_absolute_path)\n",
    "pd.DataFrame(data,columns=['Lyric']).to_csv(foldername1+\"/\"+lyrics_file,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T08:31:15.535872Z",
     "iopub.status.busy": "2022-05-04T08:31:15.535602Z",
     "iopub.status.idle": "2022-05-04T08:31:20.795694Z",
     "shell.execute_reply": "2022-05-04T08:31:20.794866Z",
     "shell.execute_reply.started": "2022-05-04T08:31:15.535829Z"
    }
   },
   "outputs": [],
   "source": [
    "lyrics1 = pd.read_csv(foldername1+\"/\"+lyrics_file)\n",
    "lyrics2 = pd.read_csv(foldername2+\"/\"+lyrics_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T08:48:06.139607Z",
     "iopub.status.busy": "2022-05-04T08:48:06.138917Z",
     "iopub.status.idle": "2022-05-04T08:48:06.147073Z",
     "shell.execute_reply": "2022-05-04T08:48:06.146252Z",
     "shell.execute_reply.started": "2022-05-04T08:48:06.139568Z"
    }
   },
   "outputs": [],
   "source": [
    "#To get the training data in csv format so as to pass as argument to load_dataset() function\n",
    "def to_train(lyrics,max_length=1024,truncate=False,max_rows=20000):\n",
    "    if lyrics == \"lyrics1\":\n",
    "        df = lyrics1\n",
    "        train_file_name = 'train1.csv'\n",
    "    else:\n",
    "        df = pd.DataFrame(lyrics2[(lyrics2['language']=='en')]['Lyric']) \n",
    "        train_file_name = 'train2.csv'\n",
    "        df = df[df['Lyric'].apply(lambda x: len(x.split(' ')) < 350 )]\n",
    "    data = np.array(df['Lyric'].apply(lambda x : x[:max_length] if len(x) > max_length else x))\n",
    "    if(data.shape[0]>max_rows and truncate):\n",
    "        train = data[:max_rows]\n",
    "    else:\n",
    "        train = data\n",
    "    pd.DataFrame(train,columns=['lyrics']).to_csv(train_file_name,index=False)\n",
    "    return train_file_name\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T09:17:09.965912Z",
     "iopub.status.busy": "2022-05-04T09:17:09.965436Z",
     "iopub.status.idle": "2022-05-04T09:17:13.035731Z",
     "shell.execute_reply": "2022-05-04T09:17:13.034989Z",
     "shell.execute_reply.started": "2022-05-04T09:17:09.965846Z"
    }
   },
   "outputs": [],
   "source": [
    "train_file_name = to_train(\"lyrics1\",truncate=True) #To train with first dataset (small)\n",
    "# train_file_name = to_train(\"lyrics2\",truncate=True) #To train with second dataset (big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T08:50:08.97581Z",
     "iopub.status.busy": "2022-05-04T08:50:08.975346Z",
     "iopub.status.idle": "2022-05-04T08:50:14.729481Z",
     "shell.execute_reply": "2022-05-04T08:50:14.728094Z",
     "shell.execute_reply.started": "2022-05-04T08:50:08.975768Z"
    }
   },
   "outputs": [],
   "source": [
    "#Importing Transformers\n",
    "import transformers\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T09:19:16.423678Z",
     "iopub.status.busy": "2022-05-04T09:19:16.423035Z",
     "iopub.status.idle": "2022-05-04T09:19:16.428201Z",
     "shell.execute_reply": "2022-05-04T09:19:16.427506Z",
     "shell.execute_reply.started": "2022-05-04T09:19:16.423635Z"
    }
   },
   "outputs": [],
   "source": [
    "#All necessary imports for finetuning\n",
    "from datasets import ClassLabel\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers import TFGPT2LMHeadModel\n",
    "from transformers import create_optimizer, AdamWeightDecay\n",
    "from transformers import DefaultDataCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T09:19:18.4632Z",
     "iopub.status.busy": "2022-05-04T09:19:18.462459Z",
     "iopub.status.idle": "2022-05-04T09:19:18.468176Z",
     "shell.execute_reply": "2022-05-04T09:19:18.467416Z",
     "shell.execute_reply.started": "2022-05-04T09:19:18.463148Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T09:19:30.909138Z",
     "iopub.status.busy": "2022-05-04T09:19:30.908472Z",
     "iopub.status.idle": "2022-05-04T09:19:31.962673Z",
     "shell.execute_reply": "2022-05-04T09:19:31.961817Z",
     "shell.execute_reply.started": "2022-05-04T09:19:30.909101Z"
    }
   },
   "outputs": [],
   "source": [
    "datasets = load_dataset(\"csv\", data_files={\"train\": train_file_name})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T09:19:41.818779Z",
     "iopub.status.busy": "2022-05-04T09:19:41.81803Z",
     "iopub.status.idle": "2022-05-04T09:19:41.82383Z",
     "shell.execute_reply": "2022-05-04T09:19:41.823042Z",
     "shell.execute_reply.started": "2022-05-04T09:19:41.818731Z"
    }
   },
   "outputs": [],
   "source": [
    "# datasets[\"train\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T09:19:39.670094Z",
     "iopub.status.busy": "2022-05-04T09:19:39.669186Z",
     "iopub.status.idle": "2022-05-04T09:19:39.677491Z",
     "shell.execute_reply": "2022-05-04T09:19:39.676747Z",
     "shell.execute_reply.started": "2022-05-04T09:19:39.670046Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(\n",
    "        dataset\n",
    "    ), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset) - 1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset) - 1)\n",
    "        picks.append(pick)\n",
    "\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T09:19:50.806259Z",
     "iopub.status.busy": "2022-05-04T09:19:50.805681Z",
     "iopub.status.idle": "2022-05-04T09:19:50.811682Z",
     "shell.execute_reply": "2022-05-04T09:19:50.809006Z",
     "shell.execute_reply.started": "2022-05-04T09:19:50.806218Z"
    }
   },
   "outputs": [],
   "source": [
    "# show_random_elements(datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tokenize all our texts with the same vocabulary that was used when training the model, \n",
    "we have to download a pretrained tokenizer. This is all done by the `GPT2Tokenizer` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T09:19:53.292043Z",
     "iopub.status.busy": "2022-05-04T09:19:53.291229Z",
     "iopub.status.idle": "2022-05-04T09:19:58.991747Z",
     "shell.execute_reply": "2022-05-04T09:19:58.99103Z",
     "shell.execute_reply.started": "2022-05-04T09:19:53.291991Z"
    }
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_checkpoint,from_pt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now call the tokenizer on all our texts. This is very simple, using the [`map`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map) \n",
    "method from the Datasets library. First we define a function that call the tokenizer on our texts:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T09:20:02.262923Z",
     "iopub.status.busy": "2022-05-04T09:20:02.26236Z",
     "iopub.status.idle": "2022-05-04T09:20:02.267521Z",
     "shell.execute_reply": "2022-05-04T09:20:02.266703Z",
     "shell.execute_reply.started": "2022-05-04T09:20:02.262876Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"lyrics\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we apply it to all the splits in our `datasets` object, using `batched=True` \n",
    "and 4 processes to speed up the preprocessing.\n",
    "We won't need the `text` column afterward, so we discard it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T09:20:04.111295Z",
     "iopub.status.busy": "2022-05-04T09:20:04.110734Z",
     "iopub.status.idle": "2022-05-04T09:20:09.069152Z",
     "shell.execute_reply": "2022-05-04T09:20:09.068354Z",
     "shell.execute_reply.started": "2022-05-04T09:20:04.111253Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_datasets = datasets.map(\n",
    "    tokenize_function, batched=True, num_proc=4, remove_columns=[\"lyrics\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now look at an element of our datasets, \n",
    "we will see the text have been replaced by the `input_ids` the model will need:\n",
    "Uncomment below to see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T09:20:18.958758Z",
     "iopub.status.busy": "2022-05-04T09:20:18.958109Z",
     "iopub.status.idle": "2022-05-04T09:20:18.961932Z",
     "shell.execute_reply": "2022-05-04T09:20:18.961203Z",
     "shell.execute_reply.started": "2022-05-04T09:20:18.958721Z"
    }
   },
   "outputs": [],
   "source": [
    "# tokenized_datasets[\"train\"][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the harder part: We need to concatenate all our texts together, and then split the result into chunks of a fixed size, which we will call `block_size`. To do this, we will use the `map` method again, with the option `batched=True`. When we use `batched=True`, the function we pass to `map()` will be passed multiple inputs at once, allowing us to group them into more or fewer examples than we had in the input. This allows us to create our new fixed-length samples.\n",
    "\n",
    "We can use any `block_size` up to the the maximum length our model was pretrained with, which for models in the `gpt2` family is usually something in the range 512-1024. This might be a bit too big to fit in your GPU RAM, though, so let's use something a bit smaller: 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T09:20:20.488698Z",
     "iopub.status.busy": "2022-05-04T09:20:20.488427Z",
     "iopub.status.idle": "2022-05-04T09:20:20.492481Z",
     "shell.execute_reply": "2022-05-04T09:20:20.491462Z",
     "shell.execute_reply.started": "2022-05-04T09:20:20.48867Z"
    }
   },
   "outputs": [],
   "source": [
    "# block_size = tokenizer.model_max_length\n",
    "block_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we write the preprocessing function that will group our texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T09:20:22.322887Z",
     "iopub.status.busy": "2022-05-04T09:20:22.322339Z",
     "iopub.status.idle": "2022-05-04T09:20:22.329447Z",
     "shell.execute_reply": "2022-05-04T09:20:22.328421Z",
     "shell.execute_reply.started": "2022-05-04T09:20:22.32283Z"
    }
   },
   "outputs": [],
   "source": [
    "# Then we write the preprocessing function that will group our texts:\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, though you could add padding instead if the model supports it\n",
    "    # In this, as in all things, we advise you to follow your heart\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we duplicate the inputs for our labels, without shifting them, even though we told you the labels need to be shifted! This is because CausalLM models in the Transformers library automatically apply right-shifting to the inputs, so we don't need to do it manually.\n",
    "\n",
    "Also note that by default, the `map` method will send a batch of 1,000 examples to be treated by the preprocessing function. So here, we will drop the remainder to make the concatenated tokenized texts a multiple of `block_size` every 1,000 examples. You can adjust this behavior by passing a higher batch size (which will also be processed slower). You can also speed-up the preprocessing by using multiprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T09:20:24.231775Z",
     "iopub.status.busy": "2022-05-04T09:20:24.231303Z",
     "iopub.status.idle": "2022-05-04T09:20:24.294005Z",
     "shell.execute_reply": "2022-05-04T09:20:24.293303Z",
     "shell.execute_reply.started": "2022-05-04T09:20:24.231727Z"
    }
   },
   "outputs": [],
   "source": [
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can check our datasets have changed: now the samples contain chunks of `block_size` contiguous tokens, \n",
    "potentially spanning several of our original texts.\n",
    "Uncomment below to see the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T09:20:26.827937Z",
     "iopub.status.busy": "2022-05-04T09:20:26.82712Z",
     "iopub.status.idle": "2022-05-04T09:20:26.831772Z",
     "shell.execute_reply": "2022-05-04T09:20:26.831076Z",
     "shell.execute_reply.started": "2022-05-04T09:20:26.827894Z"
    }
   },
   "outputs": [],
   "source": [
    "# tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data has been cleaned, we're ready to initialize our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T09:20:28.279187Z",
     "iopub.status.busy": "2022-05-04T09:20:28.278921Z",
     "iopub.status.idle": "2022-05-04T09:20:33.821903Z",
     "shell.execute_reply": "2022-05-04T09:20:33.821175Z",
     "shell.execute_reply.started": "2022-05-04T09:20:28.279159Z"
    }
   },
   "outputs": [],
   "source": [
    "model_without_finetune = TFGPT2LMHeadModel.from_pretrained(model_checkpoint)\n",
    "model = TFGPT2LMHeadModel.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've done that, it's time for our optimizer! \n",
    "We can initialize our `AdamWeightDecay` optimizer directly, \n",
    "or we can use the `create_optimizer` function to generate an \n",
    "`AdamWeightDecay` optimizer with a learning rate schedule. \n",
    "In this case, we'll just stick with a constant learning rate for simplicity, so let's just use `AdamWeightDecay`.\n",
    "This is quite different from the standard Keras way of handling losses, where labels are passed separately and not visible to the main body of the model, and loss is handled by a function that the user passes to `compile()`, which uses the model outputs and the label to compute a loss value.\n",
    "\n",
    "The approach we take is that if the user does not pass a loss to `compile()`, the model will assume you want the **internal** loss. If you are doing this, you should make sure that the labels column(s) are included in the **input dict** or in the `columns` argument to `to_tf_dataset`.\n",
    "\n",
    "If you want to use your own loss, that is of course possible too! If you do this, you should make sure your labels column(s) are passed like normal labels, either as the **second argument** to `model.fit()`, or in the `label_cols` argument to `to_tf_dataset`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T09:20:36.733445Z",
     "iopub.status.busy": "2022-05-04T09:20:36.733193Z",
     "iopub.status.idle": "2022-05-04T09:20:36.745688Z",
     "shell.execute_reply": "2022-05-04T09:20:36.744889Z",
     "shell.execute_reply.started": "2022-05-04T09:20:36.733416Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below method is used for generating texts from the prompt based on random sampling methods refer [`here`](https://huggingface.co/blog/how-to-generate) for more such methods of generating text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T09:20:40.124764Z",
     "iopub.status.busy": "2022-05-04T09:20:40.124493Z",
     "iopub.status.idle": "2022-05-04T09:20:40.13379Z",
     "shell.execute_reply": "2022-05-04T09:20:40.133103Z",
     "shell.execute_reply.started": "2022-05-04T09:20:40.124733Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate(model,tokenizer,prompt,return_tensor_type='tf'):\n",
    "    #Method to generate the possible sequences according to the following configurations based on the prompt passed \n",
    "    #as input to the function \n",
    "    input_ids = tokenizer.encode(prompt,return_tensors=return_tensor_type)\n",
    "    if return_tensor_type == 'tf':\n",
    "        generated_text_samples = model.generate(\n",
    "            input_ids, \n",
    "            max_length=500, \n",
    "            num_return_sequences=5,\n",
    "            no_repeat_ngram_size=2,\n",
    "            repetition_penalty=1.5,\n",
    "            top_p=0.1,\n",
    "            temperature=.90,\n",
    "            do_sample=True,\n",
    "            top_k=500,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    else:\n",
    "        generated_text_samples = model.to('cpu').generate(\n",
    "            input_ids, \n",
    "            max_length=500, \n",
    "            num_return_sequences=5,\n",
    "            no_repeat_ngram_size=2,\n",
    "            repetition_penalty=1.5,\n",
    "            top_p=0.1,\n",
    "            temperature=.90,\n",
    "            do_sample=True,\n",
    "            top_k=500,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    print(\"Generated lyrics : \")\n",
    "    #Print output for each sequence generated above\n",
    "    for i,sample in enumerate(generated_text_samples):\n",
    "        print(\"{}\".format(tokenizer.decode(sample, skip_special_tokens=True)))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we convert our datasets to `tf.data.Dataset`, \n",
    "which Keras understands natively. \n",
    "`Dataset` objects have a built-in method for this. \n",
    "Because all our inputs are the same length, no padding is required, \n",
    "so we can use the DefaultDataCollator. \n",
    "Note that our data collators are designed to work for multiple frameworks, \n",
    "so ensure you set the `return_tensors='tf'` argument to get Tensorflow tensors out - you don't want to accidentally get a load of `torch.Tensor` objects in the middle of your nice TF code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T09:20:44.0538Z",
     "iopub.status.busy": "2022-05-04T09:20:44.053147Z",
     "iopub.status.idle": "2022-05-04T09:20:44.099704Z",
     "shell.execute_reply": "2022-05-04T09:20:44.098753Z",
     "shell.execute_reply.started": "2022-05-04T09:20:44.053756Z"
    }
   },
   "outputs": [],
   "source": [
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")\n",
    "train_set = lm_datasets[\"train\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T09:21:14.661882Z",
     "iopub.status.busy": "2022-05-04T09:21:14.661609Z"
    }
   },
   "outputs": [],
   "source": [
    "history=model.fit(train_set, epochs=5) #Take long time if using dataset_big to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(model,tokenizer,'I love Deep Learning') #Generate output of finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(model_without_finetune,tokenizer,'I love Deep Learning') #Generate output of pretrained model without finetuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
