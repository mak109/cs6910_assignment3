{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Installing dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T03:04:55.017887Z",
     "iopub.status.busy": "2022-05-03T03:04:55.017342Z",
     "iopub.status.idle": "2022-05-03T03:05:07.055944Z",
     "shell.execute_reply": "2022-05-03T03:05:07.054908Z",
     "shell.execute_reply.started": "2022-05-03T03:04:55.017844Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install wget\n",
    "!pip install --upgrade wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing necessary libraries,packages,etc.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T03:05:07.058759Z",
     "iopub.status.busy": "2022-05-03T03:05:07.058488Z",
     "iopub.status.idle": "2022-05-03T03:05:07.071515Z",
     "shell.execute_reply": "2022-05-03T03:05:07.070790Z",
     "shell.execute_reply.started": "2022-05-03T03:05:07.058723Z"
    }
   },
   "outputs": [],
   "source": [
    "import wget\n",
    "import os\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T03:05:07.074688Z",
     "iopub.status.busy": "2022-05-03T03:05:07.074141Z",
     "iopub.status.idle": "2022-05-03T03:05:07.078165Z",
     "shell.execute_reply": "2022-05-03T03:05:07.077393Z",
     "shell.execute_reply.started": "2022-05-03T03:05:07.074653Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T03:05:07.080903Z",
     "iopub.status.busy": "2022-05-03T03:05:07.080499Z",
     "iopub.status.idle": "2022-05-03T03:05:11.167520Z",
     "shell.execute_reply": "2022-05-03T03:05:11.166793Z",
     "shell.execute_reply.started": "2022-05-03T03:05:07.080853Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T03:05:11.169208Z",
     "iopub.status.busy": "2022-05-03T03:05:11.168950Z",
     "iopub.status.idle": "2022-05-03T03:05:11.845924Z",
     "shell.execute_reply": "2022-05-03T03:05:11.845205Z",
     "shell.execute_reply.started": "2022-05-03T03:05:11.169174Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import SimpleRNN,GRU,LSTM,Embedding,Input,Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T03:05:11.847599Z",
     "iopub.status.busy": "2022-05-03T03:05:11.847343Z",
     "iopub.status.idle": "2022-05-03T03:05:37.142014Z",
     "shell.execute_reply": "2022-05-03T03:05:37.141254Z",
     "shell.execute_reply.started": "2022-05-03T03:05:11.847565Z"
    }
   },
   "outputs": [],
   "source": [
    "#Dataset downloading and extracting\n",
    "filename = 'dakshina_dataset_v1.0'\n",
    "url = 'https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar'\n",
    "if not os.path.exists(filename+'.tar') and not os.path.exists(filename):\n",
    "    filename_tar = wget.download(url)\n",
    "    file = tarfile.open(filename_tar)\n",
    "    print('\\nExtracting files ....')\n",
    "    file.extractall()\n",
    "    file.close()\n",
    "    print('Done')\n",
    "    os.remove(filename_tar)\n",
    "elif not os.path.exists(filename):\n",
    "    filename_tar = filename + '.tar'\n",
    "    file = tarfile.open(filename_tar)\n",
    "    print('\\nExtracting files ....')\n",
    "    file.extractall()\n",
    "    file.close()\n",
    "    print('Done')\n",
    "    os.remove(filename_tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T03:05:37.143897Z",
     "iopub.status.busy": "2022-05-03T03:05:37.143646Z",
     "iopub.status.idle": "2022-05-03T03:05:37.150091Z",
     "shell.execute_reply": "2022-05-03T03:05:37.148248Z",
     "shell.execute_reply.started": "2022-05-03T03:05:37.143864Z"
    }
   },
   "outputs": [],
   "source": [
    "#Paths\n",
    "lang = 'bn'\n",
    "train_path =  filename+f\"/{lang}/lexicons/{lang}.translit.sampled.train.tsv\"\n",
    "val_path = filename+f\"/{lang}/lexicons/{lang}.translit.sampled.dev.tsv\"\n",
    "test_path = filename+f\"/{lang}/lexicons/{lang}.translit.sampled.test.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T03:05:37.151412Z",
     "iopub.status.busy": "2022-05-03T03:05:37.151162Z",
     "iopub.status.idle": "2022-05-03T03:05:38.201869Z",
     "shell.execute_reply": "2022-05-03T03:05:38.200980Z",
     "shell.execute_reply.started": "2022-05-03T03:05:37.151381Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    df = pd.read_csv(path,header=None,sep='\\t')\n",
    "    df.fillna(\"NaN\",inplace=True)\n",
    "    input_texts,target_texts = df[1].to_list(),df[0].to_list()\n",
    "    return input_texts,target_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T03:05:38.204273Z",
     "iopub.status.busy": "2022-05-03T03:05:38.203646Z",
     "iopub.status.idle": "2022-05-03T03:05:39.360074Z",
     "shell.execute_reply": "2022-05-03T03:05:39.355036Z",
     "shell.execute_reply.started": "2022-05-03T03:05:38.204232Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_text(texts):\n",
    "    characters = set()\n",
    "    for text in texts:\n",
    "        for c in text:\n",
    "            if c not in characters:\n",
    "                characters.add(c)\n",
    "    characters.add(' ')\n",
    "    return sorted(list(characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T03:05:39.364045Z",
     "iopub.status.busy": "2022-05-03T03:05:39.363811Z",
     "iopub.status.idle": "2022-05-03T03:05:40.615370Z",
     "shell.execute_reply": "2022-05-03T03:05:40.614575Z",
     "shell.execute_reply.started": "2022-05-03T03:05:39.364017Z"
    }
   },
   "outputs": [],
   "source": [
    "def start_end_pad(texts):\n",
    "    for i in range(len(texts)):\n",
    "        texts[i] = \"\\t\" + texts[i] + \"\\n\"\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T03:05:40.616762Z",
     "iopub.status.busy": "2022-05-03T03:05:40.616488Z",
     "iopub.status.idle": "2022-05-03T03:05:42.488566Z",
     "shell.execute_reply": "2022-05-03T03:05:42.487842Z",
     "shell.execute_reply.started": "2022-05-03T03:05:40.616726Z"
    }
   },
   "outputs": [],
   "source": [
    "#Train test val dataset import raw\n",
    "train_input_texts,train_target_texts = read_data(train_path)\n",
    "val_input_texts,val_target_texts = read_data(val_path)\n",
    "test_input_texts,test_target_texts = read_data(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T03:05:42.491257Z",
     "iopub.status.busy": "2022-05-03T03:05:42.490844Z",
     "iopub.status.idle": "2022-05-03T03:05:43.890947Z",
     "shell.execute_reply": "2022-05-03T03:05:43.890112Z",
     "shell.execute_reply.started": "2022-05-03T03:05:42.491220Z"
    }
   },
   "outputs": [],
   "source": [
    "#Padding at beginning and end with '\\t' and '\\n' respectively\n",
    "train_target_texts = start_end_pad(train_target_texts)\n",
    "val_target_texts = start_end_pad(val_target_texts)\n",
    "test_target_texts = start_end_pad(test_target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T03:05:43.893676Z",
     "iopub.status.busy": "2022-05-03T03:05:43.893181Z",
     "iopub.status.idle": "2022-05-03T03:05:43.904123Z",
     "shell.execute_reply": "2022-05-03T03:05:43.903264Z",
     "shell.execute_reply.started": "2022-05-03T03:05:43.893640Z"
    }
   },
   "outputs": [],
   "source": [
    "#Default Configuration for training\n",
    "config_ = {\n",
    "    \"learning_rate\": 1e-3,                                      # Learning rate in gradient descent\n",
    "    \"epochs\": 10,                                               # Number of epochs to train the model   \n",
    "    \"optimizer\": 'adam',                                        # Gradient descent algorithm used for the parameter updation\n",
    "    \"batch_size\": 64,                                           # Batch size used for the optimizer\n",
    "    \"loss_function\": 'categorical_crossentropy',                # Loss function used in the optimizer                                                                      # Name of dataset\n",
    "    \"input_embedding_size\": 256,                                        # Size of input embedding layer\n",
    "    \"num_enc_layers\": 3,                                         # Number of layers in the encoder\n",
    "    \"num_dec_layers\": 3,                                         # Number of layers in the decoder\n",
    "    \"hidden_layer_size\": 256,                                      # Size of hidden layer\n",
    "    \"dropout\" : 0.30,                                            #Value of dropout used in recurrent dropout\n",
    "    'r_dropout':0.30,                                           # Value of dropout used in recurrent dropout\n",
    "    \"cell_type\": 'GRU',                                         # Type of cell used in the encoder and decoder ('RNN' or 'GRU' or 'LSTM')\n",
    "    \"beam_width\": 1                                          # Beam width used in beam decoder                                        \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T03:05:43.905861Z",
     "iopub.status.busy": "2022-05-03T03:05:43.905537Z",
     "iopub.status.idle": "2022-05-03T03:05:43.916745Z",
     "shell.execute_reply": "2022-05-03T03:05:43.916041Z",
     "shell.execute_reply.started": "2022-05-03T03:05:43.905809Z"
    }
   },
   "outputs": [],
   "source": [
    "def enc_dec_tokens(train_input_texts,train_target_texts,val_input_texts,val_target_texts):\n",
    "    #Returns encoding of characters as integer in two dictionary for input and target characters\n",
    "    #Returns number of tokens in input and output\n",
    "    #Returns the maximum sequence length from input and target texts\n",
    "    input_characters = parse_text(train_input_texts + val_input_texts)\n",
    "    target_characters = parse_text(train_target_texts + val_target_texts)\n",
    "    num_encoder_tokens = len(input_characters)\n",
    "    num_decoder_tokens = len(target_characters)\n",
    "    max_encoder_seq_length = max([len(txt) for txt in train_input_texts + val_input_texts])\n",
    "    max_decoder_seq_length = max([len(txt) for txt in train_target_texts + val_target_texts])\n",
    "\n",
    "    print(\"Number of training samples:\", len(train_input_texts))\n",
    "    print(\"Number of validation samples:\", len(val_input_texts))\n",
    "    print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
    "    print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
    "    print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
    "    print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
    "    \n",
    "    input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "    target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
    "    \n",
    "    return input_token_index,target_token_index,max_encoder_seq_length,max_decoder_seq_length,num_encoder_tokens,num_decoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T03:05:43.919524Z",
     "iopub.status.busy": "2022-05-03T03:05:43.918990Z",
     "iopub.status.idle": "2022-05-03T03:05:43.929432Z",
     "shell.execute_reply": "2022-05-03T03:05:43.928679Z",
     "shell.execute_reply.started": "2022-05-03T03:05:43.919485Z"
    }
   },
   "outputs": [],
   "source": [
    "#Data Preprocessing\n",
    "def data_processing(input_texts,enc_length,input_token_index,num_encoder_tokens, target_texts,dec_length,target_token_index,num_decoder_tokens):\n",
    "        # Returns the input and target data in a form needed by the Keras embedding layer (i.e) \n",
    "        # decoder_input & encoder_input -- (None, timesteps) where each character is encoded by an integer\n",
    "        # decoder_output -- (None, timesteps, vocabulary size) where the last dimension is the one-hot encoding\n",
    "\n",
    "        #  ' ' -- space (equivalent to no meaningful input / blank input)\n",
    "    encoder_input_data = np.zeros(\n",
    "        (len(input_texts), enc_length), dtype=\"float32\"\n",
    "    )\n",
    "    decoder_input_data = np.zeros(\n",
    "            (len(input_texts), dec_length), dtype=\"float32\"\n",
    "        )\n",
    "    decoder_target_data = np.zeros(\n",
    "            (len(input_texts), dec_length, num_decoder_tokens), dtype=\"float32\"\n",
    "        )\n",
    "\n",
    "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "        \n",
    "        for t, char in enumerate(input_text):\n",
    "            encoder_input_data[i, t] = input_token_index[char]\n",
    "        encoder_input_data[i, t + 1 :] = input_token_index[' ']\n",
    "        \n",
    "        for t, char in enumerate(target_text):\n",
    "                # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "            decoder_input_data[i, t] = target_token_index[char]\n",
    "            if t > 0:\n",
    "                    # decoder_target_data will be ahead by one timestep\n",
    "                    # and will not include the start character.\n",
    "                decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
    "        decoder_input_data[i, t + 1 :] = target_token_index[' ']\n",
    "        decoder_target_data[i, t:, target_token_index[' ']] = 1.0\n",
    "    return encoder_input_data,decoder_input_data,decoder_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T03:05:43.931149Z",
     "iopub.status.busy": "2022-05-03T03:05:43.930866Z",
     "iopub.status.idle": "2022-05-03T03:05:44.086424Z",
     "shell.execute_reply": "2022-05-03T03:05:44.085715Z",
     "shell.execute_reply.started": "2022-05-03T03:05:43.931116Z"
    }
   },
   "outputs": [],
   "source": [
    "input_token_index,target_token_index,max_encoder_seq_length,max_decoder_seq_length,num_encoder_tokens,num_decoder_tokens = enc_dec_tokens(train_input_texts,train_target_texts,val_input_texts,val_target_texts)\n",
    "#Dictionary for reverse lookup of character for its integer encode \n",
    "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T03:05:44.087743Z",
     "iopub.status.busy": "2022-05-03T03:05:44.087511Z",
     "iopub.status.idle": "2022-05-03T03:05:45.884425Z",
     "shell.execute_reply": "2022-05-03T03:05:45.883550Z",
     "shell.execute_reply.started": "2022-05-03T03:05:44.087707Z"
    }
   },
   "outputs": [],
   "source": [
    "#Preprocessed inputs this will be used in training validation and testing in future\n",
    "train_encoder_input,train_decoder_input,train_decoder_target = data_processing(train_input_texts,max_encoder_seq_length,input_token_index,num_encoder_tokens, train_target_texts,max_decoder_seq_length,target_token_index,num_decoder_tokens)\n",
    "val_encoder_input,val_decoder_input,val_decoder_target = data_processing(val_input_texts,max_encoder_seq_length,input_token_index,num_encoder_tokens, val_target_texts,max_decoder_seq_length,target_token_index,num_decoder_tokens)\n",
    "test_encoder_input,test_decoder_input,test_decoder_target = data_processing(test_input_texts,max_encoder_seq_length,input_token_index,num_encoder_tokens, test_target_texts,max_decoder_seq_length,target_token_index,num_decoder_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T03:05:45.886175Z",
     "iopub.status.busy": "2022-05-03T03:05:45.885896Z",
     "iopub.status.idle": "2022-05-03T03:05:45.899322Z",
     "shell.execute_reply": "2022-05-03T03:05:45.898647Z",
     "shell.execute_reply.started": "2022-05-03T03:05:45.886134Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_model(num_encoder_tokens,num_decoder_tokens,input_embedding_size=32,num_enc_layers=1,num_dec_layers=1,hidden_layer_size=64,cell_type='LSTM',dropout=0,r_dropout=0,cell_activation='tanh'):\n",
    "    '''\n",
    "    Function to create a seq2seq model without attention.\n",
    "    Arguments :\n",
    "        num_encoder_tokens -- (int) number of characters in input vocabulary\n",
    "        num_decoder_tokens -- (int) number of characters in output vocabulary\n",
    "        input_embedding_size -- (int, default : 64) size of input embedding layer for encoder and decoder\n",
    "        num_enc_layers -- (int, default : 1) number of layers of cell to stack in encoder\n",
    "        num_dec_layers -- (int, default : 1) number of layers of cell to stack in decoder\n",
    "        hidden_layer_size -- (int, default : 64) size of hidden layer of the encoder and decoder cells\n",
    "        cell_type -- (string, default : 'LSTM') type of cell used in encoder and decoder (possible values : 'LSTM', 'GRU', 'RNN')\n",
    "        dropout -- (float, default : 0.0) value of normal dropout (between 0 and 1)\n",
    "        r_dropout -- (float, default : 0.0) value of recurrent dropout (between 0 and 1)\n",
    "        cell_activation -- (string, default : 'tanh') type of activation used in the cell (as required by Keras)\n",
    "    Returns :\n",
    "        model -- (Keras model object) resulting attention model\n",
    "    '''\n",
    "    cell = {\n",
    "        'RNN':SimpleRNN,\n",
    "        'LSTM':LSTM,\n",
    "        'GRU':GRU\n",
    "    }\n",
    "     # Encoder input and embedding\n",
    "    encoder_input = Input(shape=(None,),name='input_1')\n",
    "    encoder_input_embedding = Embedding(num_encoder_tokens,input_embedding_size,name='embedding_1')(encoder_input)\n",
    "    \n",
    "    encoder_sequences, *encoder_state = cell[cell_type](hidden_layer_size,activation=cell_activation,return_sequences=True,return_state=True,dropout=dropout,recurrent_dropout=r_dropout,name=\"encoder_1\")(encoder_input_embedding)\n",
    "    # Encoder cell layers\n",
    "    for i in range(1,num_enc_layers):\n",
    "        encoder_sequences, *encoder_state = cell[cell_type](hidden_layer_size,activation=cell_activation,return_sequences=True,return_state=True,dropout=dropout,recurrent_dropout=r_dropout,name=f\"encoder_{i+1}\")(encoder_sequences)\n",
    "    # Decoder input and embedding    \n",
    "    decoder_input = Input(shape=(None,),name='input_2')\n",
    "    decoder_input_embedding = Embedding(num_decoder_tokens,input_embedding_size,name='embedding_2')(decoder_input)\n",
    "    \n",
    "    # Decoder cell layers\n",
    "    decoder_sequences, *decoder_state = cell[cell_type](hidden_layer_size,activation=cell_activation,return_sequences=True,return_state=True,dropout=dropout,recurrent_dropout=r_dropout,name=\"decoder_1\")(decoder_input_embedding ,initial_state=encoder_state)\n",
    "    for i in range(1,num_dec_layers):\n",
    "        decoder_sequences, *decoder_state = cell[cell_type](hidden_layer_size,activation=cell_activation,return_sequences=True,return_state=True,dropout=dropout,recurrent_dropout=r_dropout,name=f\"decoder_{i+1}\")(decoder_sequences ,initial_state=encoder_state)\n",
    "    # Time distributed Softmax FC layer\n",
    "    decoder_dense = Dense(num_decoder_tokens,activation=\"softmax\",name=\"dense_1\")(decoder_sequences)\n",
    "    \n",
    "    # Define the model that will turn encoder_input_data and decoder_input_data into decoder_target_data\n",
    "    model = keras.Model([encoder_input,decoder_input],decoder_dense)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T03:05:58.891755Z",
     "iopub.status.busy": "2022-05-03T03:05:58.891243Z",
     "iopub.status.idle": "2022-05-03T03:05:58.925399Z",
     "shell.execute_reply": "2022-05-03T03:05:58.924777Z",
     "shell.execute_reply.started": "2022-05-03T03:05:58.891719Z"
    }
   },
   "outputs": [],
   "source": [
    "def beam_decoder_util(model,input_sequences,max_decoder_seq_length,B=1,target_sequences=None,start_char=0,batch_size=64):\n",
    "    '''\n",
    "    Function to do inference on the model using beam decoder.\n",
    "    Arguments :\n",
    "        model -- (Keras model object) training model\n",
    "        input_sequences -- (numpy ndarray of size : (None, timesteps)) input to encoder\n",
    "        max_decoder_seq_length -- (int) Number of timesteps to infer in decoder\n",
    "        B -- (int, default : 1) beam width of beam decoder\n",
    "        target_sequences -- (numpy ndarray of size : (None, timesteps, num_decoder_tokens), deault : None) expected target.\n",
    "                       If None, cross entropy errors won't be calculated.\n",
    "        start_char -- (int, default : 0) Encoding integer for ' '(start char)\n",
    "        batch_size -- (int, default : 64) batch_size sent to Keras predict\n",
    "    Returns :\n",
    "        final_outputs -- (numpy ndarray of size : (None, B, timesteps)) top B output sequences\n",
    "        final_errors -- (numpy ndarray of size : (None, B)) cross entropy errors for top B output (All zeros if target_seqs == None)\n",
    "        states_values -- (numpy ndarray of size : (, None, timesteps, hidden_layer_size))  hidden states of decoder\n",
    "    '''\n",
    "    # Generating output from encoder\n",
    "    encoder_model,decoder_model,num_enc_layers,num_dec_layers=make_inference_model(model)\n",
    "    encoder_output = encoder_model.predict(input_sequences,batch_size=batch_size)\n",
    "    encoder_output = encoder_output if type(encoder_output) is list else [encoder_output]\n",
    "    # Number of input samples in the data passed\n",
    "    num_samples = input_sequences.shape[0]\n",
    "    # Top B output sequences for each input \n",
    "    outputs_fn = np.zeros((num_samples,B,max_decoder_seq_length),dtype=np.int32)\n",
    "    # Errors for top B output sequences for each input\n",
    "    errors_fn = np.zeros((num_samples,B))\n",
    "    \n",
    "     # decoder input sequence for 1 timestep (for all samples). Initially one choice only there\n",
    "    decoder_b_inputs = np.zeros((num_samples,1,1))\n",
    "    # Populate the input sequence with the start character at the 1st timestep\n",
    "    decoder_b_inputs[:, :, 0] = start_char\n",
    "    \n",
    "    # (log(probability) sequence, decoder output sequence) pairs for all choices and all samples. Probability starts with log(1) = 0\n",
    "    decoder_b_out = [[(0, [])] for t in range(num_samples)]\n",
    "    # Categorical cross entropy error in the sequence for all choice and all samples\n",
    "    errors = [[0] for t in range(num_samples)]\n",
    "    # Output states from decoder for all choices, and all samples\n",
    "    states = [encoder_output*num_dec_layers]\n",
    "    \n",
    "    # Sampling loop\n",
    "    for idx in range(max_decoder_seq_length):\n",
    "        # Storing respective data for all possibilities\n",
    "        all_b_beams = [[] for t in range(num_samples)]\n",
    "        all_decoder_states = [[] for t in range(num_samples)]\n",
    "        all_errors = [[] for t in range(num_samples)]\n",
    "        for b in range(len(decoder_b_out[0])):\n",
    "            decoder_output, *decoder_states = decoder_model.predict([decoder_b_inputs[:,b]] + states[b],batch_size=batch_size)\n",
    "            # Top B scores\n",
    "            top_b = np.argsort(decoder_output[:,-1,:],axis=-1)[:,-B:]\n",
    "            for n in range(num_samples):\n",
    "                all_b_beams[n]+= [(decoder_b_out[n][b][0] + np.log(decoder_output[n, -1, top_b[n][i]]),decoder_b_out[n][b][1] + [top_b[n][i]]) for i in range(B)]\n",
    "                if target_sequences is not None:\n",
    "                    all_errors[n] += [errors[n][b] - np.log(decoder_output[n,-1,target_sequences[n,idx]])]*B\n",
    "                all_decoder_states[n] += [[decoder_state[n:n+1] for decoder_state in decoder_states]] * B\n",
    "        # Sort and choose top B with max probabilities\n",
    "        sorted_index = list(range(len(all_b_beams[0])))\n",
    "        sorted_index = [sorted(sorted_index,key = lambda ix: all_b_beams[n][ix][0])[-B:][::-1] for n in range(num_samples)]\n",
    "        # Choose the top B decoder output sequences till now\n",
    "        decoder_b_out = [[all_b_beams[n][index] for index in sorted_index[n]] for n in range(num_samples)]\n",
    "        \n",
    "        # Update the input sequence for next 1 timestep\n",
    "        decoder_b_inputs = np.array([[all_b_beams[n][index][1][-1] for index in sorted_index[n]] for n in range(num_samples)])\n",
    "        \n",
    "         # Update states\n",
    "        states = [all_decoder_states[0][index] for index in sorted_index[0]]\n",
    "        \n",
    "        for n in range(1,num_samples):\n",
    "            states = [[np.concatenate((states[i][j],all_decoder_states[n][index][j])) for j in range(len(all_decoder_states[n][index]))] for i,index in  enumerate(sorted_index[n])]\n",
    "            \n",
    "        if target_sequences is not None:\n",
    "            errors = [[all_errors[n][index] for index in sorted_index[n]] for n in range(num_samples)]\n",
    "            \n",
    "    outputs_fn = np.array([[decoder_b_out[n][i][1] for i in range(B)] for n in range(num_samples)])\n",
    "    # Update errors\n",
    "    if target_sequences is not None:\n",
    "        errors_fn = np.array(errors)/max_decoder_seq_length\n",
    "    return outputs_fn,errors_fn,np.array(states)\n",
    "\n",
    "def calc_metrics(b_outputs, target_sequences,token_index,reverse_char_index,b_errors=None,exact_word=True,display=False):\n",
    "        # Calculates the accuracy (and mean error if info provided) for the best of B possible output sequences\n",
    "    # target_sequencess -- Expected output (encoded sequence)\n",
    "    # b_outputs -- b choices of output sequences for each sample\n",
    "    matches = np.mean(b_outputs == target_sequences.reshape(target_sequences.shape[0],1,target_sequences.shape[1]),axis=-1)\n",
    "    best_b = np.argmax(matches,axis=-1)\n",
    "    best_index = (tuple(range(best_b.shape[0])),tuple(best_b))\n",
    "    accuracy = np.mean(matches[best_index])\n",
    "    b_predictions = list()\n",
    "    loss = None\n",
    "    if b_errors is not None:\n",
    "        loss = np.mean(b_errors[best_index])\n",
    "    if exact_word:\n",
    "        equal = [0] * b_outputs.shape[0]\n",
    "        true_out = num_to_word(target_sequences,token_index,reverse_char_index)\n",
    "        for b in range(b_outputs.shape[1]):\n",
    "            pred_out = num_to_word(b_outputs[:,b], token_index,reverse_char_index)\n",
    "            equal = [equal[i] or (pred_out[i] == true_out[i]) for i in range(b_outputs.shape[0])]\n",
    "            if display==True:\n",
    "                b_predictions.append(pred_out)\n",
    "        exact_accuracy = np.mean(equal)\n",
    "        if display==True:\n",
    "            return accuracy,exact_accuracy,loss,true_out,b_predictions\n",
    "        return accuracy,exact_accuracy,loss\n",
    "    return accuracy,loss\n",
    "\n",
    "def beam_decoder(model,input_sequences,target_sequences_onehot,max_decoder_seq_length,token_index,reverse_char_index,B=1,model_batch_size=64,infer_batch_size=512,exact_word=True,return_outputs=False,return_states=False,display=False):\n",
    "    '''\n",
    "    Function to calculate/capture character-wise accuracy, exact-word-match accuracy, and loss for the seq2seq model using a beam decoder.\n",
    "    Arguments :\n",
    "        model -- (Keras model object) model used for training\n",
    "        input_sequences -- (numpy ndarray of size : (None, timesteps)) input to encoder (where characters are encoded as integers)\n",
    "        target_sequences -- (numpy ndarray of size : (None, timesteps, num_decoder_tokens)) expected target in onehot format\n",
    "        max_decoder_seq_length -- (int) Number of timesteps to infer in decoder\n",
    "        token_index -- (dict) target character encoding\n",
    "        reverse_char_index -- (dict) target character decoding\n",
    "        B -- (int, default : 1) beam width to be used in beam decoder\n",
    "        model_batch_size -- (int, default : 64) batch size to be used while evaluating model using Keras\n",
    "        infer_batch_size -- (int, default : 512) number of samples to be sent to beam_decoder_infer() at a time (to avoid RAM memory overshoot problems).\n",
    "                            We have set the default model_batch_size and infer_batch_size such that it takes the least time to run and runs without problems.\n",
    "        exact_word -- (bool, default : True) whether or not exact_accuracy has (If True, will be returned as the next argument after accuracy)\n",
    "        return_outputs -- (bool, default : True) whether or not the outputs predicted need to be returned\n",
    "        return_states -- (bool, default : True) whether or not the decoder hidden states need to be returned (for further training, another sequential model addition, etc)\n",
    "        \n",
    "    Returns :\n",
    "        accuracy -- (float) the character-wise match accuracy (as calculated by Keras fit)\n",
    "        (If exact_word is True) exact_accuracy -- (float) the exact word match accuracy\n",
    "        loss -- (float) the cross-entropy loss for the top B predictions\n",
    "        (If return_outputs is True) b_outputs -- (numpy ndarray of size : (None, B, timesteps)) top B output sequences\n",
    "        (If return_states is True) b_states -- (numpy ndarray of size : (B, None, timesteps, hidden_layer_size))  hidden states of decoder\n",
    "        \n",
    "    '''\n",
    "    target_sequences = np.argmax(target_sequences_onehot,axis=-1)\n",
    "    b_outputs,b_errors,b_states=None,None,None\n",
    "    for i in range(0,input_sequences.shape[0],infer_batch_size):\n",
    "        tmp_b_outputs,tmp_b_errors,tmp_b_states = beam_decoder_util(model,input_sequences[i:i+infer_batch_size],max_decoder_seq_length,B,target_sequences[i:i+infer_batch_size],token_index['\\t'],model_batch_size)\n",
    "        \n",
    "        if b_errors is None:\n",
    "            b_outputs,b_errors,b_states = tmp_b_outputs,tmp_b_errors,tmp_b_states\n",
    "        else:\n",
    "            b_outputs = np.concatenate((b_outputs,tmp_b_outputs))\n",
    "            b_errors = np.concatenate((b_errors,tmp_b_errors))\n",
    "            b_states = np.concatenate((b_states,tmp_b_states),axis=2)\n",
    "    return_elements = []\n",
    "    if return_outputs:\n",
    "        return_elements += [b_outputs]\n",
    "    if return_states:\n",
    "        return_elements += [b_states]\n",
    "    if len(return_elements) > 0:\n",
    "        return calc_metrics(b_outputs,target_sequences,token_index,reverse_char_index,b_errors,exact_word,display) + tuple(return_elements)\n",
    "    return calc_metrics(b_outputs,target_sequences,target_token_index,reverse_char_index,b_errors,exact_word,display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T03:05:45.912268Z",
     "iopub.status.busy": "2022-05-03T03:05:45.911779Z",
     "iopub.status.idle": "2022-05-03T03:05:46.230857Z",
     "shell.execute_reply": "2022-05-03T03:05:46.230102Z",
     "shell.execute_reply.started": "2022-05-03T03:05:45.912214Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Nadam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def model_train_util(config):\n",
    "    #utility function to build and compile model based on the configuration passed as input and return the model\n",
    "    model = make_model(num_encoder_tokens,num_decoder_tokens,config['input_embedding_size'],config['num_enc_layers'],config['num_dec_layers'],config['hidden_layer_size'],config['cell_type'],config['dropout'],config['r_dropout'])\n",
    "    optimizer = config['optimizer']\n",
    "    if config['loss_function'] == 'categorical_crossentropy':\n",
    "        loss_fn = CategoricalCrossentropy\n",
    "    if optimizer == 'adam':\n",
    "        model.compile(optimizer = Adam(learning_rate=config['learning_rate']), loss = loss_fn(), metrics = ['accuracy'])\n",
    "    elif optimizer == 'momentum':\n",
    "        model.compile(optimizer = SGD(learning_rate=config['learning_rate'], momentum = 0.9), loss = loss_fn(), metrics = ['accuracy'])\n",
    "    elif optimizer == 'rmsprop':\n",
    "        model.compile(optimizer = RMSprop(learning_rate=config['learning_rate']), loss = loss_fn(), metrics = ['accuracy'])\n",
    "    elif optimizer == 'nesterov':\n",
    "        model.compile(optimizer = SGD(learning_rate=config['learning_rate'], momentum = 0.9, nesterov = True), loss = loss_fn(), metrics = ['accuracy'])\n",
    "    elif optimizer == 'nadam':\n",
    "        model.compile(optimizer = Nadam(learning_rate=config['learning_rate']), loss = loss_fn(), metrics = ['accuracy'])\n",
    "    else:\n",
    "        model.compile(optimizer = SGD(learning_rate=config['learning_rate']), loss = loss_fn(), metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T03:05:58.874902Z",
     "iopub.status.busy": "2022-05-03T03:05:58.874189Z",
     "iopub.status.idle": "2022-05-03T03:05:58.889697Z",
     "shell.execute_reply": "2022-05-03T03:05:58.888957Z",
     "shell.execute_reply.started": "2022-05-03T03:05:58.874816Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_inference_model(model):\n",
    "            '''\n",
    "    Function to return models needed for inference from the original model (with attention).\n",
    "    Arguments :\n",
    "        model -- (Keras model object) attention model used for training\n",
    "    Returns :\n",
    "        encoder_model -- (Keras model object) \n",
    "        deocder_model -- (Keras model object)\n",
    "        num_enc_layers -- (int) number of layers in the encoder\n",
    "        num_dec_layers -- (int) number of layers in the decoder\n",
    "    '''\n",
    "    # Calculating number of layers in encoder and decoder\n",
    "    num_enc_layers, num_dec_layers = 0, 0\n",
    "    for layer in model.layers:\n",
    "        num_enc_layers += layer.name.startswith('encoder')\n",
    "        num_dec_layers += layer.name.startswith('decoder')\n",
    "\n",
    "    # Encoder input\n",
    "    encoder_input = model.input[0]      # Input_1\n",
    "    # Encoder cell final layer\n",
    "    encoder_cell = model.get_layer(\"encoder_\"+str(num_enc_layers))\n",
    "    encoder_type = encoder_cell.__class__.__name__\n",
    "    encoder_seq, *encoder_state = encoder_cell.output\n",
    "    # Encoder model\n",
    "    encoder_model = keras.Model(encoder_input, encoder_state)\n",
    "\n",
    "    # Decoder input\n",
    "    decoder_input = model.input[1]      # Input_2\n",
    "    decoder_input_embedding = model.get_layer(\"embedding_2\")(decoder_input)\n",
    "    decoder_sequences = decoder_input_embedding\n",
    "    # Inputs to decoder layers' initial states\n",
    "    decoder_states, decoder_state_inputs = [], []\n",
    "    for i in range(1, num_dec_layers+1):\n",
    "        if encoder_type == 'LSTM':\n",
    "            decoder_state_input = [Input(shape=(encoder_state[0].shape[1],), name=\"input_\"+str(2*i+1)), \n",
    "                                   Input(shape=(encoder_state[1].shape[1],), name=\"input_\"+str(2*i+2))]\n",
    "        else:\n",
    "            decoder_state_input = [Input(shape=(encoder_state[0].shape[1],), name=\"input_\"+str(i+2))]\n",
    "\n",
    "        decoder_cell = model.get_layer(\"decoder_\"+str(i))\n",
    "        decoder_sequences, *decoder_state = decoder_cell(decoder_sequences, initial_state=decoder_state_input)\n",
    "        decoder_states += decoder_state\n",
    "        decoder_state_inputs += decoder_state_input\n",
    "\n",
    "    # Softmax FC layer\n",
    "    decoder_dense = model.get_layer(\"dense_1\")\n",
    "    decoder_dense_output = decoder_dense(decoder_sequences)\n",
    "\n",
    "    # Decoder model\n",
    "    decoder_model = keras.Model(\n",
    "        [decoder_input] + decoder_state_inputs, [decoder_dense_output] + decoder_states\n",
    "    )\n",
    "\n",
    "    return encoder_model, decoder_model, num_enc_layers, num_dec_layers\n",
    "\n",
    "\n",
    "def num_to_word(num_encoded, token_index, reverse_char_index = None):\n",
    "    # Function to return the predictions after cutting the '\\n' and ' ' s at the end.\n",
    "    # If reverse_char_index == None, the predictions are in the form of decoded string, otherwise as list of integers\n",
    "    num_samples = len(num_encoded) if type(num_encoded) is list else num_encoded.shape[0]\n",
    "    predicted_words = ['' for t in range(num_samples)]\n",
    "    for i, encode in enumerate(num_encoded):\n",
    "        for l in encode:\n",
    "            # Stop word : '\\n'\n",
    "            if l == token_index['\\n']:\n",
    "                break\n",
    "            predicted_words[i] += reverse_char_index[l] if reverse_char_index is not None else str(l)\n",
    "    \n",
    "    return predicted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T03:05:46.232594Z",
     "iopub.status.busy": "2022-05-03T03:05:46.232318Z",
     "iopub.status.idle": "2022-05-03T03:05:46.240213Z",
     "shell.execute_reply": "2022-05-03T03:05:46.239490Z",
     "shell.execute_reply.started": "2022-05-03T03:05:46.232555Z"
    }
   },
   "outputs": [],
   "source": [
    "class customCallback(keras.callbacks.Callback):\n",
    "     # Custom class to provide callback after each epoch of training to calculate custom metrics for validation set with beam decoder\n",
    "    def __init__(self, val_enc_input, val_dec_target, beam_width=1, batch_size=64, attention=False):\n",
    "        self.beam_width = beam_width\n",
    "        self.validation_input = val_enc_input\n",
    "        self.validation_target = val_dec_target\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        val_accuracy, val_exact_accuracy, val_loss = beam_decoder(self.model, self.validation_input, self.validation_target, max_decoder_seq_length, \n",
    "                                                                  target_token_index, reverse_target_char_index, self.beam_width, self.batch_size)\n",
    "\n",
    "        # Log them to reflect in WANDB callback and EarlyStopping\n",
    "        logs[\"val_accuracy\"] = val_accuracy\n",
    "        logs[\"val_exact_accuracy\"] = val_exact_accuracy\n",
    "        logs[\"val_loss\"] = val_loss             # Validation loss calculates categorical cross entropy loss\n",
    "\n",
    "        print(\"— val_loss: {:.3f} — val_accuracy: {:.3f} — val_exact_accuracy: {:.5f}\".format(val_loss, val_accuracy, val_exact_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T03:05:46.265705Z",
     "iopub.status.busy": "2022-05-03T03:05:46.265062Z",
     "iopub.status.idle": "2022-05-03T03:05:58.847442Z",
     "shell.execute_reply": "2022-05-03T03:05:58.846584Z",
     "shell.execute_reply.started": "2022-05-03T03:05:46.265663Z"
    }
   },
   "outputs": [],
   "source": [
    "#Wandb import and authentication\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "wandb.login(key='b44266d937596fcef83bedbe7330d6cee108a277')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T03:05:58.849472Z",
     "iopub.status.busy": "2022-05-03T03:05:58.849191Z",
     "iopub.status.idle": "2022-05-03T03:05:58.861900Z",
     "shell.execute_reply": "2022-05-03T03:05:58.861163Z",
     "shell.execute_reply.started": "2022-05-03T03:05:58.849434Z"
    }
   },
   "outputs": [],
   "source": [
    "def model_train(config,iswandb=False):\n",
    "    '''\n",
    "    Arguments:\n",
    "        config -- (dict) Hyperparameter config with which model is trained\n",
    "        iswandb -- (bool,default:False) If True training is done by logging metrics to wandb otherwise training is done as it is\n",
    "    Returns:\n",
    "        wid: Wandb run id if any \n",
    "        model - Keras model\n",
    "        history - metrics of training\n",
    "        config - Same as passed in argument\n",
    "    '''\n",
    "    wid = None\n",
    "    if iswandb:\n",
    "        wid = wandb.util.generate_id()\n",
    "        run = wandb.init(id = wid, project=\"cs6910_assignment_3\", entity=\"dlstack\", reinit=True, config=config)\n",
    "        wandb.run.name = f\"ies_{config['input_embedding_size']}_nenc_{config['num_enc_layers']}_ndec_{config['num_dec_layers']}_cell_{config['cell_type']}_drop_{config['dropout']}_rdrop_{config['r_dropout']}\"\n",
    "        wandb.run.name += f\"_hs_{config['hidden_layer_size']}_B_{config['beam_width']}\"\n",
    "        wandb.run.save()\n",
    "        print(wandb.run.name)\n",
    "\n",
    "    model = model_train_util(config)\n",
    "    if iswandb:\n",
    "        call_list = [customCallback(val_encoder_input,val_decoder_target,beam_width=config['beam_width'],batch_size=config['batch_size']),WandbCallback(monitor='val_accuracy'),EarlyStopping(monitor='val_accuracy',patience=4)]\n",
    "    else:\n",
    "        call_list = [customCallback(val_encoder_input,val_decoder_target,beam_width=config['beam_width'],batch_size=config['batch_size']),EarlyStopping(monitor='val_accuracy',patience=4)]\n",
    "    history = model.fit(\n",
    "        [train_encoder_input,train_decoder_input],\n",
    "        train_decoder_target,\n",
    "        batch_size=config['batch_size'],\n",
    "        verbose = 1,\n",
    "        epochs=config['epochs'],\n",
    "        callbacks = call_list\n",
    "    )    \n",
    "    if iswandb:\n",
    "        run.finish()\n",
    "\n",
    "    return model, history,config, wid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T03:05:46.242405Z",
     "iopub.status.busy": "2022-05-03T03:05:46.241272Z",
     "iopub.status.idle": "2022-05-03T03:05:46.263436Z",
     "shell.execute_reply": "2022-05-03T03:05:46.262698Z",
     "shell.execute_reply.started": "2022-05-03T03:05:46.242362Z"
    }
   },
   "outputs": [],
   "source": [
    "# model,history,config,_ = model_train(config_)\n",
    "# model.save(\"s2s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T03:05:58.866263Z",
     "iopub.status.busy": "2022-05-03T03:05:58.864020Z",
     "iopub.status.idle": "2022-05-03T03:05:58.872878Z",
     "shell.execute_reply": "2022-05-03T03:05:58.872207Z",
     "shell.execute_reply.started": "2022-05-03T03:05:58.866221Z"
    }
   },
   "outputs": [],
   "source": [
    "def wandb_sweep():\n",
    "    # Wrapper function to call the model_train() function for sweeping with different hyperparameters\n",
    "\n",
    "    # Initialize a new wandb run\n",
    "    run = wandb.init(config=config_, reinit=True)\n",
    "\n",
    "    # Config is a variable that holds and saves hyperparameters and inputs\n",
    "    config = wandb.config\n",
    "\n",
    "    wandb.run.name = f'ies_{config.input_embedding_size}_nenc_{config.num_enc_layers}_ndec_{config.num_dec_layers}_cell_{config.cell_type}_drop_{config.dropout}_rdrop_{config.r_dropout}'\n",
    "    wandb.run.name += f'_hs_{config.hidden_layer_size}_B_{config.beam_width}'\n",
    "    wandb.run.save()\n",
    "    print(wandb.run.name)\n",
    "\n",
    "    model, *_ = model_train(config, iswandb=True)\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T03:05:58.937857Z",
     "iopub.status.busy": "2022-05-03T03:05:58.937393Z",
     "iopub.status.idle": "2022-05-03T03:05:58.945346Z",
     "shell.execute_reply": "2022-05-03T03:05:58.944694Z",
     "shell.execute_reply.started": "2022-05-03T03:05:58.937802Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameter choices to sweep \n",
    "sweep_config_1 = {\n",
    "    'name': 'RNNs2s',\n",
    "    'method': 'bayes',                   # Possible search : grid, random, bayes\n",
    "    'metric': {\n",
    "      'name': 'val_accuracy',\n",
    "      'goal': 'maximize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'epochs':{\n",
    "            'values':[10,15,20]\n",
    "        },\n",
    "        'learning_rate':{\n",
    "            'values':[0.001,0.0001,0.005]\n",
    "        },\n",
    "        'optimizer':{\n",
    "            'value':'adam'\n",
    "        },\n",
    "        'loss_function':{\n",
    "          'value':'categorical_crossentropy'  \n",
    "        },\n",
    "        'input_embedding_size': {\n",
    "            'values': [64,128,256]\n",
    "        },\n",
    "        'num_enc_layers': {\n",
    "            'values': [2,3]\n",
    "        },\n",
    "        'num_dec_layers': {\n",
    "            'values': [3,5]\n",
    "        },\n",
    "        'hidden_layer_size': {\n",
    "            'values': [256,512,768]\n",
    "        },\n",
    "        'cell_type': {\n",
    "            'values': ['GRU']\n",
    "        },\n",
    "        'dropout' :{\n",
    "            'values': [0.20,0.30]\n",
    "        },\n",
    "        'r_dropout': {\n",
    "            'values': [0.20,0.30]\n",
    "        },\n",
    "        'beam_width': {\n",
    "            'values': [1,3,5]\n",
    "        },\n",
    "        'batch_size':{\n",
    "            'values':[128,256]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "sweep_id = wandb.sweep(sweep_config_1,entity='dlstack',project='cs6910_assignment_3')\n",
    "wandb.agent(sweep_id,lambda:wandb_sweep(),count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T03:05:58.949520Z",
     "iopub.status.busy": "2022-05-03T03:05:58.946304Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameter choices to sweep \n",
    "sweep_config_1 = {\n",
    "    'name': 'RNNs2s',\n",
    "    'method': 'bayes',                   # Possible search : grid, random, bayes\n",
    "    'metric': {\n",
    "      'name': 'val_accuracy',\n",
    "      'goal': 'maximize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'epochs':{\n",
    "            'values':[10,15,20]\n",
    "        },\n",
    "        'learning_rate':{\n",
    "            'values':[1e-3,1e-4]\n",
    "        },\n",
    "        'optimizer':{\n",
    "            'value':'adam'\n",
    "        },\n",
    "        'loss_function':{\n",
    "          'value':'categorical_crossentropy'  \n",
    "        },\n",
    "        'input_embedding_size': {\n",
    "            'values': [32, 64, 128,256]\n",
    "        },\n",
    "        'num_enc_layers': {\n",
    "            'values': [2,3]\n",
    "        },\n",
    "        'num_dec_layers': {\n",
    "            'values': [2,3,5]\n",
    "        },\n",
    "        'hidden_layer_size': {\n",
    "            'values': [ 128,256,512,768]\n",
    "        },\n",
    "        'cell_type': {\n",
    "            'values': [ 'RNN','LSTM', 'GRU']\n",
    "        },\n",
    "        'dropout' :{\n",
    "            'values': [0.20,0.25,0.30]\n",
    "        },\n",
    "        'r_dropout':{\n",
    "          'values': [0.0,0.20,0.30]  \n",
    "        },\n",
    "        'beam_width': {\n",
    "            'values': [1, 3,5]\n",
    "        },\n",
    "        'batch_size':{\n",
    "            'values':[64,128,256]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "sweep_id = wandb.sweep(sweep_config_1,entity='dlstack',project='cs6910_assignment_3')\n",
    "wandb.agent(sweep_id,lambda:wandb_sweep(),count=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter choices to sweep \n",
    "sweep_config_1 = {\n",
    "    'name': 'RNNs2s',\n",
    "    'method': 'bayes',                   # Possible search : grid, random, bayes\n",
    "    'metric': {\n",
    "      'name': 'val_accuracy',\n",
    "      'goal': 'maximize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'epochs':{\n",
    "            'values':[10,15,20]\n",
    "        },\n",
    "        'learning_rate':{\n",
    "            'values':[1e-3,1e-4]\n",
    "        },\n",
    "        'optimizer':{\n",
    "            'values':['rmsprop','adam','nadam','nesterov','sgd']\n",
    "        },\n",
    "        'loss_function':{\n",
    "          'value':'categorical_crossentropy'  \n",
    "        },\n",
    "        'input_embedding_size': {\n",
    "            'values': [32, 64,256]\n",
    "        },\n",
    "        'num_enc_layers': {\n",
    "            'values': [ 2, 3,4]\n",
    "        },\n",
    "        'num_dec_layers': {\n",
    "            'values': [ 2, 3,4]\n",
    "        },\n",
    "        'hidden_layer_size': {\n",
    "            'values': [64, 128, 256,512]\n",
    "        },\n",
    "        'cell_type': {\n",
    "            'values': ['RNN', 'LSTM', 'GRU']\n",
    "        },\n",
    "        'dropout' :{\n",
    "            'values': [0, 0.25, 0.3,0.4]\n",
    "        },\n",
    "        'r_dropout':{\n",
    "          'values':[0.0,0.1]  \n",
    "        },\n",
    "        'batch_size':{\n",
    "          'values':[64,128,256]  \n",
    "        },\n",
    "        'beam_width': {\n",
    "            'values': [1, 5]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "sweep_id = wandb.sweep(sweep_config_1,entity='dlstack',project='cs6910_assignment_3')\n",
    "wandb.agent(sweep_id,lambda:wandb_sweep(),count=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
