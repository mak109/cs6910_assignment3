{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mak109/cs6910_assignment3/blob/main/cs6910_assignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBH6NLvwX83y",
        "outputId": "5bdf2a20-a0b0-4e1e-9ce8-e870c71e706c"
      },
      "id": "yBH6NLvwX83y",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=f00aca1afb3b425762f69d056bec4413c59ec9be34a4cfb1ce0784144b61b177\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "016c396b",
      "metadata": {
        "id": "016c396b"
      },
      "outputs": [],
      "source": [
        "import wget\n",
        "import os\n",
        "import tarfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d75b50f0",
      "metadata": {
        "id": "d75b50f0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7cec174a",
      "metadata": {
        "id": "7cec174a"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6f330ce0",
      "metadata": {
        "id": "6f330ce0"
      },
      "outputs": [],
      "source": [
        "from keras.layers import SimpleRNN,GRU,LSTM,Embedding,Input,Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4dbf4807",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dbf4807",
        "outputId": "0ee6603a-03d0-4d7d-f0dc-236311d117de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracting files ....\n",
            "Done\n"
          ]
        }
      ],
      "source": [
        "filename = 'dakshina_dataset_v1.0'\n",
        "url = 'https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar'\n",
        "if not os.path.exists(filename+'.tar') and not os.path.exists(filename):\n",
        "    filename_tar = wget.download(url)\n",
        "    file = tarfile.open(filename_tar)\n",
        "    print('\\nExtracting files ....')\n",
        "    file.extractall()\n",
        "    file.close()\n",
        "    print('Done')\n",
        "    os.remove(filename_tar)\n",
        "elif not os.path.exists(filename):\n",
        "    filename_tar = filename + '.tar'\n",
        "    file = tarfile.open(filename_tar)\n",
        "    print('\\nExtracting files ....')\n",
        "    file.extractall()\n",
        "    file.close()\n",
        "    print('Done')\n",
        "    os.remove(filename_tar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "5ebb6603",
      "metadata": {
        "id": "5ebb6603"
      },
      "outputs": [],
      "source": [
        "lang = 'hi'\n",
        "train_path =  filename+f\"/{lang}/lexicons/{lang}.translit.sampled.train.tsv\"\n",
        "val_path = filename+f\"/{lang}/lexicons/{lang}.translit.sampled.dev.tsv\"\n",
        "test_path = filename+f\"/{lang}/lexicons/{lang}.translit.sampled.test.tsv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8414ebed",
      "metadata": {
        "id": "8414ebed"
      },
      "outputs": [],
      "source": [
        "def read_data(path):\n",
        "    df = pd.read_csv(path,header=None,sep='\\t')\n",
        "    df.dropna(inplace=True)\n",
        "    input_texts,target_texts = df[1].to_list(),df[0].to_list()\n",
        "    return input_texts,target_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "962458af",
      "metadata": {
        "id": "962458af"
      },
      "outputs": [],
      "source": [
        "def parse_text(texts):\n",
        "    characters = set()\n",
        "    for text in texts:\n",
        "        for c in text:\n",
        "            if c not in characters:\n",
        "                characters.add(c)\n",
        "    characters.add(' ')\n",
        "    return sorted(list(characters))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "dd8bf987",
      "metadata": {
        "id": "dd8bf987"
      },
      "outputs": [],
      "source": [
        "def start_end_pad(texts):\n",
        "    for i in range(len(texts)):\n",
        "        texts[i] = \"\\t\" + texts[i] + \"\\n\"\n",
        "    return texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "5802cab2",
      "metadata": {
        "id": "5802cab2"
      },
      "outputs": [],
      "source": [
        "train_input_texts,train_target_texts = read_data(train_path)\n",
        "val_input_texts,val_target_texts = read_data(val_path)\n",
        "test_input_texts,test_target_texts = read_data(test_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "254339dc",
      "metadata": {
        "id": "254339dc"
      },
      "outputs": [],
      "source": [
        "train_target_texts = start_end_pad(train_target_texts)\n",
        "val_target_texts = start_end_pad(val_target_texts)\n",
        "test_target_texts = start_end_pad(test_target_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "ff791142",
      "metadata": {
        "id": "ff791142"
      },
      "outputs": [],
      "source": [
        "config_ = {\n",
        "    \"learning_rate\": 1e-3,                                      # Learning rate in gradient descent\n",
        "    \"epochs\": 10,                                               # Number of epochs to train the model   \n",
        "    \"optimizer\": 'adam',                                        # Gradient descent algorithm used for the parameter updation\n",
        "    \"batch_size\": 64,                                           # Batch size used for the optimizer\n",
        "    \"loss_function\": 'categorical_crossentropy',                # Loss function used in the optimizer                                                                      # Name of dataset\n",
        "    \"input_embedding_size\": 64,                                        # Size of input embedding layer\n",
        "    \"num_enc_layers\": 2,                                         # Number of layers in the encoder\n",
        "    \"num_dec_layers\": 2,                                         # Number of layers in the decoder\n",
        "    \"hidden_layer_size\": 128,                                      # Size of hidden layer\n",
        "    \"dropout\" : 0.20,                                           # Value of dropout used in the normal and recurrent dropout\n",
        "    \"cell_type\": 'LSTM',                                         # Type of cell used in the encoder and decoder ('RNN' or 'GRU' or 'LSTM')\n",
        "    \"beam_width\": 5,                                            # Beam width used in beam decoder\n",
        "    \"attention\": False                                          # Whether or not attention is used\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "afc360d0",
      "metadata": {
        "id": "afc360d0"
      },
      "outputs": [],
      "source": [
        "def enc_dec_tokens(train_input_texts,train_target_texts,val_input_texts,val_target_texts):\n",
        "    \n",
        "    input_characters = parse_text(train_input_texts + val_input_texts)\n",
        "    target_characters = parse_text(train_target_texts + val_target_texts)\n",
        "    num_encoder_tokens = len(input_characters)\n",
        "    num_decoder_tokens = len(target_characters)\n",
        "    max_encoder_seq_length = max([len(txt) for txt in train_input_texts + val_input_texts])\n",
        "    max_decoder_seq_length = max([len(txt) for txt in train_target_texts + val_target_texts])\n",
        "\n",
        "    print(\"Number of training samples:\", len(train_input_texts))\n",
        "    print(\"Number of validation samples:\", len(val_input_texts))\n",
        "    print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "    print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "    print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "    print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "    \n",
        "    input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "    target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "    \n",
        "    return input_token_index,target_token_index,max_encoder_seq_length,max_decoder_seq_length,num_encoder_tokens,num_decoder_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "372d4d70",
      "metadata": {
        "id": "372d4d70"
      },
      "outputs": [],
      "source": [
        "def data_processing(input_texts,enc_length,input_token_index,num_encoder_tokens, target_texts,dec_length,target_token_index,num_decoder_tokens):\n",
        "    encoder_input_data = np.zeros(\n",
        "        (len(input_texts), enc_length), dtype=\"float32\"\n",
        "    )\n",
        "    decoder_input_data = np.zeros(\n",
        "            (len(input_texts), dec_length), dtype=\"float32\"\n",
        "        )\n",
        "    decoder_target_data = np.zeros(\n",
        "            (len(input_texts), dec_length, num_decoder_tokens), dtype=\"float32\"\n",
        "        )\n",
        "\n",
        "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "        \n",
        "        for t, char in enumerate(input_text):\n",
        "            encoder_input_data[i, t] = input_token_index[char]\n",
        "        encoder_input_data[i, t + 1 :] = input_token_index[' ']\n",
        "        \n",
        "        for t, char in enumerate(target_text):\n",
        "                # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "            decoder_input_data[i, t] = target_token_index[char]\n",
        "            if t > 0:\n",
        "                    # decoder_target_data will be ahead by one timestep\n",
        "                    # and will not include the start character.\n",
        "                decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "        decoder_input_data[i, t + 1 :] = target_token_index[' ']\n",
        "        decoder_target_data[i, t:, target_token_index[' ']] = 1.0\n",
        "    return encoder_input_data,decoder_input_data,decoder_target_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "51ac3146",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51ac3146",
        "outputId": "5fa0f20b-779e-446e-ebdc-29346b3bbaca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples: 44202\n",
            "Number of validation samples: 4358\n",
            "Number of unique input tokens: 27\n",
            "Number of unique output tokens: 66\n",
            "Max sequence length for inputs: 20\n",
            "Max sequence length for outputs: 21\n"
          ]
        }
      ],
      "source": [
        "input_token_index,target_token_index,max_encoder_seq_length,max_decoder_seq_length,num_encoder_tokens,num_decoder_tokens = enc_dec_tokens(train_input_texts,train_target_texts,val_input_texts,val_target_texts)\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "3a1f066b",
      "metadata": {
        "id": "3a1f066b"
      },
      "outputs": [],
      "source": [
        "train_encoder_input,train_decoder_input,train_decoder_target = data_processing(train_input_texts,max_encoder_seq_length,input_token_index,num_encoder_tokens, train_target_texts,max_decoder_seq_length,target_token_index,num_decoder_tokens)\n",
        "val_encoder_input,val_decoder_input,val_decoder_target = data_processing(val_input_texts,max_encoder_seq_length,input_token_index,num_encoder_tokens, val_target_texts,max_decoder_seq_length,target_token_index,num_decoder_tokens)\n",
        "test_encoder_input,test_decoder_input,test_decoder_target = data_processing(test_input_texts,max_encoder_seq_length,input_token_index,num_encoder_tokens, test_target_texts,max_decoder_seq_length,target_token_index,num_decoder_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "8ca85285",
      "metadata": {
        "id": "8ca85285"
      },
      "outputs": [],
      "source": [
        "def make_model(num_encoder_tokens,num_decoder_tokens,input_embedding_size=32,num_enc_layers=1,num_dec_layers=1,hidden_layer_size=64,cell_type='LSTM',dropout=0,r_dropout=0,cell_activation='tanh'):\n",
        "    cell = {\n",
        "        'RNN':SimpleRNN,\n",
        "        'LSTM':LSTM,\n",
        "        'GRU':GRU\n",
        "    }\n",
        "    encoder_input = Input(shape=(None,),name='input_1')\n",
        "    encoder_input_embedding = Embedding(num_encoder_tokens,input_embedding_size,name='embedding_1')(encoder_input)\n",
        "    \n",
        "    encoder_sequences, *encoder_state = cell[cell_type](hidden_layer_size,activation=cell_activation,return_sequences=True,return_state=True,dropout=dropout,recurrent_dropout=r_dropout,name=\"encoder_1\")(encoder_input_embedding)\n",
        "    \n",
        "    for i in range(1,num_enc_layers):\n",
        "        encoder_sequences, *encoder_state = cell[cell_type](hidden_layer_size,activation=cell_activation,return_sequences=True,return_state=True,dropout=dropout,recurrent_dropout=r_dropout,name=f\"encoder_{i+1}\")(encoder_sequences)\n",
        "        \n",
        "    decoder_input = Input(shape=(None,),name='input_2')\n",
        "    decoder_input_embedding = Embedding(num_decoder_tokens,input_embedding_size,name='embedding_2')(decoder_input)\n",
        "    \n",
        "    decoder_sequences, *decoder_state = cell[cell_type](hidden_layer_size,activation=cell_activation,return_sequences=True,return_state=True,dropout=dropout,recurrent_dropout=r_dropout,name=\"decoder_1\")(decoder_input_embedding ,initial_state=encoder_state)\n",
        "    \n",
        "    for i in range(1,num_dec_layers):\n",
        "        decoder_sequences, *decoder_state = cell[cell_type](hidden_layer_size,activation=cell_activation,return_sequences=True,return_state=True,dropout=dropout,recurrent_dropout=r_dropout,name=f\"decoder_{i+1}\")(decoder_sequences ,initial_state=encoder_state)\n",
        "    \n",
        "    decoder_dense = Dense(num_decoder_tokens,activation=\"softmax\",name=\"dense_1\")(decoder_sequences)\n",
        "    \n",
        "    model = keras.Model([encoder_input,decoder_input],decoder_dense)\n",
        "    model.summary()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "dcfd6397",
      "metadata": {
        "id": "dcfd6397"
      },
      "outputs": [],
      "source": [
        "# model = make_model(num_encoder_tokens,num_decoder_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "7af61f7b",
      "metadata": {
        "scrolled": false,
        "id": "7af61f7b"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Nadam\n",
        "from tensorflow.keras.metrics import categorical_crossentropy\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "def model_train_util(config):\n",
        "    model = make_model(num_encoder_tokens,num_decoder_tokens,config['input_embedding_size'],config['num_enc_layers'],config['num_dec_layers'],config['hidden_layer_size'],config['cell_type'],config['dropout'],config['dropout'])\n",
        "    optimizer = config['optimizer']\n",
        "    if config['loss_function'] == 'categorical_crossentropy':\n",
        "        loss_fn = CategoricalCrossentropy\n",
        "    if optimizer == 'adam':\n",
        "        model.compile(optimizer = Adam(learning_rate=config['learning_rate']), loss = loss_fn(), metrics = ['accuracy'])\n",
        "    elif optimizer == 'momentum':\n",
        "        model.compile(optimizer = SGD(learning_rate=config['learning_rate'], momentum = 0.9), loss = loss_fn(), metrics = ['accuracy'])\n",
        "    elif optimizer == 'rmsprop':\n",
        "        model.compile(optimizer = RMSprop(learning_rate=config['learning_rate']), loss = loss_fn(), metrics = ['accuracy'])\n",
        "    elif optimizer == 'nesterov':\n",
        "        model.compile(optimizer = SGD(learning_rate=config['learning_rate'], momentum = 0.9, nesterov = True), loss = loss_fn(), metrics = ['accuracy'])\n",
        "    elif optimizer == 'nadam':\n",
        "        model.compile(optimizer = Nadam(learning_rate=config['learning_rate']), loss = loss_fn(), metrics = ['accuracy'])\n",
        "    else:\n",
        "        model.compile(optimizer = SGD(learning_rate=config['learning_rate']), loss = loss_fn(), metrics = ['accuracy'])\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "fa545a61",
      "metadata": {
        "id": "fa545a61"
      },
      "outputs": [],
      "source": [
        "class customCallback(keras.callbacks.Callback):\n",
        "     # Custom class to provide callback after each epoch of training to calculate custom metrics for validation set with beam decoder\n",
        "    def __init__(self, val_enc_input, val_dec_target, beam_width=1, batch_size=64, attention=False):\n",
        "        self.beam_width = beam_width\n",
        "        self.validation_input = val_enc_input\n",
        "        self.validation_target = val_dec_target\n",
        "        self.batch_size = batch_size\n",
        "        self.attention = attention\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        val_accuracy, val_exact_accuracy, val_loss = beam_decoder(self.model, self.validation_input, self.validation_target, max_decoder_seq_length, \n",
        "                                                                  target_token_index, reverse_target_char_index, self.beam_width, self.batch_size, self.attention)\n",
        "\n",
        "        # Log them to reflect in WANDB callback and EarlyStopping\n",
        "        logs[\"val_accuracy\"] = val_accuracy\n",
        "        logs[\"val_exact_accuracy\"] = val_exact_accuracy\n",
        "        logs[\"val_loss\"] = val_loss             # Validation loss calculates categorical cross entropy loss\n",
        "\n",
        "        print(\"— val_loss: {:.3f} — val_accuracy: {:.3f} — val_exact_accuracy: {:.5f}\".format(val_loss, val_accuracy, val_exact_accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "dab01929",
      "metadata": {
        "id": "dab01929"
      },
      "outputs": [],
      "source": [
        "# model,history = model_train_util(config_,[train_encoder_input,train_decoder_input],train_decoder_target,[val_encoder_input,val_decoder_input],val_decoder_target)\n",
        "# model.save(\"s2s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "d7749a5c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7749a5c",
        "outputId": "5b5b7171-3aa4-4fb6-d490-4232200660f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.12.15-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.10-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 51.2 MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.2.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 65.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.3 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=c3a69db4d5c147eb3bde72505af73d9353d8cd6511ca22fa04cd0ae7a8a15a8b\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.10 setproctitle-1.2.3 shortuuid-1.0.8 smmap-5.0.0 wandb-0.12.15\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "# Installing and logging into WANDB\n",
        "!pip install --upgrade wandb\n",
        "!wandb login b44266d937596fcef83bedbe7330d6cee108a277\n",
        "\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "1ed63c04",
      "metadata": {
        "id": "1ed63c04"
      },
      "outputs": [],
      "source": [
        "def model_train(config,wandb=True):\n",
        "    wid = None\n",
        "    if wandb:\n",
        "        wid = wandb.util.generate_id()\n",
        "        run = wandb.init(id = wid, project=\"cs6910_assignment_3\", entity=\"dlstack\", reinit=True, config=config)\n",
        "        wandb.run.name = f\"ies_{config['input_embedding_size']}_nenc_{config['num_enc_layers']}_ndec_{config['num_dec_layers']}_cell_{config['cell_type']}_drop_{config['dropout']}\"\n",
        "        wandb.run.name += f\"_hs_{config['hidden_layer_size']}_B_{config['beam_width']}_attn_{config['attention']}\"\n",
        "        wandb.run.save()\n",
        "        print(wandb.run.name)\n",
        "\n",
        "    model = model_train_util(config)\n",
        "    if wandb:\n",
        "        call_list = [customCallback(val_encoder_input,val_decoder_target,beam_width=config['beam_width'],batch_size=config['batch_size'],attention=config['attention']),WandbCallback(monitor='val_accuracy'),EarlyStopping(monitor='val_accuracy', patience=4)]\n",
        "    else:\n",
        "        call_list = [customCallback(val_encoder_input,val_decoder_target,beam_width=config['beam_width'],batch_size=config['batch_size'],attention=config['attention']),EarlyStopping(monitor='val_accuracy', patience=4)]\n",
        "    history = model.fit(\n",
        "       [train_encoder_input,train_decoder_input],\n",
        "        train_decoder_target,\n",
        "        batch_size=config['batch_size'],\n",
        "        verbose = 2,\n",
        "        epochs=config['epochs'],\n",
        "        callbacks = call_list\n",
        "    )    \n",
        "    if wandb:\n",
        "        run.finish()\n",
        "\n",
        "    return model, history,config, wid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "97f7277c",
      "metadata": {
        "id": "97f7277c"
      },
      "outputs": [],
      "source": [
        "def wandb_sweep():\n",
        "    # Wrapper function to call the model_train() function for sweeping with different hyperparameters\n",
        "\n",
        "    # Initialize a new wandb run\n",
        "    run = wandb.init(config=config_, reinit=True)\n",
        "\n",
        "    # Config is a variable that holds and saves hyperparameters and inputs\n",
        "    config = wandb.config\n",
        "\n",
        "    wandb.run.name = f'ies_{config.input_embedding_size}_nenc_{config.num_enc_layers}_ndec_{config.num_dec_layers}_cell_{config.cell_type}_drop_{config.dropout}'\n",
        "    wandb.run.name += f'_hs_{config.hidden_layer_size}_B_{config.beam_width}'\n",
        "    wandb.run.save()\n",
        "    print(wandb.run.name)\n",
        "\n",
        "    model, *_ = model_train(config, wandb=False)\n",
        "    run.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "6adbfc73",
      "metadata": {
        "id": "6adbfc73"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter choices to sweep \n",
        "sweep_config_1 = {\n",
        "    'name': 'RNN',\n",
        "    'method': 'bayes',                   # Possible search : grid, random, bayes\n",
        "    'metric': {\n",
        "      'name': 'val_accuracy',\n",
        "      'goal': 'maximize'   \n",
        "    },\n",
        "    'parameters': {\n",
        "        'learning_rate':{\n",
        "            'values':[1e-3,1e-4]\n",
        "        },\n",
        "        'optimizer':{\n",
        "            'values':['rmsprop','adam','nadam']\n",
        "        },\n",
        "        'loss_function':{\n",
        "          'value':'categorical_crossentropy' \n",
        "        },\n",
        "        'input_embedding_size': {\n",
        "            'values': [32, 64, 256]\n",
        "        },\n",
        "        'num_enc_layers': {\n",
        "            'values': [1, 2, 3]\n",
        "        },\n",
        "        'num_dec_layers': {\n",
        "            'values': [1, 2, 3]\n",
        "        },\n",
        "        'hidden_layer_size': {\n",
        "            'values': [32, 64, 256]\n",
        "        },\n",
        "        'cell_type': {\n",
        "            'values': ['RNN', 'LSTM', 'GRU']\n",
        "        },\n",
        "        'dropout' :{\n",
        "            'values': [0, 0.25, 0.3,0.4]\n",
        "        },\n",
        "        'beam_width': {\n",
        "            'values': [1, 5]\n",
        "        }\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "f0eafb5f",
      "metadata": {
        "scrolled": true,
        "id": "f0eafb5f"
      },
      "outputs": [],
      "source": [
        "def make_inference_model(model):\n",
        "    # Calculating number of layers in encoder and decoder\n",
        "    num_enc_layers, num_dec_layers = 0, 0\n",
        "    for layer in model.layers:\n",
        "        num_enc_layers += layer.name.startswith('encoder')\n",
        "        num_dec_layers += layer.name.startswith('decoder')\n",
        "\n",
        "    # Encoder input\n",
        "    encoder_input = model.input[0]      # Input_1\n",
        "    # Encoder cell final layer\n",
        "    encoder_cell = model.get_layer(\"encoder_\"+str(num_enc_layers))\n",
        "    encoder_type = encoder_cell.__class__.__name__\n",
        "    encoder_seq, *encoder_state = encoder_cell.output\n",
        "    # Encoder model\n",
        "    encoder_model = keras.Model(encoder_input, encoder_state)\n",
        "\n",
        "    # Decoder input\n",
        "    decoder_input = model.input[1]      # Input_2\n",
        "    decoder_input_embedding = model.get_layer(\"embedding_2\")(decoder_input)\n",
        "    decoder_sequences = decoder_input_embedding\n",
        "    # Inputs to decoder layers' initial states\n",
        "    decoder_states, decoder_state_inputs = [], []\n",
        "    for i in range(1, num_dec_layers+1):\n",
        "        if encoder_type == 'LSTM':\n",
        "            decoder_state_input = [Input(shape=(encoder_state[0].shape[1],), name=\"input_\"+str(2*i+1)), \n",
        "                                   Input(shape=(encoder_state[1].shape[1],), name=\"input_\"+str(2*i+2))]\n",
        "        else:\n",
        "            decoder_state_input = [Input(shape=(encoder_state[0].shape[1],), name=\"input_\"+str(i+2))]\n",
        "\n",
        "        decoder_cell = model.get_layer(\"decoder_\"+str(i))\n",
        "        decoder_sequences, *decoder_state = decoder_cell(decoder_sequences, initial_state=decoder_state_input)\n",
        "        decoder_states += decoder_state\n",
        "        decoder_state_inputs += decoder_state_input\n",
        "\n",
        "    # Softmax FC layer\n",
        "    decoder_dense = model.get_layer(\"dense_1\")\n",
        "    decoder_dense_output = decoder_dense(decoder_sequences)\n",
        "\n",
        "    # Decoder model\n",
        "    decoder_model = keras.Model(\n",
        "        [decoder_input] + decoder_state_inputs, [decoder_dense_output] + decoder_states\n",
        "    )\n",
        "\n",
        "    return encoder_model, decoder_model, num_enc_layers, num_dec_layers\n",
        "\n",
        "\n",
        "def num_to_word(num_encoded, token_index, reverse_char_index = None):\n",
        "    # Function to return the predictions after cutting the END_CHAR and BLANK_CHAR s at the end.\n",
        "    # If char_dec == None, the predictions are in the form of decoded string, otherwise as list of integers\n",
        "    num_samples = len(num_encoded) if type(num_encoded) is list else num_encoded.shape[0]\n",
        "    predicted_words = [\"\"]*num_samples\n",
        "    for i, encode in enumerate(num_encoded):\n",
        "        for l in encode:\n",
        "            # Stop word : '\\n'\n",
        "            if l == token_index['\\n']:\n",
        "                break\n",
        "            predicted_words[i] += reverse_char_index[l] if reverse_char_index is not None else str(l)\n",
        "    \n",
        "    return predicted_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "4f8a58d6",
      "metadata": {
        "id": "4f8a58d6"
      },
      "outputs": [],
      "source": [
        "def beam_decoder_util(model,input_sequences,max_decoder_seq_length,B=1,target_sequences=None,start_char=0,batch_size=64):\n",
        "    encoder_model,decoder_model,num_enc_layers,num_dec_layers=make_inference_model(model)\n",
        "    encoder_output = encoder_model.predict(input_sequences,batch_size=batch_size)\n",
        "    encoder_output = encoder_output if type(encoder_output) is list else [encoder_output]\n",
        "    \n",
        "    num_samples = input_sequences.shape[0]\n",
        "    \n",
        "    outputs_fn = np.zeros((num_samples,B,max_decoder_seq_length),dtype=np.int32)\n",
        "    \n",
        "    errors_fn = np.zeros((num_samples,B))\n",
        "    \n",
        "    decoder_b_inputs = np.zeros((num_samples,1,1))\n",
        "    decoder_b_inputs[:, :, 0] = start_char\n",
        "    \n",
        "    decoder_b_out = [[(0, [])] for t in range(num_samples)]\n",
        "    errors = [[0] for t in range(num_samples)]\n",
        "    \n",
        "    states = [encoder_output*num_dec_layers]\n",
        "    \n",
        "    for idx in range(max_decoder_seq_length):\n",
        "        all_b_beams = [[] for t in range(num_samples)]\n",
        "        all_decoder_states = [[] for t in range(num_samples)]\n",
        "        all_errors = [[] for t in range(num_samples)]\n",
        "        for b in range(len(decoder_b_out[0])):\n",
        "            decoder_output, *decoder_states = decoder_model.predict([decoder_b_inputs[:,b]] + states[b],batch_size=batch_size)\n",
        "            top_b = np.argsort(decoder_output[:,-1,:],axis=-1)[:,-B:]\n",
        "            for n in range(num_samples):\n",
        "                all_b_beams[n]+= [(decoder_b_out[n][b][0] + np.log(decoder_output[n, -1, top_b[n][i]]),decoder_b_out[n][b][1] + [top_b[n][i]]) for i in range(B)]\n",
        "                if target_sequences is not None:\n",
        "                    all_errors[n] += [errors[n][b] - np.log(decoder_output[n,-1,target_sequences[n,idx]])]*B\n",
        "                all_decoder_states[n] += [[decoder_state[n:n+1] for decoder_state in decoder_states]] * B\n",
        "        sorted_index = list(range(len(all_b_beams[0])))\n",
        "        sorted_index = [sorted(sorted_index,key = lambda ix: all_b_beams[n][ix][0],reverse=True)[:B] for n in range(num_samples)]\n",
        "        decoder_b_out = [[all_b_beams[n][index] for index in sorted_index[n]] for n in range(num_samples)]\n",
        "        \n",
        "        decoder_b_inputs = np.array([[all_b_beams[n][index][-1] for index in sorted_index[n]] for n in range(num_samples)])\n",
        "        \n",
        "        states = [all_decoder_states[0][index] for index in sorted_index[0]]\n",
        "        \n",
        "        for n in range(1,num_samples):\n",
        "            states = [[np.concatenate((states[i][j],all_decoder_states[n][index][j])) for j in range(len(all_decoder_states[n][index]))] for i,index in  enumerate(sorted_index[n])]\n",
        "        if target_sequences is not None:\n",
        "            errors = [[all_errors[n][index] for index in sorted_index[n]] for n in range(num_samples)]\n",
        "    outputs_fn = np.array([[decoder_b_out[n][i][1] for i in range(B)] for n in range(num_samples)])\n",
        "    if target_sequences is not None:\n",
        "        errors_fn = np.array(errors)/max_decoder_seq_length\n",
        "    return outputs_fn,errors_fn,np.array(states)\n",
        "def calc_metrics(b_outputs, target_sequences,token_index,reverse_char_index,b_errors=None,exact_word=True,display=False):\n",
        "    matches = np.mean(b_outputs == np.repeat(target_sequences.reshape((target_sequences.shape[0],1,target_sequences.shape[1])),b_outputs.shape[1],axis=1),axis=-1)\n",
        "    best_b = np.argmax(matches,axis=-1)\n",
        "    best_index = (tuple(range(best_b.shape[0])),tuple(best_b))\n",
        "    accuracy = np.mean(matches[best_index])\n",
        "    b_predictions = list()\n",
        "    loss = None\n",
        "    if b_errors is not None:\n",
        "        loss = np.mean(b_errors[best_index])\n",
        "    if exact_word:\n",
        "        equal = [0] * b_outputs.shape[0]\n",
        "        true_out = num_to_word(target_sequences,token_index,reverse_char_index)\n",
        "        for b in range(b_outputs.shape[1]):\n",
        "            pred_out = num_to_word(b_outputs[:,b], token_index,reverse_char_index)\n",
        "            equal = [equal[i] or (pred_out[i] == true_out[i]) for i in range(b_outputs.shape[0])]\n",
        "            if display==True:\n",
        "                b_predictions.append(pred_out)\n",
        "        exact_accuracy = np.mean(equal)\n",
        "        if display==True:\n",
        "            return accuracy,exact_accuracy,loss,true_out,b_predictions\n",
        "        return accuracy,exact_accuracy,loss\n",
        "    return accuracy,loss\n",
        "def beam_decoder(model,input_sequences,target_sequences_onehot,max_decoder_seq_length,token_index,reverse_char_index,B=1,model_batch_size=64,attention=False,infer_batch_size=512,exact_word=True,return_outputs=False,return_states=False,display=False):\n",
        "    target_sequences = np.argmax(target_sequences_onehot,axis=-1)\n",
        "    b_outputs,b_errors,b_states=None,None,None\n",
        "    for i in range(0,input_sequences.shape[0],infer_batch_size):\n",
        "        tmp_b_outputs,tmp_b_errors,tmp_b_states = beam_decoder_util(model,input_sequences[i:i+infer_batch_size],max_decoder_seq_length,B,target_sequences[i:i+infer_batch_size],token_index['\\t'],model_batch_size)\n",
        "        \n",
        "        if b_errors is None:\n",
        "            b_outputs,b_errors,b_states = tmp_b_outputs,tmp_b_errors,tmp_b_states\n",
        "        else:\n",
        "            b_outputs = np.concatenate((b_outputs,tmp_b_outputs))\n",
        "            b_errors = np.concatenate((b_errors,tmp_b_errors))\n",
        "            b_states = np.concatenate((b_states,tmp_b_states),axis=2)\n",
        "    return_elements = []\n",
        "    if return_outputs:\n",
        "        return_elements += [b_outputs]\n",
        "    if return_states:\n",
        "        return_elements += [b_states]\n",
        "    if len(return_elements) > 0:\n",
        "        return calc_metrics(b_outputs,target_sequences,token_index,reverse_char_index,b_errors,exact_word,display) + tuple(return_elements)\n",
        "    return calc_metrics(b_outputs,target_sequences,target_token_index,reverse_char_index,b_errors,exact_word,display)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8a08c82",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "id": "d8a08c82",
        "outputId": "bcfde2a9-370f-4420-b5bd-df41b18da59a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: lxvyrl4y\n",
            "Sweep URL: https://wandb.ai/dlstack/cs6910_assignment_3/sweeps/lxvyrl4y\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: a77rt0nk with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_size: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinput_embedding_size: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss_function: categorical_crossentropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_dec_layers: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_enc_layers: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.15"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220422_184813-a77rt0nk</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/dlstack/cs6910_assignment_3/runs/a77rt0nk\" target=\"_blank\">prime-sweep-1</a></strong> to <a href=\"https://wandb.ai/dlstack/cs6910_assignment_3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/dlstack/cs6910_assignment_3/sweeps/lxvyrl4y\" target=\"_blank\">https://wandb.ai/dlstack/cs6910_assignment_3/sweeps/lxvyrl4y</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ies_256_nenc_1_ndec_2_cell_LSTM_drop_0_hs_256_B_5\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, None, 256)    6912        ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_2 (Embedding)        (None, None, 256)    16896       ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " encoder_1 (LSTM)               [(None, None, 256),  525312      ['embedding_1[0][0]']            \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " decoder_1 (LSTM)               [(None, None, 256),  525312      ['embedding_2[0][0]',            \n",
            "                                 (None, 256),                     'encoder_1[0][1]',              \n",
            "                                 (None, 256)]                     'encoder_1[0][2]']              \n",
            "                                                                                                  \n",
            " decoder_2 (LSTM)               [(None, None, 256),  525312      ['decoder_1[0][0]',              \n",
            "                                 (None, 256),                     'encoder_1[0][1]',              \n",
            "                                 (None, 256)]                     'encoder_1[0][2]']              \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, None, 66)     16962       ['decoder_2[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,616,706\n",
            "Trainable params: 1,616,706\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/10\n"
          ]
        }
      ],
      "source": [
        "sweep_id = wandb.sweep(sweep_config_1,entity='dlstack',project='cs6910_assignment_3')\n",
        "wandb.agent(sweep_id,lambda:wandb_sweep(),count=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de56242a",
      "metadata": {
        "id": "de56242a"
      },
      "outputs": [],
      "source": [
        "# Define sampling models\n",
        "# Restore the model and construct the encoder and decoder.\n",
        "\n",
        "\n",
        "# encoder_inputs = model.input[0]  # input_1\n",
        "# encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n",
        "# encoder_states = [state_h_enc, state_c_enc]\n",
        "# encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# decoder_inputs = model.input[1]  # input_2\n",
        "# decoder_state_input_h = keras.Input(shape=(latent_dim,))\n",
        "# decoder_state_input_c = keras.Input(shape=(latent_dim,))\n",
        "# decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "# decoder_lstm = model.layers[3]\n",
        "# decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
        "#     decoder_inputs, initial_state=decoder_states_inputs\n",
        "# )\n",
        "# decoder_states = [state_h_dec, state_c_dec]\n",
        "# decoder_dense = model.layers[4]\n",
        "# decoder_outputs = decoder_dense(decoder_outputs)\n",
        "# decoder_model = keras.Model(\n",
        "#     [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
        "# )\n",
        "\n",
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "\n",
        "# encoder_model,decoder_model,num_enc_layers,num_dec_layers=create_inference_model(model)\n",
        "\n",
        "# model = keras.models.load_model(\"s2s\")\n",
        "# outs = beam_decoder(model,val_encoder_input,val_decoder_target,max_decoder_seq_length,target_token_index,reverse_target_char_index,B=4,display=True)\n",
        "# np.array(outs[-2])[np.array(outs[-1][0]) == np.array(outs[-2])]\n",
        "# np.array(val_input_texts)[np.array(outs[-1][0]) == np.array(outs[-2])]"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "name": "cs6910_assignment3.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}