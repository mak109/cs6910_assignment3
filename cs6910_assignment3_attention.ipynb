{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Installing dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T11:03:04.21788Z",
     "iopub.status.busy": "2022-05-02T11:03:04.217501Z",
     "iopub.status.idle": "2022-05-02T11:03:20.077239Z",
     "shell.execute_reply": "2022-05-02T11:03:20.076091Z",
     "shell.execute_reply.started": "2022-05-02T11:03:04.217793Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install wget\n",
    "!pip install --upgrade wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing necessary libraries,packages,etc.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T11:03:20.080009Z",
     "iopub.status.busy": "2022-05-02T11:03:20.079644Z",
     "iopub.status.idle": "2022-05-02T11:03:20.095626Z",
     "shell.execute_reply": "2022-05-02T11:03:20.09458Z",
     "shell.execute_reply.started": "2022-05-02T11:03:20.079972Z"
    }
   },
   "outputs": [],
   "source": [
    "import wget\n",
    "import os\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T11:03:20.099907Z",
     "iopub.status.busy": "2022-05-02T11:03:20.098597Z",
     "iopub.status.idle": "2022-05-02T11:03:20.105071Z",
     "shell.execute_reply": "2022-05-02T11:03:20.10378Z",
     "shell.execute_reply.started": "2022-05-02T11:03:20.099846Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T11:03:20.109488Z",
     "iopub.status.busy": "2022-05-02T11:03:20.108701Z",
     "iopub.status.idle": "2022-05-02T11:03:25.000886Z",
     "shell.execute_reply": "2022-05-02T11:03:24.999938Z",
     "shell.execute_reply.started": "2022-05-02T11:03:20.109439Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T11:03:25.002613Z",
     "iopub.status.busy": "2022-05-02T11:03:25.002314Z",
     "iopub.status.idle": "2022-05-02T11:03:25.802126Z",
     "shell.execute_reply": "2022-05-02T11:03:25.801022Z",
     "shell.execute_reply.started": "2022-05-02T11:03:25.00256Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import SimpleRNN,GRU,LSTM,Embedding,Input,Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T11:03:25.803952Z",
     "iopub.status.busy": "2022-05-02T11:03:25.803641Z",
     "iopub.status.idle": "2022-05-02T11:03:53.555172Z",
     "shell.execute_reply": "2022-05-02T11:03:53.553949Z",
     "shell.execute_reply.started": "2022-05-02T11:03:25.803912Z"
    }
   },
   "outputs": [],
   "source": [
    "#Dataset downloading and extracting\n",
    "filename = 'dakshina_dataset_v1.0'\n",
    "url = 'https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar'\n",
    "if not os.path.exists(filename+'.tar') and not os.path.exists(filename):\n",
    "    filename_tar = wget.download(url)\n",
    "    file = tarfile.open(filename_tar)\n",
    "    print('\\nExtracting files ....')\n",
    "    file.extractall()\n",
    "    file.close()\n",
    "    print('Done')\n",
    "    os.remove(filename_tar)\n",
    "elif not os.path.exists(filename):\n",
    "    filename_tar = filename + '.tar'\n",
    "    file = tarfile.open(filename_tar)\n",
    "    print('\\nExtracting files ....')\n",
    "    file.extractall()\n",
    "    file.close()\n",
    "    print('Done')\n",
    "    os.remove(filename_tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T11:03:53.56468Z",
     "iopub.status.busy": "2022-05-02T11:03:53.561617Z",
     "iopub.status.idle": "2022-05-02T11:03:53.573325Z",
     "shell.execute_reply": "2022-05-02T11:03:53.572221Z",
     "shell.execute_reply.started": "2022-05-02T11:03:53.564595Z"
    }
   },
   "outputs": [],
   "source": [
    "#Paths\n",
    "lang = 'bn'\n",
    "train_path =  filename+f\"/{lang}/lexicons/{lang}.translit.sampled.train.tsv\"\n",
    "val_path = filename+f\"/{lang}/lexicons/{lang}.translit.sampled.dev.tsv\"\n",
    "test_path = filename+f\"/{lang}/lexicons/{lang}.translit.sampled.test.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T11:03:53.582357Z",
     "iopub.status.busy": "2022-05-02T11:03:53.579317Z",
     "iopub.status.idle": "2022-05-02T11:03:53.977521Z",
     "shell.execute_reply": "2022-05-02T11:03:53.976136Z",
     "shell.execute_reply.started": "2022-05-02T11:03:53.582305Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    df = pd.read_csv(path,header=None,sep='\\t')\n",
    "    df.fillna(\"NaN\",inplace=True)\n",
    "    input_texts,target_texts = df[1].to_list(),df[0].to_list()\n",
    "    return input_texts,target_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T11:03:53.982839Z",
     "iopub.status.busy": "2022-05-02T11:03:53.981244Z",
     "iopub.status.idle": "2022-05-02T11:03:53.99569Z",
     "shell.execute_reply": "2022-05-02T11:03:53.994509Z",
     "shell.execute_reply.started": "2022-05-02T11:03:53.982792Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_text(texts):\n",
    "    characters = set()\n",
    "    for text in texts:\n",
    "        for c in text:\n",
    "            if c not in characters:\n",
    "                characters.add(c)\n",
    "    characters.add(' ')\n",
    "    return sorted(list(characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T11:03:54.010162Z",
     "iopub.status.busy": "2022-05-02T11:03:54.008953Z",
     "iopub.status.idle": "2022-05-02T11:03:54.021415Z",
     "shell.execute_reply": "2022-05-02T11:03:54.020184Z",
     "shell.execute_reply.started": "2022-05-02T11:03:54.010106Z"
    }
   },
   "outputs": [],
   "source": [
    "def start_end_pad(texts):\n",
    "    for i in range(len(texts)):\n",
    "        texts[i] = \"\\t\" + texts[i] + \"\\n\"\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T11:03:54.025938Z",
     "iopub.status.busy": "2022-05-02T11:03:54.025116Z",
     "iopub.status.idle": "2022-05-02T11:03:54.363716Z",
     "shell.execute_reply": "2022-05-02T11:03:54.362713Z",
     "shell.execute_reply.started": "2022-05-02T11:03:54.025892Z"
    }
   },
   "outputs": [],
   "source": [
    "#Train test val dataset import raw\n",
    "train_input_texts,train_target_texts = read_data(train_path)\n",
    "val_input_texts,val_target_texts = read_data(val_path)\n",
    "test_input_texts,test_target_texts = read_data(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T11:03:54.372457Z",
     "iopub.status.busy": "2022-05-02T11:03:54.369872Z",
     "iopub.status.idle": "2022-05-02T11:03:54.459599Z",
     "shell.execute_reply": "2022-05-02T11:03:54.458407Z",
     "shell.execute_reply.started": "2022-05-02T11:03:54.372408Z"
    }
   },
   "outputs": [],
   "source": [
    "#Padding at beginning and end with '\\t' and '\\n' respectively\n",
    "train_target_texts = start_end_pad(train_target_texts)\n",
    "val_target_texts = start_end_pad(val_target_texts)\n",
    "test_target_texts = start_end_pad(test_target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T11:03:54.461786Z",
     "iopub.status.busy": "2022-05-02T11:03:54.461194Z",
     "iopub.status.idle": "2022-05-02T11:03:54.474918Z",
     "shell.execute_reply": "2022-05-02T11:03:54.473935Z",
     "shell.execute_reply.started": "2022-05-02T11:03:54.461748Z"
    }
   },
   "outputs": [],
   "source": [
    "#Default Configuration for training\n",
    "config_ = {\n",
    "    \"learning_rate\": 1e-3,                                      # Learning rate in gradient descent\n",
    "    \"epochs\": 10,                                               # Number of epochs to train the model   \n",
    "    \"optimizer\": 'adam',                                        # Gradient descent algorithm used for the parameter updation\n",
    "    \"batch_size\": 64,                                           # Batch size used for the optimizer\n",
    "    \"loss_function\": 'categorical_crossentropy',                # Loss function used in the optimizer                                                                      # Name of dataset\n",
    "    \"input_embedding_size\": 256,                                        # Size of input embedding layer\n",
    "    \"num_enc_layers\": 3,                                         # Number of layers in the encoder\n",
    "    \"num_dec_layers\": 3,                                         # Number of layers in the decoder\n",
    "    \"hidden_layer_size\": 256,                                      # Size of hidden layer\n",
    "    \"dropout\" : 0.30,                                            #Value of dropout used in recurrent dropout\n",
    "    'r_dropout':0.30,                                           # Value of dropout used in recurrent dropout\n",
    "    \"cell_type\": 'GRU',                                         # Type of cell used in the encoder and decoder ('RNN' or 'GRU' or 'LSTM')\n",
    "    \"beam_width\": 1                                          # Beam width used in beam decoder                                        \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T11:03:54.477749Z",
     "iopub.status.busy": "2022-05-02T11:03:54.477109Z",
     "iopub.status.idle": "2022-05-02T11:03:54.543056Z",
     "shell.execute_reply": "2022-05-02T11:03:54.542082Z",
     "shell.execute_reply.started": "2022-05-02T11:03:54.477703Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def enc_dec_tokens(train_input_texts,train_target_texts,val_input_texts,val_target_texts):\n",
    "    #Returns encoding of characters as integer in two dictionary for input and target characters\n",
    "    #Returns number of tokens in input and output\n",
    "    #Returns the maximum sequence length from input and target texts\n",
    "    input_characters = parse_text(train_input_texts + val_input_texts)\n",
    "    target_characters = parse_text(train_target_texts + val_target_texts)\n",
    "    num_encoder_tokens = len(input_characters)\n",
    "    num_decoder_tokens = len(target_characters)\n",
    "    max_encoder_seq_length = max([len(txt) for txt in train_input_texts + val_input_texts])\n",
    "    max_decoder_seq_length = max([len(txt) for txt in train_target_texts + val_target_texts])\n",
    "\n",
    "    print(\"Number of training samples:\", len(train_input_texts))\n",
    "    print(\"Number of validation samples:\", len(val_input_texts))\n",
    "    print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
    "    print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
    "    print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
    "    print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
    "    \n",
    "    input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "    target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
    "    \n",
    "    return input_token_index,target_token_index,max_encoder_seq_length,max_decoder_seq_length,num_encoder_tokens,num_decoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T11:03:54.545967Z",
     "iopub.status.busy": "2022-05-02T11:03:54.544656Z",
     "iopub.status.idle": "2022-05-02T11:03:54.55917Z",
     "shell.execute_reply": "2022-05-02T11:03:54.558246Z",
     "shell.execute_reply.started": "2022-05-02T11:03:54.545904Z"
    }
   },
   "outputs": [],
   "source": [
    "#Data Preprocessing\n",
    "def data_processing(input_texts,enc_length,input_token_index,num_encoder_tokens, target_texts,dec_length,target_token_index,num_decoder_tokens):\n",
    "    # Returns the input and target data in a form needed by the Keras embedding layer (i.e) \n",
    "    # decoder_input & encoder_input -- (None, timesteps) where each character is encoded by an integer\n",
    "    # decoder_output -- (None, timesteps, vocabulary size) where the last dimension is the one-hot encoding\n",
    "\n",
    "    # ' ' -- space (equivalent to no meaningful input / blank input)\n",
    "    encoder_input_data = np.zeros(\n",
    "        (len(input_texts), enc_length), dtype=\"float32\"\n",
    "    )\n",
    "    decoder_input_data = np.zeros(\n",
    "            (len(input_texts), dec_length), dtype=\"float32\"\n",
    "        )\n",
    "    decoder_target_data = np.zeros(\n",
    "            (len(input_texts), dec_length, num_decoder_tokens), dtype=\"float32\"\n",
    "        )\n",
    "\n",
    "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "        \n",
    "        for t, char in enumerate(input_text):\n",
    "            encoder_input_data[i, t] = input_token_index[char]\n",
    "        encoder_input_data[i, t + 1 :] = input_token_index[' ']\n",
    "        \n",
    "        for t, char in enumerate(target_text):\n",
    "                # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "            decoder_input_data[i, t] = target_token_index[char]\n",
    "            if t > 0:\n",
    "                    # decoder_target_data will be ahead by one timestep\n",
    "                    # and will not include the start character.\n",
    "                decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
    "        decoder_input_data[i, t + 1 :] = target_token_index[' ']\n",
    "        decoder_target_data[i, t:, target_token_index[' ']] = 1.0\n",
    "    return encoder_input_data,decoder_input_data,decoder_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T11:03:54.56148Z",
     "iopub.status.busy": "2022-05-02T11:03:54.560967Z",
     "iopub.status.idle": "2022-05-02T11:03:54.744166Z",
     "shell.execute_reply": "2022-05-02T11:03:54.743265Z",
     "shell.execute_reply.started": "2022-05-02T11:03:54.561423Z"
    }
   },
   "outputs": [],
   "source": [
    "input_token_index,target_token_index,max_encoder_seq_length,max_decoder_seq_length,num_encoder_tokens,num_decoder_tokens = enc_dec_tokens(train_input_texts,train_target_texts,val_input_texts,val_target_texts)\n",
    "#Dictionary for reverse lookup of character for its integer encode \n",
    "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T11:03:54.746132Z",
     "iopub.status.busy": "2022-05-02T11:03:54.745417Z",
     "iopub.status.idle": "2022-05-02T11:03:56.791612Z",
     "shell.execute_reply": "2022-05-02T11:03:56.790558Z",
     "shell.execute_reply.started": "2022-05-02T11:03:54.746084Z"
    }
   },
   "outputs": [],
   "source": [
    "#Preprocessed inputs this will be used in training validation and testing in future\n",
    "train_encoder_input,train_decoder_input,train_decoder_target = data_processing(train_input_texts,max_encoder_seq_length,input_token_index,num_encoder_tokens, train_target_texts,max_decoder_seq_length,target_token_index,num_decoder_tokens)\n",
    "val_encoder_input,val_decoder_input,val_decoder_target = data_processing(val_input_texts,max_encoder_seq_length,input_token_index,num_encoder_tokens, val_target_texts,max_decoder_seq_length,target_token_index,num_decoder_tokens)\n",
    "test_encoder_input,test_decoder_input,test_decoder_target = data_processing(test_input_texts,max_encoder_seq_length,input_token_index,num_encoder_tokens, test_target_texts,max_decoder_seq_length,target_token_index,num_decoder_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T11:03:56.795931Z",
     "iopub.status.busy": "2022-05-02T11:03:56.795645Z",
     "iopub.status.idle": "2022-05-02T11:03:56.830078Z",
     "shell.execute_reply": "2022-05-02T11:03:56.829057Z",
     "shell.execute_reply.started": "2022-05-02T11:03:56.795899Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Layer\n",
    "import keras.backend as K\n",
    "\n",
    "class Attention(Layer):\n",
    "    \"\"\"\n",
    "    This Attention layer class code is used from : https://github.com/thushv89/attention_keras/blob/master/src/layers/attention.py\n",
    "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
    "    There are three sets of weights introduced W_a, U_a, and V_a\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        # Create a trainable weight variable for this layer.\n",
    "\n",
    "        self.W_a = self.add_weight(name='W_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.U_a = self.add_weight(name='U_a',\n",
    "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.V_a = self.add_weight(name='V_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "\n",
    "        super(Attention, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, inputs, verbose=False):\n",
    "        \"\"\"\n",
    "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
    "        \"\"\"\n",
    "        assert type(inputs) == list\n",
    "        encoder_out_seq, decoder_out_seq = inputs\n",
    "        if verbose:\n",
    "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
    "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
    "\n",
    "        def energy_step(inputs, states):\n",
    "            \"\"\" Step function for computing energy for a single decoder state\n",
    "            inputs: (batchsize * 1 * de_in_dim)\n",
    "            states: (batchsize * 1 * de_latent_dim)\n",
    "            \"\"\"\n",
    "\n",
    "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
    "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
    "\n",
    "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
    "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
    "            de_hidden = inputs.shape[-1]\n",
    "\n",
    "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
    "            # <= batch size * en_seq_len * latent_dim\n",
    "            W_a_dot_s = K.dot(encoder_out_seq, self.W_a)\n",
    "\n",
    "            \"\"\" Computing hj.Ua \"\"\"\n",
    "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
    "            if verbose:\n",
    "                print('Ua.h>', U_a_dot_h.shape)\n",
    "\n",
    "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n",
    "            if verbose:\n",
    "                print('Ws+Uh>', Ws_plus_Uh.shape)\n",
    "\n",
    "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.softmax(e_i)\n",
    "\n",
    "            if verbose:\n",
    "                print('ei>', e_i.shape)\n",
    "\n",
    "            return e_i, [e_i]\n",
    "\n",
    "        def context_step(inputs, states):\n",
    "            \"\"\" Step function for computing ci using ei \"\"\"\n",
    "\n",
    "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
    "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
    "\n",
    "            # <= batch_size, hidden_size\n",
    "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
    "            if verbose:\n",
    "                print('ci>', c_i.shape)\n",
    "            return c_i, [c_i]\n",
    "\n",
    "        fake_state_c = K.sum(encoder_out_seq, axis=1)\n",
    "        fake_state_e = K.sum(encoder_out_seq, axis=2)  # <= (batch_size, enc_seq_len, latent_dim\n",
    "\n",
    "        \"\"\" Computing energy outputs \"\"\"\n",
    "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
    "        last_out, e_outputs, _ = K.rnn(\n",
    "            energy_step, decoder_out_seq, [fake_state_e],\n",
    "        )\n",
    "\n",
    "        \"\"\" Computing context vectors \"\"\"\n",
    "        last_out, c_outputs, _ = K.rnn(\n",
    "            context_step, e_outputs, [fake_state_c],\n",
    "        )\n",
    "\n",
    "        return c_outputs, e_outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\" Outputs produced by the layer \"\"\"\n",
    "        return [\n",
    "            # (batch_size, decoder_timesteps, decoder_hid_layer_size)\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
    "            # (batch_size, decoder_timesteps, encoder_timesteps)\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T11:03:56.832101Z",
     "iopub.status.busy": "2022-05-02T11:03:56.831587Z",
     "iopub.status.idle": "2022-05-02T11:03:58.634844Z",
     "shell.execute_reply": "2022-05-02T11:03:58.633723Z",
     "shell.execute_reply.started": "2022-05-02T11:03:56.832046Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_model(num_encoder_tokens,num_decoder_tokens,input_embedding_size=64,num_enc_layers=1,num_dec_layers=1,hidden_layer_size=64,cell_type='LSTM',dropout=0,r_dropout=0,cell_activation='tanh'):\n",
    "    '''\n",
    "    Function to create a seq2seq model with attention.\n",
    "    Arguments :\n",
    "        num_encoder_tokens -- (int) number of characters in input vocabulary\n",
    "        num_decoder_tokens -- (int) number of characters in output vocabulary\n",
    "        input_embedding_size -- (int, default : 64) size of input embedding layer for encoder and decoder\n",
    "        num_enc_layers -- (int, default : 1) number of layers of cell to stack in encoder\n",
    "        num_dec_layers -- (int, default : 1) number of layers of cell to stack in decoder\n",
    "        hidden_layer_size -- (int, default : 64) size of hidden layer of the encoder and decoder cells\n",
    "        cell_type -- (string, default : 'LSTM') type of cell used in encoder and decoder (possible values : 'LSTM', 'GRU', 'RNN')\n",
    "        dropout -- (float, default : 0.0) value of normal dropout (between 0 and 1)\n",
    "        r_dropout -- (float, default : 0.0) value of recurrent dropout (between 0 and 1)\n",
    "        cell_activation -- (string, default : 'tanh') type of activation used in the cell (as required by Keras)\n",
    "    Returns :\n",
    "        model -- (Keras model object) resulting attention model\n",
    "    '''\n",
    "    # Getting cell type\n",
    "    cell = {\n",
    "        'RNN':SimpleRNN,\n",
    "        'LSTM':LSTM,\n",
    "        'GRU':GRU\n",
    "    }\n",
    "     # Encoder input and embedding\n",
    "    encoder_input = Input(shape=(None,),name='input_1')\n",
    "    encoder_input_embedding = Embedding(num_encoder_tokens,input_embedding_size,name='embedding_1')(encoder_input)\n",
    "    \n",
    "    encoder_sequences, *encoder_state = cell[cell_type](hidden_layer_size,activation=cell_activation,return_sequences=True,return_state=True,dropout=dropout,recurrent_dropout=r_dropout,name=\"encoder_1\")(encoder_input_embedding)\n",
    "    # Encoder cell layers\n",
    "    for i in range(1,num_enc_layers):\n",
    "        encoder_sequences, *encoder_state = cell[cell_type](hidden_layer_size,activation=cell_activation,return_sequences=True,return_state=True,dropout=dropout,recurrent_dropout=r_dropout,name=f\"encoder_{i+1}\")(encoder_sequences)\n",
    "    \n",
    "    # Decoder input and embedding\n",
    "    decoder_input = Input(shape=(None,),name='input_2')\n",
    "    decoder_input_embedding = Embedding(num_decoder_tokens,input_embedding_size,name='embedding_2')(decoder_input)\n",
    "    \n",
    "    decoder_sequences = decoder_input_embedding\n",
    "    # Decoder cell layers\n",
    "    for i in range(num_dec_layers-1):\n",
    "        decoder_sequences, *decoder_state = cell[cell_type](hidden_layer_size,activation=cell_activation,return_sequences=True,return_state=True,dropout=dropout,recurrent_dropout=r_dropout,name=f\"decoder_{i+1}\")(decoder_sequences ,initial_state=encoder_state)\n",
    "    # Decoder last layer\n",
    "    decoder_sequences, *decoder_state = cell[cell_type](hidden_layer_size,activation=cell_activation,return_sequences=True,return_state=True,dropout=dropout,recurrent_dropout=r_dropout,name=\"decoder_1\")(decoder_input_embedding ,initial_state=encoder_state)\n",
    "    \n",
    "    # Attention layer\n",
    "    attention_out,attention_scores = Attention(name=\"attention_1\")([encoder_sequences,decoder_sequences])\n",
    "    \n",
    "    # Concat attention output and decoder output\n",
    "    dense_concat_input = keras.layers.Concatenate(axis=-1,name='concat_layer_1')([decoder_sequences,attention_out])\n",
    "    # Time distributed Softmax FC layer\n",
    "    decoder_dense = Dense(num_decoder_tokens,activation=\"softmax\",name=\"dense_1\")(dense_concat_input)\n",
    "    \n",
    "    # Define the model that will turn encoder_input_data and decoder_input_data into decoder_target_data\n",
    "    model = keras.Model([encoder_input,decoder_input],decoder_dense)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T11:03:58.63942Z",
     "iopub.status.busy": "2022-05-02T11:03:58.639116Z",
     "iopub.status.idle": "2022-05-02T11:03:58.657483Z",
     "shell.execute_reply": "2022-05-02T11:03:58.65646Z",
     "shell.execute_reply.started": "2022-05-02T11:03:58.639381Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_inference_model(model):\n",
    "    '''\n",
    "    Function to return models needed for inference from the original model (with attention).\n",
    "    Arguments :\n",
    "        model -- (Keras model object) attention model used for training\n",
    "    Returns :\n",
    "        encoder_model -- (Keras model object) \n",
    "        deocder_model -- (Keras model object)\n",
    "        num_enc_layers -- (int) number of layers in the encoder\n",
    "        num_dec_layers -- (int) number of layers in the decoder\n",
    "    '''\n",
    "    # Calculating number of layers in encoder and decoder\n",
    "    num_enc_layers, num_dec_layers = 0, 0\n",
    "    for layer in model.layers:\n",
    "        num_enc_layers += layer.name.startswith('encoder')\n",
    "        num_dec_layers += layer.name.startswith('decoder')\n",
    "\n",
    "    # Encoder input\n",
    "    encoder_input = model.input[0]      # Input_1\n",
    "    # Encoder cell final layer\n",
    "    encoder_cell = model.get_layer(\"encoder_\"+str(num_enc_layers))\n",
    "    encoder_type = encoder_cell.__class__.__name__\n",
    "    encoder_sequences, *encoder_state = encoder_cell.output\n",
    "    # Encoder model\n",
    "    encoder_model = keras.Model(encoder_input, encoder_state)\n",
    "\n",
    "    # Decoder input\n",
    "    decoder_input = model.input[1]      # Input_2\n",
    "    decoder_input_embedding = model.get_layer(\"embedding_2\")(decoder_input)\n",
    "    decoder_sequences = decoder_input_embedding\n",
    "    # Inputs to decoder layers' initial states\n",
    "    decoder_states, decoder_state_inputs = [], []\n",
    "    for i in range(1, num_dec_layers+1):\n",
    "        if encoder_type == 'LSTM':\n",
    "            decoder_state_input = [Input(shape=(encoder_state[0].shape[1],), name=\"input_\"+str(2*i+1)), \n",
    "                                   Input(shape=(encoder_state[1].shape[1],), name=\"input_\"+str(2*i+2))]\n",
    "        else:\n",
    "            decoder_state_input = [Input(shape=(encoder_state[0].shape[1],), name=\"input_\"+str(i+2))]\n",
    "\n",
    "        decoder_cell = model.get_layer(\"decoder_\"+str(i))\n",
    "        decoder_sequences, *decoder_state = decoder_cell(decoder_sequences, initial_state=decoder_state_input)\n",
    "        decoder_states += decoder_state\n",
    "        decoder_state_inputs += decoder_state_input\n",
    "    # Attention layer\n",
    "    attention_out,attention_scores = model.get_layer(\"attention_1\")([encoder_sequences,decoder_sequences])\n",
    "    \n",
    "    dense_concat_input = keras.layers.Concatenate(axis=-1,name='concat_layer_1')([decoder_sequences,attention_out])\n",
    "    # Softmax FC layer\n",
    "    decoder_dense = model.get_layer(\"dense_1\")\n",
    "    decoder_dense_output = decoder_dense(dense_concat_input)\n",
    "\n",
    "    # Decoder model\n",
    "    decoder_model = keras.Model(\n",
    "        [encoder_input,decoder_input] + decoder_state_inputs, [attention_scores,decoder_dense_output] + decoder_states\n",
    "    )\n",
    "\n",
    "    return encoder_model, decoder_model, num_enc_layers, num_dec_layers\n",
    "\n",
    "\n",
    "def num_to_word(num_encoded, token_index, reverse_char_index = None):\n",
    "    # Function to return the predictions after cutting the '\\n' and ' ' s at the end.\n",
    "    # If reverse_char_index == None, the predictions are in the form of decoded string, otherwise as list of integers\n",
    "    num_samples = len(num_encoded) if type(num_encoded) is list else num_encoded.shape[0]\n",
    "    predicted_words = ['' for t in range(num_samples)]\n",
    "    for i, encode in enumerate(num_encoded):\n",
    "        for l in encode:\n",
    "            # Stop word : '\\n'\n",
    "            if l == token_index['\\n']:\n",
    "                break\n",
    "            predicted_words[i] += reverse_char_index[l] if reverse_char_index is not None else str(l)\n",
    "    \n",
    "    return predicted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T11:03:58.659973Z",
     "iopub.status.busy": "2022-05-02T11:03:58.659389Z",
     "iopub.status.idle": "2022-05-02T11:03:58.702709Z",
     "shell.execute_reply": "2022-05-02T11:03:58.701516Z",
     "shell.execute_reply.started": "2022-05-02T11:03:58.659924Z"
    }
   },
   "outputs": [],
   "source": [
    "def beam_decoder_util(model,input_sequences,max_decoder_seq_length,B=1,target_sequences=None,start_char=0,batch_size=64):\n",
    "    '''\n",
    "    Function to do inference on the model using beam decoder.\n",
    "    Arguments :\n",
    "        model -- (Keras model object) training model\n",
    "        input_sequences -- (numpy ndarray of size : (None, timesteps)) input to encoder\n",
    "        max_decoder_seq_length -- (int) Number of timesteps to infer in decoder\n",
    "        B -- (int, default : 1) beam width of beam decoder\n",
    "        target_sequences -- (numpy ndarray of size : (None, timesteps, num_decoder_tokens), deault : None) expected target.\n",
    "                       If None, cross entropy errors won't be calculated.\n",
    "        start_char -- (int, default : 0) Encoding integer for ' '(start char)\n",
    "        batch_size -- (int, default : 64) batch_size sent to Keras predict\n",
    "    Returns :\n",
    "        final_outputs -- (numpy ndarray of size : (None, B, timesteps)) top B output sequences\n",
    "        final_errors -- (numpy ndarray of size : (None, B)) cross entropy errors for top B output (All zeros if target_seqs == None)\n",
    "        states_values -- (numpy ndarray of size : (, None, timesteps, hidden_layer_size))  hidden states of decoder\n",
    "        final_attn_scores -- (numpy ndarray of size : (None, B, decoder_timesteps(max_decoder_seq_length), encoder_timesteps(max_encoder_seq_length))) attention to all encoder timesteps for a decoder timestep \n",
    "    '''\n",
    "    # Generating output from encoder\n",
    "    encoder_model,decoder_model,num_enc_layers,num_dec_layers=make_inference_model(model)\n",
    "    encoder_output = encoder_model.predict(input_sequences,batch_size=batch_size)\n",
    "    encoder_output = encoder_output if type(encoder_output) is list else [encoder_output]\n",
    "    # Number of input samples in the data passed\n",
    "    num_samples = input_sequences.shape[0]\n",
    "    # Top B output sequences for each input \n",
    "    outputs_fn = np.zeros((num_samples,B,max_decoder_seq_length),dtype=np.int32)\n",
    "    # Errors for top B output sequences for each input\n",
    "    errors_fn = np.zeros((num_samples,B))\n",
    "    \n",
    "    # decoder input sequence for 1 timestep (for all samples). Initially one choice only there\n",
    "    decoder_b_inputs = np.zeros((num_samples,1,1))\n",
    "    # Populate the input sequence with the start character at the 1st timestep\n",
    "    decoder_b_inputs[:, :, 0] = start_char\n",
    "    \n",
    "    # (log(probability) sequence, decoder output sequence) pairs for all choices and all samples. Probability starts with log(1) = 0\n",
    "    decoder_b_out = [[(0, [])] for t in range(num_samples)]\n",
    "    # Categorical cross entropy error in the sequence for all choice and all samples\n",
    "    errors = [[0] for t in range(num_samples)]\n",
    "    # Output states from decoder for all choices, and all samples\n",
    "    states = [encoder_output*num_dec_layers]\n",
    "    # Attention weights output\n",
    "    attn_b_scores = [[None] for t in range(num_samples)]\n",
    "    \n",
    "    # Sampling loop\n",
    "    for idx in range(max_decoder_seq_length):\n",
    "        # Storing respective data for all possibilities\n",
    "        all_b_beams = [[] for t in range(num_samples)]\n",
    "        all_decoder_states = [[] for t in range(num_samples)]\n",
    "        all_errors = [[] for t in range(num_samples)]\n",
    "        all_attn_scores = [[] for t in range(num_samples)]\n",
    "        for b in range(len(decoder_b_out[0])):\n",
    "            attn_scores,decoder_output, *decoder_states = decoder_model.predict([input_sequences,decoder_b_inputs[:,b]] + states[b],batch_size=batch_size)\n",
    "            # Top B scores\n",
    "            top_b = np.argsort(decoder_output[:,-1,:],axis=-1)[:,-B:]\n",
    "            for n in range(num_samples):\n",
    "                all_b_beams[n]+= [(decoder_b_out[n][b][0] + np.log(decoder_output[n, -1, top_b[n][i]]),decoder_b_out[n][b][1] + [top_b[n][i]]) for i in range(B)]\n",
    "                all_attn_scores[n] += [attn_scores[n]]*B if attn_b_scores[n][b] is None else [np.concatenate((attn_b_scores[n][b],attn_scores[n]),axis=0)]*B\n",
    "                if target_sequences is not None:\n",
    "                    all_errors[n] += [errors[n][b] - np.log(decoder_output[n,-1,target_sequences[n,idx]])]*B\n",
    "                all_decoder_states[n] += [[decoder_state[n:n+1] for decoder_state in decoder_states]] * B\n",
    "        # Sort and choose top B with max probabilities\n",
    "        sorted_index = list(range(len(all_b_beams[0])))\n",
    "        sorted_index = [sorted(sorted_index,key = lambda ix: all_b_beams[n][ix][0])[-B:][::-1] for n in range(num_samples)]\n",
    "        # Choose the top B decoder output sequences till now\n",
    "        decoder_b_out = [[all_b_beams[n][index] for index in sorted_index[n]] for n in range(num_samples)]\n",
    "        \n",
    "        # Update the input sequence for next 1 timestep\n",
    "        decoder_b_inputs = np.array([[all_b_beams[n][index][1][-1] for index in sorted_index[n]] for n in range(num_samples)])\n",
    "        # Update states\n",
    "        states = [all_decoder_states[0][index] for index in sorted_index[0]]\n",
    "        for n in range(1,num_samples):\n",
    "            states = [[np.concatenate((states[i][j],all_decoder_states[n][index][j])) for j in range(len(all_decoder_states[n][index]))] for i,index in  enumerate(sorted_index[n])]\n",
    "        # Update attention scores\n",
    "        attn_b_scores = [[all_attn_scores[n][index] for index in sorted_index[n]] for n in range(num_samples)]    \n",
    "        if target_sequences is not None:\n",
    "            errors = [[all_errors[n][index] for index in sorted_index[n]] for n in range(num_samples)]\n",
    "            \n",
    "    outputs_fn = np.array([[decoder_b_out[n][i][1] for i in range(B)] for n in range(num_samples)])\n",
    "    # Update errors\n",
    "    if target_sequences is not None:\n",
    "        errors_fn = np.array(errors)/max_decoder_seq_length\n",
    "    return outputs_fn,errors_fn,np.array(states),np.array(attn_b_scores)\n",
    "\n",
    "def calc_metrics(b_outputs, target_sequences,token_index,reverse_char_index,b_errors=None,exact_word=True,display=False):\n",
    "    # Calculates the accuracy (and mean error if info provided) for the best of B possible output sequences\n",
    "    # target_sequencess -- Expected output (encoded sequence)\n",
    "    # b_outputs -- b choices of output sequences for each sample\n",
    "    matches = np.mean(b_outputs == target_sequences.reshape((target_sequences.shape[0],1,target_sequences.shape[1])),axis=-1)\n",
    "    best_b = np.argmax(matches,axis=-1)\n",
    "    best_index = (tuple(range(best_b.shape[0])),tuple(best_b))\n",
    "    accuracy = np.mean(matches[best_index])\n",
    "    b_predictions = list()\n",
    "    loss = None\n",
    "    if b_errors is not None:\n",
    "        loss = np.mean(b_errors[best_index])\n",
    "    if exact_word:\n",
    "        equal = [0] * b_outputs.shape[0]\n",
    "        true_out = num_to_word(target_sequences,token_index,reverse_char_index)\n",
    "        for b in range(b_outputs.shape[1]):\n",
    "            pred_out = num_to_word(b_outputs[:,b], token_index,reverse_char_index)\n",
    "            equal = [equal[i] or (pred_out[i] == true_out[i]) for i in range(b_outputs.shape[0])]\n",
    "            if display==True:\n",
    "                b_predictions.append(pred_out)\n",
    "        exact_accuracy = np.mean(equal)\n",
    "        if display==True:\n",
    "            return accuracy,exact_accuracy,loss,true_out,b_predictions\n",
    "        return accuracy,exact_accuracy,loss\n",
    "    return accuracy,loss\n",
    "def beam_decoder(model,input_sequences,target_sequences_onehot,max_decoder_seq_length,token_index,reverse_char_index,B=1,model_batch_size=64,infer_batch_size=512,exact_word=True,return_outputs=False,return_states=False,return_attention=False,display=False):\n",
    "    '''\n",
    "    Function to calculate/capture character-wise accuracy, exact-word-match accuracy, and loss for the seq2seq model using a beam decoder.\n",
    "    Arguments :\n",
    "        model -- (Keras model object) model used for training\n",
    "        input_sequences -- (numpy ndarray of size : (None, timesteps)) input to encoder (where characters are encoded as integers)\n",
    "        target_sequences -- (numpy ndarray of size : (None, timesteps, num_decoder_tokens)) expected target in onehot format\n",
    "        max_decoder_seq_length -- (int) Number of timesteps to infer in decoder\n",
    "        token_index -- (dict) target character encoding\n",
    "        reverse_char_index -- (dict) target character decoding\n",
    "        B -- (int, default : 1) beam width to be used in beam decoder\n",
    "        model_batch_size -- (int, default : 64) batch size to be used while evaluating model using Keras\n",
    "        infer_batch_size -- (int, default : 512) number of samples to be sent to beam_decoder_infer() at a time (to avoid RAM memory overshoot problems).\n",
    "                            We have set the default model_batch_size and infer_batch_size such that it takes the least time to run and runs without problems.\n",
    "        exact_word -- (bool, default : True) whether or not exact_accuracy has (If True, will be returned as the next argument after accuracy)\n",
    "        return_outputs -- (bool, default : True) whether or not the outputs predicted need to be returned\n",
    "        return_states -- (bool, default : True) whether or not the decoder hidden states need to be returned (for further training, another sequential model addition, etc)\n",
    "        return_attn_scores -- (bool, default : True) whether or not the attention scores need to be returned\n",
    "    Returns :\n",
    "        accuracy -- (float) the character-wise match accuracy (as calculated by Keras fit)\n",
    "        (If exact_word is True) exact_accuracy -- (float) the exact word match accuracy\n",
    "        loss -- (float) the cross-entropy loss for the top B predictions\n",
    "        (If return_outputs is True) b_outputs -- (numpy ndarray of size : (None, B, timesteps)) top B output sequences\n",
    "        (If return_states is True) b_states -- (numpy ndarray of size : (B, None, timesteps, hidden_layer_size))  hidden states of decoder\n",
    "        (If return_attn_scores is True) b_attn_scores -- (numpy ndarray of size : (None, B, decoder_timesteps, encoder_timesteps)) attention scores\n",
    "    '''\n",
    "    target_sequences = np.argmax(target_sequences_onehot,axis=-1)\n",
    "    b_outputs,b_errors,b_states,b_attention=None,None,None,None\n",
    "    for i in range(0,input_sequences.shape[0],infer_batch_size):\n",
    "        tmp_b_outputs,tmp_b_errors,tmp_b_states,tmp_b_attention = beam_decoder_util(model,input_sequences[i:i+infer_batch_size],max_decoder_seq_length,B,target_sequences[i:i+infer_batch_size],token_index['\\t'],model_batch_size)\n",
    "        \n",
    "        if b_errors is None:\n",
    "            b_outputs,b_errors,b_states,b_attention = tmp_b_outputs,tmp_b_errors,tmp_b_states,tmp_b_attention\n",
    "        else:\n",
    "            b_outputs = np.concatenate((b_outputs,tmp_b_outputs))\n",
    "            b_errors = np.concatenate((b_errors,tmp_b_errors))\n",
    "            b_states = np.concatenate((b_states,tmp_b_states),axis=2)\n",
    "            b_attention = np.concatenate((b_attention,tmp_b_attention))\n",
    "    return_elements = []\n",
    "    if return_outputs:\n",
    "        return_elements += [b_outputs]\n",
    "    if return_states:\n",
    "        return_elements += [b_states]\n",
    "    if return_attention:\n",
    "        return_elements += [b_attention]\n",
    "    if len(return_elements) > 0:\n",
    "        return calc_metrics(b_outputs,target_sequences,token_index,reverse_char_index,b_errors,exact_word,display) + tuple(return_elements)\n",
    "    return calc_metrics(b_outputs,target_sequences,target_token_index,reverse_char_index,b_errors,exact_word,display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T11:03:58.720253Z",
     "iopub.status.busy": "2022-05-02T11:03:58.719891Z",
     "iopub.status.idle": "2022-05-02T11:03:59.105492Z",
     "shell.execute_reply": "2022-05-02T11:03:59.104582Z",
     "shell.execute_reply.started": "2022-05-02T11:03:58.720188Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Nadam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy,sparse_categorical_crossentropy\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy,CategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "def model_train_util(config):\n",
    "    #utility function to build and compile model based on the configuration passed as input and return the model\n",
    "    model = make_model(num_encoder_tokens,num_decoder_tokens,config['input_embedding_size'],config['num_enc_layers'],config['num_dec_layers'],config['hidden_layer_size'],config['cell_type'],config['dropout'],config['r_dropout'])\n",
    "    optimizer = config['optimizer']\n",
    "    if config['loss_function'] == 'categorical_crossentropy':\n",
    "        loss_fn = CategoricalCrossentropy\n",
    "    else:\n",
    "        loss_fn = SparseCategoricalCrossentropy\n",
    "    if optimizer == 'adam':\n",
    "        model.compile(optimizer = Adam(learning_rate=config['learning_rate']), loss = loss_fn(), metrics = ['accuracy'])\n",
    "    elif optimizer == 'momentum':\n",
    "        model.compile(optimizer = SGD(learning_rate=config['learning_rate'], momentum = 0.9), loss = loss_fn(), metrics = ['accuracy'])\n",
    "    elif optimizer == 'rmsprop':\n",
    "        model.compile(optimizer = RMSprop(learning_rate=config['learning_rate']), loss = loss_fn(), metrics = ['accuracy'])\n",
    "    elif optimizer == 'nesterov':\n",
    "        model.compile(optimizer = SGD(learning_rate=config['learning_rate'], momentum = 0.9, nesterov = True), loss = loss_fn(), metrics = ['accuracy'])\n",
    "    elif optimizer == 'nadam':\n",
    "        model.compile(optimizer = Nadam(learning_rate=config['learning_rate']), loss = loss_fn(), metrics = ['accuracy'])\n",
    "    else:\n",
    "        model.compile(optimizer = SGD(learning_rate=config['learning_rate']), loss = loss_fn(), metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T11:03:59.109455Z",
     "iopub.status.busy": "2022-05-02T11:03:59.109191Z",
     "iopub.status.idle": "2022-05-02T11:03:59.122424Z",
     "shell.execute_reply": "2022-05-02T11:03:59.118492Z",
     "shell.execute_reply.started": "2022-05-02T11:03:59.109425Z"
    }
   },
   "outputs": [],
   "source": [
    "class customCallback(keras.callbacks.Callback):\n",
    "     # Custom class to provide callback after each epoch of training to calculate custom metrics for validation set with beam decoder\n",
    "    def __init__(self, val_enc_input, val_dec_target, beam_width=1, batch_size=64):\n",
    "        self.beam_width = beam_width\n",
    "        self.validation_input = val_enc_input\n",
    "        self.validation_target = val_dec_target\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        val_accuracy, val_exact_accuracy, val_loss = beam_decoder(self.model, self.validation_input, self.validation_target, max_decoder_seq_length, \n",
    "                                                                  target_token_index, reverse_target_char_index, self.beam_width, self.batch_size)\n",
    "\n",
    "        # Log them to reflect in WANDB callback and EarlyStopping\n",
    "        logs[\"val_accuracy\"] = val_accuracy\n",
    "        logs[\"val_exact_accuracy\"] = val_exact_accuracy\n",
    "        logs[\"val_loss\"] = val_loss             # Validation loss calculates categorical cross entropy loss\n",
    "\n",
    "        print(\"— val_loss: {:.3f} — val_accuracy: {:.3f} — val_exact_accuracy: {:.5f}\".format(val_loss, val_accuracy, val_exact_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T11:03:59.141392Z",
     "iopub.status.busy": "2022-05-02T11:03:59.140962Z",
     "iopub.status.idle": "2022-05-02T11:04:14.876736Z",
     "shell.execute_reply": "2022-05-02T11:04:14.875635Z",
     "shell.execute_reply.started": "2022-05-02T11:03:59.141344Z"
    }
   },
   "outputs": [],
   "source": [
    "#Wandb import and authentication\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "wandb.login(key='b44266d937596fcef83bedbe7330d6cee108a277')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T11:04:14.879625Z",
     "iopub.status.busy": "2022-05-02T11:04:14.879231Z",
     "iopub.status.idle": "2022-05-02T11:04:14.892839Z",
     "shell.execute_reply": "2022-05-02T11:04:14.891434Z",
     "shell.execute_reply.started": "2022-05-02T11:04:14.87956Z"
    }
   },
   "outputs": [],
   "source": [
    "def model_train(config,iswandb=False):\n",
    "    '''\n",
    "    Arguments:\n",
    "        config -- (dict) Hyperparameter config with which model is trained\n",
    "        iswandb -- (bool,default:False) If True training is done by logging metrics to wandb otherwise training is done as it is\n",
    "    Returns:\n",
    "        wid: Wandb run id if any \n",
    "        model - Keras model\n",
    "        history - metrics of training\n",
    "        config - Same as passed in argument\n",
    "    '''\n",
    "    \n",
    "    wid = None\n",
    "    if iswandb:\n",
    "        wid = wandb.util.generate_id()\n",
    "        run = wandb.init(id = wid, project=\"cs6910_assignment_3_attention\", entity=\"dlstack\", reinit=True, config=config)\n",
    "        wandb.run.name = f\"ies_{config['input_embedding_size']}_nenc_{config['num_enc_layers']}_ndec_{config['num_dec_layers']}_cell_{config['cell_type']}_drop_{config['dropout']}_rdrop{config['r_dropout']}\"\n",
    "        wandb.run.name += f\"_hs_{config['hidden_layer_size']}_B_{config['beam_width']}_attn\"\n",
    "        wandb.run.save()\n",
    "        print(wandb.run.name)\n",
    "\n",
    "    model = model_train_util(config)\n",
    "    if iswandb:\n",
    "        call_list = [customCallback(val_encoder_input,val_decoder_target,beam_width=config['beam_width'],batch_size=config['batch_size']),WandbCallback(monitor='val_accuracy'),EarlyStopping(monitor='val_accuracy',patience=4)]\n",
    "    else:\n",
    "        call_list = [customCallback(val_encoder_input,val_decoder_target,beam_width=config['beam_width'],batch_size=config['batch_size']),EarlyStopping(monitor='val_accuracy',patience=4)]\n",
    "    history = model.fit(\n",
    "        [train_encoder_input,train_decoder_input],\n",
    "        train_decoder_target,\n",
    "        batch_size=config['batch_size'],\n",
    "        verbose = 1,\n",
    "        epochs=config['epochs'],\n",
    "        callbacks = call_list\n",
    "    )    \n",
    "    if iswandb:\n",
    "        run.finish()\n",
    "\n",
    "    return model, history,config, wid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T11:04:14.902171Z",
     "iopub.status.busy": "2022-05-02T11:04:14.901932Z",
     "iopub.status.idle": "2022-05-02T11:04:14.909573Z",
     "shell.execute_reply": "2022-05-02T11:04:14.908423Z",
     "shell.execute_reply.started": "2022-05-02T11:04:14.902141Z"
    }
   },
   "outputs": [],
   "source": [
    "def wandb_sweep():\n",
    "    # Wrapper function to call the model_train() function for sweeping with different hyperparameters\n",
    "\n",
    "    # Initialize a new wandb run\n",
    "    run = wandb.init(config=config_, reinit=True)\n",
    "\n",
    "    # Config is a variable that holds and saves hyperparameters and inputs\n",
    "    config = wandb.config\n",
    "\n",
    "    wandb.run.name = f'ies_{config.input_embedding_size}_nenc_{config.num_enc_layers}_ndec_{config.num_dec_layers}_cell_{config.cell_type}_drop_{config.dropout}_rdrop_{config.r_dropout}'\n",
    "    wandb.run.name += f'_hs_{config.hidden_layer_size}_B_{config.beam_width}_attn'\n",
    "    wandb.run.save()\n",
    "    print(wandb.run.name)\n",
    "\n",
    "    model, *_ = model_train(config, iswandb=True)\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T11:03:59.125133Z",
     "iopub.status.busy": "2022-05-02T11:03:59.124381Z",
     "iopub.status.idle": "2022-05-02T11:03:59.139461Z",
     "shell.execute_reply": "2022-05-02T11:03:59.138525Z",
     "shell.execute_reply.started": "2022-05-02T11:03:59.125018Z"
    }
   },
   "outputs": [],
   "source": [
    "# model,history,config,_ = model_train(config_)\n",
    "# model.save(\"s2s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T11:04:14.912099Z",
     "iopub.status.busy": "2022-05-02T11:04:14.911615Z",
     "iopub.status.idle": "2022-05-02T12:14:20.306744Z",
     "shell.execute_reply": "2022-05-02T12:14:20.305773Z",
     "shell.execute_reply.started": "2022-05-02T11:04:14.912058Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameter choices to sweep \n",
    "sweep_config_1 = {\n",
    "    'name': 'RNNs2s_attn',\n",
    "    'method': 'bayes',                   # Possible search : grid, random, bayes\n",
    "    'metric': {\n",
    "      'name': 'val_accuracy',\n",
    "      'goal': 'maximize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'epochs':{\n",
    "            'values':[10,15,20]\n",
    "        },\n",
    "        'learning_rate':{\n",
    "            'values':[0.001,0.0001,0.005]\n",
    "        },\n",
    "        'optimizer':{\n",
    "            'value':'adam'\n",
    "        },\n",
    "        'loss_function':{\n",
    "          'value':'categorical_crossentropy'  \n",
    "        },\n",
    "        'input_embedding_size': {\n",
    "            'values': [64,128,256]\n",
    "        },\n",
    "        'num_enc_layers': {\n",
    "            'values': [2,3]\n",
    "        },\n",
    "        'num_dec_layers': {\n",
    "            'values': [3,5]\n",
    "        },\n",
    "        'hidden_layer_size': {\n",
    "            'values': [256,512,768]\n",
    "        },\n",
    "        'cell_type': {\n",
    "            'values': ['GRU']\n",
    "        },\n",
    "        'dropout' :{\n",
    "            'values': [0.20,0.30]\n",
    "        },\n",
    "        'r_dropout': {\n",
    "            'values': [0.20,0.30]\n",
    "        },\n",
    "        'beam_width': {\n",
    "            'values': [1,3,5]\n",
    "        },\n",
    "        'batch_size':{\n",
    "            'values':[128,256]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "sweep_id = wandb.sweep(sweep_config_1,entity='dlstack',project='cs6910_assignment_3_attention')\n",
    "wandb.agent(sweep_id,lambda:wandb_sweep(),count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter choices to sweep \n",
    "sweep_config_1 = {\n",
    "    'name': 'RNNs2s_attn',\n",
    "    'method': 'bayes',                   # Possible search : grid, random, bayes\n",
    "    'metric': {\n",
    "      'name': 'val_accuracy',\n",
    "      'goal': 'maximize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'epochs':{\n",
    "            'values':[10,15,20]\n",
    "        },\n",
    "        'learning_rate':{\n",
    "            'values':[0.001,0.0001,0.005]\n",
    "        },\n",
    "        'optimizer':{\n",
    "            'value':'adam'\n",
    "        },\n",
    "        'loss_function':{\n",
    "          'value':'categorical_crossentropy'  \n",
    "        },\n",
    "        'input_embedding_size': {\n",
    "            'values': [64,128,256]\n",
    "        },\n",
    "        'num_enc_layers': {\n",
    "            'values': [2,3]\n",
    "        },\n",
    "        'num_dec_layers': {\n",
    "            'values': [3,5]\n",
    "        },\n",
    "        'hidden_layer_size': {\n",
    "            'values': [256,512,768]\n",
    "        },\n",
    "        'cell_type': {\n",
    "            'values': ['GRU']\n",
    "        },\n",
    "        'dropout' :{\n",
    "            'values': [0.20,0.30]\n",
    "        },\n",
    "        'r_dropout': {\n",
    "            'values': [0.20,0.30]\n",
    "        },\n",
    "        'beam_width': {\n",
    "            'values': [1,3,5]\n",
    "        },\n",
    "        'batch_size':{\n",
    "            'values':[128,256]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "sweep_id = wandb.sweep(sweep_config_1,entity='dlstack',project='cs6910_assignment_3_attention')\n",
    "wandb.agent(sweep_id,lambda:wandb_sweep(),count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter choices to sweep \n",
    "sweep_config_1 = {\n",
    "    'name': 'RNNs2s_attn',\n",
    "    'method': 'bayes',                   # Possible search : grid, random, bayes\n",
    "    'metric': {\n",
    "      'name': 'val_accuracy',\n",
    "      'goal': 'maximize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'epochs':{\n",
    "            'values':[10,15,20]\n",
    "        },\n",
    "        'learning_rate':{\n",
    "            'values':[1e-3,1e-4]\n",
    "        },\n",
    "        'optimizer':{\n",
    "            'values':['rmsprop','adam','nadam','nesterov','sgd']\n",
    "        },\n",
    "        'loss_function':{\n",
    "          'value':'categorical_crossentropy'  \n",
    "        },\n",
    "        'input_embedding_size': {\n",
    "            'values': [32, 64,256]\n",
    "        },\n",
    "        'num_enc_layers': {\n",
    "            'values': [ 2, 3,4]\n",
    "        },\n",
    "        'num_dec_layers': {\n",
    "            'values': [ 2, 3,4]\n",
    "        },\n",
    "        'hidden_layer_size': {\n",
    "            'values': [64, 128, 256,512]\n",
    "        },\n",
    "        'cell_type': {\n",
    "            'values': ['RNN', 'LSTM', 'GRU']\n",
    "        },\n",
    "        'dropout' :{\n",
    "            'values': [0, 0.25, 0.3,0.4]\n",
    "        },\n",
    "        'r_dropout':{\n",
    "          'values':[0.0,0.1]  \n",
    "        },\n",
    "        'batch_size':{\n",
    "          'values':[64,128,256]  \n",
    "        },\n",
    "        'beam_width': {\n",
    "            'values': [1, 5]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "sweep_id = wandb.sweep(sweep_config_1,entity='dlstack',project='cs6910_assignment_3_attention')\n",
    "wandb.agent(sweep_id,lambda:wandb_sweep(),count=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
