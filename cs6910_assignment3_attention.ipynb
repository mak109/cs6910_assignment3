{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install wget","metadata":{"execution":{"iopub.status.busy":"2022-05-02T11:03:04.217501Z","iopub.execute_input":"2022-05-02T11:03:04.21788Z","iopub.status.idle":"2022-05-02T11:03:20.077239Z","shell.execute_reply.started":"2022-05-02T11:03:04.217793Z","shell.execute_reply":"2022-05-02T11:03:20.076091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wget\nimport os\nimport tarfile","metadata":{"execution":{"iopub.status.busy":"2022-05-02T11:03:20.079644Z","iopub.execute_input":"2022-05-02T11:03:20.080009Z","iopub.status.idle":"2022-05-02T11:03:20.095626Z","shell.execute_reply.started":"2022-05-02T11:03:20.079972Z","shell.execute_reply":"2022-05-02T11:03:20.09458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2022-05-02T11:03:20.098597Z","iopub.execute_input":"2022-05-02T11:03:20.099907Z","iopub.status.idle":"2022-05-02T11:03:20.105071Z","shell.execute_reply.started":"2022-05-02T11:03:20.099846Z","shell.execute_reply":"2022-05-02T11:03:20.10378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras","metadata":{"execution":{"iopub.status.busy":"2022-05-02T11:03:20.108701Z","iopub.execute_input":"2022-05-02T11:03:20.109488Z","iopub.status.idle":"2022-05-02T11:03:25.000886Z","shell.execute_reply.started":"2022-05-02T11:03:20.109439Z","shell.execute_reply":"2022-05-02T11:03:24.999938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.layers import SimpleRNN,GRU,LSTM,Embedding,Input,Dense","metadata":{"execution":{"iopub.status.busy":"2022-05-02T11:03:25.002314Z","iopub.execute_input":"2022-05-02T11:03:25.002613Z","iopub.status.idle":"2022-05-02T11:03:25.802126Z","shell.execute_reply.started":"2022-05-02T11:03:25.00256Z","shell.execute_reply":"2022-05-02T11:03:25.801022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename = 'dakshina_dataset_v1.0'\nurl = 'https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar'\nif not os.path.exists(filename+'.tar') and not os.path.exists(filename):\n    filename_tar = wget.download(url)\n    file = tarfile.open(filename_tar)\n    print('\\nExtracting files ....')\n    file.extractall()\n    file.close()\n    print('Done')\n    os.remove(filename_tar)\nelif not os.path.exists(filename):\n    filename_tar = filename + '.tar'\n    file = tarfile.open(filename_tar)\n    print('\\nExtracting files ....')\n    file.extractall()\n    file.close()\n    print('Done')\n    os.remove(filename_tar)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T11:03:25.803641Z","iopub.execute_input":"2022-05-02T11:03:25.803952Z","iopub.status.idle":"2022-05-02T11:03:53.555172Z","shell.execute_reply.started":"2022-05-02T11:03:25.803912Z","shell.execute_reply":"2022-05-02T11:03:53.553949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lang = 'bn'\ntrain_path =  filename+f\"/{lang}/lexicons/{lang}.translit.sampled.train.tsv\"\nval_path = filename+f\"/{lang}/lexicons/{lang}.translit.sampled.dev.tsv\"\ntest_path = filename+f\"/{lang}/lexicons/{lang}.translit.sampled.test.tsv\"","metadata":{"execution":{"iopub.status.busy":"2022-05-02T11:03:53.561617Z","iopub.execute_input":"2022-05-02T11:03:53.56468Z","iopub.status.idle":"2022-05-02T11:03:53.573325Z","shell.execute_reply.started":"2022-05-02T11:03:53.564595Z","shell.execute_reply":"2022-05-02T11:03:53.572221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_data(path):\n    df = pd.read_csv(path,header=None,sep='\\t')\n    df.fillna(\"NaN\",inplace=True)\n    input_texts,target_texts = df[1].to_list(),df[0].to_list()\n    return input_texts,target_texts","metadata":{"execution":{"iopub.status.busy":"2022-05-02T11:03:53.579317Z","iopub.execute_input":"2022-05-02T11:03:53.582357Z","iopub.status.idle":"2022-05-02T11:03:53.977521Z","shell.execute_reply.started":"2022-05-02T11:03:53.582305Z","shell.execute_reply":"2022-05-02T11:03:53.976136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def parse_text(texts):\n    characters = set()\n    for text in texts:\n        for c in text:\n            if c not in characters:\n                characters.add(c)\n    characters.add(' ')\n    return sorted(list(characters))","metadata":{"execution":{"iopub.status.busy":"2022-05-02T11:03:53.981244Z","iopub.execute_input":"2022-05-02T11:03:53.982839Z","iopub.status.idle":"2022-05-02T11:03:53.99569Z","shell.execute_reply.started":"2022-05-02T11:03:53.982792Z","shell.execute_reply":"2022-05-02T11:03:53.994509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def start_end_pad(texts):\n    for i in range(len(texts)):\n        texts[i] = \"\\t\" + texts[i] + \"\\n\"\n    return texts","metadata":{"execution":{"iopub.status.busy":"2022-05-02T11:03:54.008953Z","iopub.execute_input":"2022-05-02T11:03:54.010162Z","iopub.status.idle":"2022-05-02T11:03:54.021415Z","shell.execute_reply.started":"2022-05-02T11:03:54.010106Z","shell.execute_reply":"2022-05-02T11:03:54.020184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_input_texts,train_target_texts = read_data(train_path)\nval_input_texts,val_target_texts = read_data(val_path)\ntest_input_texts,test_target_texts = read_data(test_path)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T11:03:54.025116Z","iopub.execute_input":"2022-05-02T11:03:54.025938Z","iopub.status.idle":"2022-05-02T11:03:54.363716Z","shell.execute_reply.started":"2022-05-02T11:03:54.025892Z","shell.execute_reply":"2022-05-02T11:03:54.362713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_target_texts = start_end_pad(train_target_texts)\nval_target_texts = start_end_pad(val_target_texts)\ntest_target_texts = start_end_pad(test_target_texts)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T11:03:54.369872Z","iopub.execute_input":"2022-05-02T11:03:54.372457Z","iopub.status.idle":"2022-05-02T11:03:54.459599Z","shell.execute_reply.started":"2022-05-02T11:03:54.372408Z","shell.execute_reply":"2022-05-02T11:03:54.458407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config_ = {\n    \"learning_rate\": 1e-3,                                      # Learning rate in gradient descent\n    \"epochs\": 10,                                               # Number of epochs to train the model   \n    \"optimizer\": 'adam',                                        # Gradient descent algorithm used for the parameter updation\n    \"batch_size\": 64,                                           # Batch size used for the optimizer\n    \"loss_function\": 'categorical_crossentropy',                # Loss function used in the optimizer                                                                      # Name of dataset\n    \"input_embedding_size\": 64,                                        # Size of input embedding layer\n    \"num_enc_layers\": 1,                                         # Number of layers in the encoder\n    \"num_dec_layers\": 1,                                         # Number of layers in the decoder\n    \"hidden_layer_size\": 128,                                      # Size of hidden layer\n    \"dropout\" : 0.20, \n    'r_dropout':0.20,# Value of dropout used in the normal and recurrent dropout\n    \"cell_type\": 'LSTM',                                         # Type of cell used in the encoder and decoder ('RNN' or 'GRU' or 'LSTM')\n    \"beam_width\": 5                                           # Beam width used in beam decoder                                         # Whether or not attention is used\n}","metadata":{"execution":{"iopub.status.busy":"2022-05-02T11:03:54.461194Z","iopub.execute_input":"2022-05-02T11:03:54.461786Z","iopub.status.idle":"2022-05-02T11:03:54.474918Z","shell.execute_reply.started":"2022-05-02T11:03:54.461748Z","shell.execute_reply":"2022-05-02T11:03:54.473935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def enc_dec_tokens(train_input_texts,train_target_texts,val_input_texts,val_target_texts):\n    \n    input_characters = parse_text(train_input_texts + val_input_texts)\n    target_characters = parse_text(train_target_texts + val_target_texts)\n    num_encoder_tokens = len(input_characters)\n    num_decoder_tokens = len(target_characters)\n    max_encoder_seq_length = max([len(txt) for txt in train_input_texts + val_input_texts])\n    max_decoder_seq_length = max([len(txt) for txt in train_target_texts + val_target_texts])\n\n    print(\"Number of training samples:\", len(train_input_texts))\n    print(\"Number of validation samples:\", len(val_input_texts))\n    print(\"Number of unique input tokens:\", num_encoder_tokens)\n    print(\"Number of unique output tokens:\", num_decoder_tokens)\n    print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n    print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n    \n    input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n    target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n    \n    return input_token_index,target_token_index,max_encoder_seq_length,max_decoder_seq_length,num_encoder_tokens,num_decoder_tokens","metadata":{"execution":{"iopub.status.busy":"2022-05-02T11:03:54.477109Z","iopub.execute_input":"2022-05-02T11:03:54.477749Z","iopub.status.idle":"2022-05-02T11:03:54.543056Z","shell.execute_reply.started":"2022-05-02T11:03:54.477703Z","shell.execute_reply":"2022-05-02T11:03:54.542082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def data_processing(input_texts,enc_length,input_token_index,num_encoder_tokens, target_texts,dec_length,target_token_index,num_decoder_tokens):\n    encoder_input_data = np.zeros(\n        (len(input_texts), enc_length), dtype=\"float32\"\n    )\n    decoder_input_data = np.zeros(\n            (len(input_texts), dec_length), dtype=\"float32\"\n        )\n    decoder_target_data = np.zeros(\n            (len(input_texts), dec_length, num_decoder_tokens), dtype=\"float32\"\n        )\n\n    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n        \n        for t, char in enumerate(input_text):\n            encoder_input_data[i, t] = input_token_index[char]\n        encoder_input_data[i, t + 1 :] = input_token_index[' ']\n        \n        for t, char in enumerate(target_text):\n                # decoder_target_data is ahead of decoder_input_data by one timestep\n            decoder_input_data[i, t] = target_token_index[char]\n            if t > 0:\n                    # decoder_target_data will be ahead by one timestep\n                    # and will not include the start character.\n                decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n        decoder_input_data[i, t + 1 :] = target_token_index[' ']\n        decoder_target_data[i, t:, target_token_index[' ']] = 1.0\n    return encoder_input_data,decoder_input_data,decoder_target_data","metadata":{"execution":{"iopub.status.busy":"2022-05-02T11:03:54.544656Z","iopub.execute_input":"2022-05-02T11:03:54.545967Z","iopub.status.idle":"2022-05-02T11:03:54.55917Z","shell.execute_reply.started":"2022-05-02T11:03:54.545904Z","shell.execute_reply":"2022-05-02T11:03:54.558246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_token_index,target_token_index,max_encoder_seq_length,max_decoder_seq_length,num_encoder_tokens,num_decoder_tokens = enc_dec_tokens(train_input_texts,train_target_texts,val_input_texts,val_target_texts)\nreverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\nreverse_target_char_index = dict((i, char) for char, i in target_token_index.items())","metadata":{"execution":{"iopub.status.busy":"2022-05-02T11:03:54.560967Z","iopub.execute_input":"2022-05-02T11:03:54.56148Z","iopub.status.idle":"2022-05-02T11:03:54.744166Z","shell.execute_reply.started":"2022-05-02T11:03:54.561423Z","shell.execute_reply":"2022-05-02T11:03:54.743265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_encoder_input,train_decoder_input,train_decoder_target = data_processing(train_input_texts,max_encoder_seq_length,input_token_index,num_encoder_tokens, train_target_texts,max_decoder_seq_length,target_token_index,num_decoder_tokens)\nval_encoder_input,val_decoder_input,val_decoder_target = data_processing(val_input_texts,max_encoder_seq_length,input_token_index,num_encoder_tokens, val_target_texts,max_decoder_seq_length,target_token_index,num_decoder_tokens)\ntest_encoder_input,test_decoder_input,test_decoder_target = data_processing(test_input_texts,max_encoder_seq_length,input_token_index,num_encoder_tokens, test_target_texts,max_decoder_seq_length,target_token_index,num_decoder_tokens)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T11:03:54.745417Z","iopub.execute_input":"2022-05-02T11:03:54.746132Z","iopub.status.idle":"2022-05-02T11:03:56.791612Z","shell.execute_reply.started":"2022-05-02T11:03:54.746084Z","shell.execute_reply":"2022-05-02T11:03:56.790558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.layers import Layer\nimport keras.backend as K\n\nclass Attention(Layer):\n    \"\"\"\n    This Attention layer class code is used from : https://github.com/thushv89/attention_keras/blob/master/src/layers/attention.py\n    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n    There are three sets of weights introduced W_a, U_a, and V_a\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert isinstance(input_shape, list)\n        # Create a trainable weight variable for this layer.\n\n        self.W_a = self.add_weight(name='W_a',\n                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n                                   initializer='uniform',\n                                   trainable=True)\n        self.U_a = self.add_weight(name='U_a',\n                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n                                   initializer='uniform',\n                                   trainable=True)\n        self.V_a = self.add_weight(name='V_a',\n                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n                                   initializer='uniform',\n                                   trainable=True)\n\n        super(Attention, self).build(input_shape)  # Be sure to call this at the end\n\n    def call(self, inputs, verbose=False):\n        \"\"\"\n        inputs: [encoder_output_sequence, decoder_output_sequence]\n        \"\"\"\n        assert type(inputs) == list\n        encoder_out_seq, decoder_out_seq = inputs\n        if verbose:\n            print('encoder_out_seq>', encoder_out_seq.shape)\n            print('decoder_out_seq>', decoder_out_seq.shape)\n\n        def energy_step(inputs, states):\n            \"\"\" Step function for computing energy for a single decoder state\n            inputs: (batchsize * 1 * de_in_dim)\n            states: (batchsize * 1 * de_latent_dim)\n            \"\"\"\n\n            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n\n            \"\"\" Some parameters required for shaping tensors\"\"\"\n            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n            de_hidden = inputs.shape[-1]\n\n            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n            # <= batch size * en_seq_len * latent_dim\n            W_a_dot_s = K.dot(encoder_out_seq, self.W_a)\n\n            \"\"\" Computing hj.Ua \"\"\"\n            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n            if verbose:\n                print('Ua.h>', U_a_dot_h.shape)\n\n            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n            # <= batch_size*en_seq_len, latent_dim\n            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n            if verbose:\n                print('Ws+Uh>', Ws_plus_Uh.shape)\n\n            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n            # <= batch_size, en_seq_len\n            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n            # <= batch_size, en_seq_len\n            e_i = K.softmax(e_i)\n\n            if verbose:\n                print('ei>', e_i.shape)\n\n            return e_i, [e_i]\n\n        def context_step(inputs, states):\n            \"\"\" Step function for computing ci using ei \"\"\"\n\n            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n\n            # <= batch_size, hidden_size\n            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n            if verbose:\n                print('ci>', c_i.shape)\n            return c_i, [c_i]\n\n        fake_state_c = K.sum(encoder_out_seq, axis=1)\n        fake_state_e = K.sum(encoder_out_seq, axis=2)  # <= (batch_size, enc_seq_len, latent_dim\n\n        \"\"\" Computing energy outputs \"\"\"\n        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n        last_out, e_outputs, _ = K.rnn(\n            energy_step, decoder_out_seq, [fake_state_e],\n        )\n\n        \"\"\" Computing context vectors \"\"\"\n        last_out, c_outputs, _ = K.rnn(\n            context_step, e_outputs, [fake_state_c],\n        )\n\n        return c_outputs, e_outputs\n\n    def compute_output_shape(self, input_shape):\n        \"\"\" Outputs produced by the layer \"\"\"\n        return [\n            # (batch_size, decoder_timesteps, decoder_hid_layer_size)\n            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n            # (batch_size, decoder_timesteps, encoder_timesteps)\n            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n        ]","metadata":{"execution":{"iopub.status.busy":"2022-05-02T11:03:56.795645Z","iopub.execute_input":"2022-05-02T11:03:56.795931Z","iopub.status.idle":"2022-05-02T11:03:56.830078Z","shell.execute_reply.started":"2022-05-02T11:03:56.795899Z","shell.execute_reply":"2022-05-02T11:03:56.829057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_model(num_encoder_tokens,num_decoder_tokens,input_embedding_size=32,num_enc_layers=1,num_dec_layers=1,hidden_layer_size=64,cell_type='LSTM',dropout=0,r_dropout=0,cell_activation='tanh'):\n    cell = {\n        'RNN':SimpleRNN,\n        'LSTM':LSTM,\n        'GRU':GRU\n    }\n    encoder_input = Input(shape=(None,),name='input_1')\n    encoder_input_embedding = Embedding(num_encoder_tokens,input_embedding_size,name='embedding_1')(encoder_input)\n    \n    encoder_sequences, *encoder_state = cell[cell_type](hidden_layer_size,activation=cell_activation,return_sequences=True,return_state=True,dropout=dropout,recurrent_dropout=r_dropout,name=\"encoder_1\")(encoder_input_embedding)\n    \n    for i in range(1,num_enc_layers):\n        encoder_sequences, *encoder_state = cell[cell_type](hidden_layer_size,activation=cell_activation,return_sequences=True,return_state=True,dropout=dropout,recurrent_dropout=r_dropout,name=f\"encoder_{i+1}\")(encoder_sequences)\n        \n    decoder_input = Input(shape=(None,),name='input_2')\n    decoder_input_embedding = Embedding(num_decoder_tokens,input_embedding_size,name='embedding_2')(decoder_input)\n    \n    decoder_sequences = decoder_input_embedding\n    for i in range(num_dec_layers-1):\n        decoder_sequences, *decoder_state = cell[cell_type](hidden_layer_size,activation=cell_activation,return_sequences=True,return_state=True,dropout=dropout,recurrent_dropout=r_dropout,name=f\"decoder_{i+1}\")(decoder_sequences ,initial_state=encoder_state)\n    decoder_sequences, *decoder_state = cell[cell_type](hidden_layer_size,activation=cell_activation,return_sequences=True,return_state=True,dropout=dropout,recurrent_dropout=r_dropout,name=\"decoder_1\")(decoder_input_embedding ,initial_state=encoder_state)\n    \n    attention_out,attention_scores = Attention(name=\"attention_1\")([encoder_sequences,decoder_sequences])\n    \n    dense_concat_input = keras.layers.Concatenate(axis=-1,name='concat_layer_1')([decoder_sequences,attention_out])\n    decoder_dense = Dense(num_decoder_tokens,activation=\"softmax\",name=\"dense_1\")(dense_concat_input)\n    \n    model = keras.Model([encoder_input,decoder_input],decoder_dense)\n    model.summary()\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-05-02T11:03:56.831587Z","iopub.execute_input":"2022-05-02T11:03:56.832101Z","iopub.status.idle":"2022-05-02T11:03:58.634844Z","shell.execute_reply.started":"2022-05-02T11:03:56.832046Z","shell.execute_reply":"2022-05-02T11:03:58.633723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_inference_model(model):\n    # Calculating number of layers in encoder and decoder\n    num_enc_layers, num_dec_layers = 0, 0\n    for layer in model.layers:\n        num_enc_layers += layer.name.startswith('encoder')\n        num_dec_layers += layer.name.startswith('decoder')\n\n    # Encoder input\n    encoder_input = model.input[0]      # Input_1\n    # Encoder cell final layer\n    encoder_cell = model.get_layer(\"encoder_\"+str(num_enc_layers))\n    encoder_type = encoder_cell.__class__.__name__\n    encoder_sequences, *encoder_state = encoder_cell.output\n    # Encoder model\n    encoder_model = keras.Model(encoder_input, encoder_state)\n\n    # Decoder input\n    decoder_input = model.input[1]      # Input_2\n    decoder_input_embedding = model.get_layer(\"embedding_2\")(decoder_input)\n    decoder_sequences = decoder_input_embedding\n    # Inputs to decoder layers' initial states\n    decoder_states, decoder_state_inputs = [], []\n    for i in range(1, num_dec_layers+1):\n        if encoder_type == 'LSTM':\n            decoder_state_input = [Input(shape=(encoder_state[0].shape[1],), name=\"input_\"+str(2*i+1)), \n                                   Input(shape=(encoder_state[1].shape[1],), name=\"input_\"+str(2*i+2))]\n        else:\n            decoder_state_input = [Input(shape=(encoder_state[0].shape[1],), name=\"input_\"+str(i+2))]\n\n        decoder_cell = model.get_layer(\"decoder_\"+str(i))\n        decoder_sequences, *decoder_state = decoder_cell(decoder_sequences, initial_state=decoder_state_input)\n        decoder_states += decoder_state\n        decoder_state_inputs += decoder_state_input\n    \n    attention_out,attention_scores = model.get_layer(\"attention_1\")([encoder_sequences,decoder_sequences])\n    \n    dense_concat_input = keras.layers.Concatenate(axis=-1,name='concat_layer_1')([decoder_sequences,attention_out])\n    # Softmax FC layer\n    decoder_dense = model.get_layer(\"dense_1\")\n    decoder_dense_output = decoder_dense(dense_concat_input)\n\n    # Decoder model\n    decoder_model = keras.Model(\n        [encoder_input,decoder_input] + decoder_state_inputs, [attention_scores,decoder_dense_output] + decoder_states\n    )\n\n    return encoder_model, decoder_model, num_enc_layers, num_dec_layers\n\n\ndef num_to_word(num_encoded, token_index, reverse_char_index = None):\n    # Function to return the predictions after cutting the END_CHAR and BLANK_CHAR s at the end.\n    # If char_dec == None, the predictions are in the form of decoded string, otherwise as list of integers\n    num_samples = len(num_encoded) if type(num_encoded) is list else num_encoded.shape[0]\n    predicted_words = ['' for t in range(num_samples)]\n    for i, encode in enumerate(num_encoded):\n        for l in encode:\n            # Stop word : '\\n'\n            if l == token_index['\\n']:\n                break\n            predicted_words[i] += reverse_char_index[l] if reverse_char_index is not None else str(l)\n    \n    return predicted_words","metadata":{"execution":{"iopub.status.busy":"2022-05-02T11:03:58.639116Z","iopub.execute_input":"2022-05-02T11:03:58.63942Z","iopub.status.idle":"2022-05-02T11:03:58.657483Z","shell.execute_reply.started":"2022-05-02T11:03:58.639381Z","shell.execute_reply":"2022-05-02T11:03:58.65646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def beam_decoder_util(model,input_sequences,max_decoder_seq_length,B=1,target_sequences=None,start_char=0,batch_size=64):\n    encoder_model,decoder_model,num_enc_layers,num_dec_layers=make_inference_model(model)\n    encoder_output = encoder_model.predict(input_sequences,batch_size=batch_size)\n    encoder_output = encoder_output if type(encoder_output) is list else [encoder_output]\n    \n    num_samples = input_sequences.shape[0]\n    \n    outputs_fn = np.zeros((num_samples,B,max_decoder_seq_length),dtype=np.int32)\n    \n    errors_fn = np.zeros((num_samples,B))\n    \n    decoder_b_inputs = np.zeros((num_samples,1,1))\n    decoder_b_inputs[:, :, 0] = start_char\n    \n    decoder_b_out = [[(0, [])] for t in range(num_samples)]\n    errors = [[0] for t in range(num_samples)]\n    \n    states = [encoder_output*num_dec_layers]\n    attn_b_scores = [[None] for t in range(num_samples)]\n    \n    for idx in range(max_decoder_seq_length):\n        all_b_beams = [[] for t in range(num_samples)]\n        all_decoder_states = [[] for t in range(num_samples)]\n        all_errors = [[] for t in range(num_samples)]\n        all_attn_scores = [[] for t in range(num_samples)]\n        for b in range(len(decoder_b_out[0])):\n            attn_scores,decoder_output, *decoder_states = decoder_model.predict([input_sequences,decoder_b_inputs[:,b]] + states[b],batch_size=batch_size)\n            top_b = np.argsort(decoder_output[:,-1,:],axis=-1)[:,-B:]\n            for n in range(num_samples):\n                all_b_beams[n]+= [(decoder_b_out[n][b][0] + np.log(decoder_output[n, -1, top_b[n][i]]),decoder_b_out[n][b][1] + [top_b[n][i]]) for i in range(B)]\n                all_attn_scores[n] += [attn_scores[n]]*B if attn_b_scores[n][b] is None else [np.concatenate((attn_b_scores[n][b],attn_scores[n]),axis=0)]*B\n                if target_sequences is not None:\n                    all_errors[n] += [errors[n][b] - np.log(decoder_output[n,-1,target_sequences[n,idx]])]*B\n                all_decoder_states[n] += [[decoder_state[n:n+1] for decoder_state in decoder_states]] * B\n        sorted_index = list(range(len(all_b_beams[0])))\n        sorted_index = [sorted(sorted_index,key = lambda ix: all_b_beams[n][ix][0])[-B:][::-1] for n in range(num_samples)]\n        decoder_b_out = [[all_b_beams[n][index] for index in sorted_index[n]] for n in range(num_samples)]\n        \n        decoder_b_inputs = np.array([[all_b_beams[n][index][1][-1] for index in sorted_index[n]] for n in range(num_samples)])\n        \n        states = [all_decoder_states[0][index] for index in sorted_index[0]]\n        \n        for n in range(1,num_samples):\n            states = [[np.concatenate((states[i][j],all_decoder_states[n][index][j])) for j in range(len(all_decoder_states[n][index]))] for i,index in  enumerate(sorted_index[n])]\n        attn_b_scores = [[all_attn_scores[n][index] for index in sorted_index[n]] for n in range(num_samples)]    \n        if target_sequences is not None:\n            errors = [[all_errors[n][index] for index in sorted_index[n]] for n in range(num_samples)]\n    outputs_fn = np.array([[decoder_b_out[n][i][1] for i in range(B)] for n in range(num_samples)])\n    if target_sequences is not None:\n        errors_fn = np.array(errors)/max_decoder_seq_length\n    return outputs_fn,errors_fn,np.array(states),np.array(attn_b_scores)\ndef calc_metrics(b_outputs, target_sequences,token_index,reverse_char_index,b_errors=None,exact_word=True,display=False):\n    matches = np.mean(b_outputs == target_sequences.reshape((target_sequences.shape[0],1,target_sequences.shape[1])),axis=-1)\n    best_b = np.argmax(matches,axis=-1)\n    best_index = (tuple(range(best_b.shape[0])),tuple(best_b))\n    accuracy = np.mean(matches[best_index])\n    b_predictions = list()\n    loss = None\n    if b_errors is not None:\n        loss = np.mean(b_errors[best_index])\n    if exact_word:\n        equal = [0] * b_outputs.shape[0]\n        true_out = num_to_word(target_sequences,token_index,reverse_char_index)\n        for b in range(b_outputs.shape[1]):\n            pred_out = num_to_word(b_outputs[:,b], token_index,reverse_char_index)\n            equal = [equal[i] or (pred_out[i] == true_out[i]) for i in range(b_outputs.shape[0])]\n            if display==True:\n                b_predictions.append(pred_out)\n        exact_accuracy = np.mean(equal)\n        if display==True:\n            return accuracy,exact_accuracy,loss,true_out,b_predictions\n        return accuracy,exact_accuracy,loss\n    return accuracy,loss\ndef beam_decoder(model,input_sequences,target_sequences_onehot,max_decoder_seq_length,token_index,reverse_char_index,B=1,model_batch_size=64,infer_batch_size=512,exact_word=True,return_outputs=False,return_states=False,return_attention=False,display=False):\n    target_sequences = np.argmax(target_sequences_onehot,axis=-1)\n    b_outputs,b_errors,b_states,b_attention=None,None,None,None\n    for i in range(0,input_sequences.shape[0],infer_batch_size):\n        tmp_b_outputs,tmp_b_errors,tmp_b_states,tmp_b_attention = beam_decoder_util(model,input_sequences[i:i+infer_batch_size],max_decoder_seq_length,B,target_sequences[i:i+infer_batch_size],token_index['\\t'],model_batch_size)\n        \n        if b_errors is None:\n            b_outputs,b_errors,b_states,b_attention = tmp_b_outputs,tmp_b_errors,tmp_b_states,tmp_b_attention\n        else:\n            b_outputs = np.concatenate((b_outputs,tmp_b_outputs))\n            b_errors = np.concatenate((b_errors,tmp_b_errors))\n            b_states = np.concatenate((b_states,tmp_b_states),axis=2)\n            b_attention = np.concatenate((b_attention,tmp_b_attention))\n    return_elements = []\n    if return_outputs:\n        return_elements += [b_outputs]\n    if return_states:\n        return_elements += [b_states]\n    if return_attention:\n        return_elements += [b_attention]\n    if len(return_elements) > 0:\n        return calc_metrics(b_outputs,target_sequences,token_index,reverse_char_index,b_errors,exact_word,display) + tuple(return_elements)\n    return calc_metrics(b_outputs,target_sequences,target_token_index,reverse_char_index,b_errors,exact_word,display)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T11:03:58.659389Z","iopub.execute_input":"2022-05-02T11:03:58.659973Z","iopub.status.idle":"2022-05-02T11:03:58.702709Z","shell.execute_reply.started":"2022-05-02T11:03:58.659924Z","shell.execute_reply":"2022-05-02T11:03:58.701516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = make_model(num_encoder_tokens,num_decoder_tokens)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T11:03:58.704511Z","iopub.execute_input":"2022-05-02T11:03:58.704823Z","iopub.status.idle":"2022-05-02T11:03:58.718023Z","shell.execute_reply.started":"2022-05-02T11:03:58.70478Z","shell.execute_reply":"2022-05-02T11:03:58.717018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Nadam\nfrom tensorflow.keras.metrics import categorical_crossentropy,sparse_categorical_crossentropy\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy,CategoricalCrossentropy\nfrom tensorflow.keras.callbacks import EarlyStopping\ndef model_train_util(config):\n    model = make_model(num_encoder_tokens,num_decoder_tokens,config['input_embedding_size'],config['num_enc_layers'],config['num_dec_layers'],config['hidden_layer_size'],config['cell_type'],config['dropout'],config['r_dropout'])\n    optimizer = config['optimizer']\n    if config['loss_function'] == 'categorical_crossentropy':\n        loss_fn = CategoricalCrossentropy\n    else:\n        loss_fn = SparseCategoricalCrossentropy\n    if optimizer == 'adam':\n        model.compile(optimizer = Adam(learning_rate=config['learning_rate']), loss = loss_fn(), metrics = ['accuracy'])\n    elif optimizer == 'momentum':\n        model.compile(optimizer = SGD(learning_rate=config['learning_rate'], momentum = 0.9), loss = loss_fn(), metrics = ['accuracy'])\n    elif optimizer == 'rmsprop':\n        model.compile(optimizer = RMSprop(learning_rate=config['learning_rate']), loss = loss_fn(), metrics = ['accuracy'])\n    elif optimizer == 'nesterov':\n        model.compile(optimizer = SGD(learning_rate=config['learning_rate'], momentum = 0.9, nesterov = True), loss = loss_fn(), metrics = ['accuracy'])\n    elif optimizer == 'nadam':\n        model.compile(optimizer = Nadam(learning_rate=config['learning_rate']), loss = loss_fn(), metrics = ['accuracy'])\n    else:\n        model.compile(optimizer = SGD(learning_rate=config['learning_rate']), loss = loss_fn(), metrics = ['accuracy'])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-05-02T11:03:58.719891Z","iopub.execute_input":"2022-05-02T11:03:58.720253Z","iopub.status.idle":"2022-05-02T11:03:59.105492Z","shell.execute_reply.started":"2022-05-02T11:03:58.720188Z","shell.execute_reply":"2022-05-02T11:03:59.104582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class customCallback(keras.callbacks.Callback):\n     # Custom class to provide callback after each epoch of training to calculate custom metrics for validation set with beam decoder\n    def __init__(self, val_enc_input, val_dec_target, beam_width=1, batch_size=64):\n        self.beam_width = beam_width\n        self.validation_input = val_enc_input\n        self.validation_target = val_dec_target\n        self.batch_size = batch_size\n\n    def on_epoch_end(self, epoch, logs):\n        val_accuracy, val_exact_accuracy, val_loss = beam_decoder(self.model, self.validation_input, self.validation_target, max_decoder_seq_length, \n                                                                  target_token_index, reverse_target_char_index, self.beam_width, self.batch_size)\n\n        # Log them to reflect in WANDB callback and EarlyStopping\n        logs[\"val_accuracy\"] = val_accuracy\n        logs[\"val_exact_accuracy\"] = val_exact_accuracy\n        logs[\"val_loss\"] = val_loss             # Validation loss calculates categorical cross entropy loss\n\n        print(\"— val_loss: {:.3f} — val_accuracy: {:.3f} — val_exact_accuracy: {:.5f}\".format(val_loss, val_accuracy, val_exact_accuracy))","metadata":{"execution":{"iopub.status.busy":"2022-05-02T11:03:59.109191Z","iopub.execute_input":"2022-05-02T11:03:59.109455Z","iopub.status.idle":"2022-05-02T11:03:59.122424Z","shell.execute_reply.started":"2022-05-02T11:03:59.109425Z","shell.execute_reply":"2022-05-02T11:03:59.118492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model,history = model_train_util(config_,[train_encoder_input,train_decoder_input],train_decoder_target,[val_encoder_input,val_decoder_input],val_decoder_target)\n# model.save(\"s2s\")","metadata":{"execution":{"iopub.status.busy":"2022-05-02T11:03:59.124381Z","iopub.execute_input":"2022-05-02T11:03:59.125133Z","iopub.status.idle":"2022-05-02T11:03:59.139461Z","shell.execute_reply.started":"2022-05-02T11:03:59.125018Z","shell.execute_reply":"2022-05-02T11:03:59.138525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Installing and logging into WANDB\n!pip install --upgrade wandb\n!wandb login b44266d937596fcef83bedbe7330d6cee108a277\n\nimport wandb\nfrom wandb.keras import WandbCallback","metadata":{"execution":{"iopub.status.busy":"2022-05-02T11:03:59.140962Z","iopub.execute_input":"2022-05-02T11:03:59.141392Z","iopub.status.idle":"2022-05-02T11:04:14.876736Z","shell.execute_reply.started":"2022-05-02T11:03:59.141344Z","shell.execute_reply":"2022-05-02T11:04:14.875635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_train(config,iswandb=False):\n    wid = None\n    if iswandb:\n        wid = wandb.util.generate_id()\n        run = wandb.init(id = wid, project=\"cs6910_assignment_3\", entity=\"dlstack\", reinit=True, config=config)\n        wandb.run.name = f\"ies_{config['input_embedding_size']}_nenc_{config['num_enc_layers']}_ndec_{config['num_dec_layers']}_cell_{config['cell_type']}_drop_{config['dropout']}_rdrop{config['r_dropout']}\"\n        wandb.run.name += f\"_hs_{config['hidden_layer_size']}_B_{config['beam_width']}_attn\"\n        wandb.run.save()\n        print(wandb.run.name)\n\n    model = model_train_util(config)\n    if iswandb:\n        call_list = [customCallback(val_encoder_input,val_decoder_target,beam_width=config['beam_width'],batch_size=config['batch_size']),WandbCallback(monitor='val_accuracy'),EarlyStopping(monitor='val_accuracy',patience=4)]\n    else:\n        call_list = [customCallback(val_encoder_input,val_decoder_target,beam_width=config['beam_width'],batch_size=config['batch_size']),EarlyStopping(monitor='val_accuracy',patience=4)]\n    history = model.fit(\n        [train_encoder_input,train_decoder_input],\n        train_decoder_target,\n        batch_size=config['batch_size'],\n        verbose = 1,\n        epochs=config['epochs'],\n        callbacks = call_list\n    )    \n    if iswandb:\n        run.finish()\n\n    return model, history,config, wid","metadata":{"execution":{"iopub.status.busy":"2022-05-02T11:04:14.879231Z","iopub.execute_input":"2022-05-02T11:04:14.879625Z","iopub.status.idle":"2022-05-02T11:04:14.892839Z","shell.execute_reply.started":"2022-05-02T11:04:14.87956Z","shell.execute_reply":"2022-05-02T11:04:14.891434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def wandb_sweep():\n    # Wrapper function to call the model_train() function for sweeping with different hyperparameters\n\n    # Initialize a new wandb run\n    run = wandb.init(config=config_, reinit=True)\n\n    # Config is a variable that holds and saves hyperparameters and inputs\n    config = wandb.config\n\n    wandb.run.name = f'ies_{config.input_embedding_size}_nenc_{config.num_enc_layers}_ndec_{config.num_dec_layers}_cell_{config.cell_type}_drop_{config.dropout}_rdrop_{config.r_dropout}'\n    wandb.run.name += f'_hs_{config.hidden_layer_size}_B_{config.beam_width}_attn'\n    wandb.run.save()\n    print(wandb.run.name)\n\n    model, *_ = model_train(config, iswandb=True)\n    run.finish()","metadata":{"execution":{"iopub.status.busy":"2022-05-02T11:04:14.901932Z","iopub.execute_input":"2022-05-02T11:04:14.902171Z","iopub.status.idle":"2022-05-02T11:04:14.909573Z","shell.execute_reply.started":"2022-05-02T11:04:14.902141Z","shell.execute_reply":"2022-05-02T11:04:14.908423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hyperparameter choices to sweep \nsweep_config_1 = {\n    'name': 'RNNs2s_attn',\n    'method': 'bayes',                   # Possible search : grid, random, bayes\n    'metric': {\n      'name': 'val_accuracy',\n      'goal': 'maximize'   \n    },\n    'parameters': {\n        'epochs':{\n            'values':[10,15,20]\n        },\n        'learning_rate':{\n            'values':[0.001,0.0001,0.005]\n        },\n        'optimizer':{\n            'value':'adam'\n        },\n        'loss_function':{\n          'value':'categorical_crossentropy'  \n        },\n        'input_embedding_size': {\n            'values': [64,128,256]\n        },\n        'num_enc_layers': {\n            'values': [2,3]\n        },\n        'num_dec_layers': {\n            'values': [3,5]\n        },\n        'hidden_layer_size': {\n            'values': [256,512,768]\n        },\n        'cell_type': {\n            'values': ['GRU']\n        },\n        'dropout' :{\n            'values': [0.20,0.30]\n        },\n        'r_dropout': {\n            'values': [0.20,0.30]\n        },\n        'beam_width': {\n            'values': [1,3,5]\n        },\n        'batch_size':{\n            'values':[128,256]\n        }\n    }\n}\nsweep_id = wandb.sweep(sweep_config_1,entity='dlstack',project='cs6910_assignment_3')\nwandb.agent(sweep_id,lambda:wandb_sweep(),count=10)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T11:04:14.911615Z","iopub.execute_input":"2022-05-02T11:04:14.912099Z","iopub.status.idle":"2022-05-02T12:14:20.306744Z","shell.execute_reply.started":"2022-05-02T11:04:14.912058Z","shell.execute_reply":"2022-05-02T12:14:20.305773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hyperparameter choices to sweep \nsweep_config_1 = {\n    'name': 'RNNs2s_attn',\n    'method': 'bayes',                   # Possible search : grid, random, bayes\n    'metric': {\n      'name': 'val_accuracy',\n      'goal': 'maximize'   \n    },\n    'parameters': {\n        'epochs':{\n            'values':[10,15,20]\n        },\n        'learning_rate':{\n            'values':[0.001,0.0001,0.005]\n        },\n        'optimizer':{\n            'value':'adam'\n        },\n        'loss_function':{\n          'value':'categorical_crossentropy'  \n        },\n        'input_embedding_size': {\n            'values': [64,128,256]\n        },\n        'num_enc_layers': {\n            'values': [2,3]\n        },\n        'num_dec_layers': {\n            'values': [3,5]\n        },\n        'hidden_layer_size': {\n            'values': [256,512,768]\n        },\n        'cell_type': {\n            'values': ['GRU']\n        },\n        'dropout' :{\n            'values': [0.20,0.30]\n        },\n        'r_dropout': {\n            'values': [0.20,0.30]\n        },\n        'beam_width': {\n            'values': [1,3,5]\n        },\n        'batch_size':{\n            'values':[128,256]\n        }\n    }\n}\nsweep_id = wandb.sweep(sweep_config_1,entity='dlstack',project='cs6910_assignment_3')\nwandb.agent(sweep_id,lambda:wandb_sweep(),count=10)","metadata":{},"execution_count":null,"outputs":[]}]}