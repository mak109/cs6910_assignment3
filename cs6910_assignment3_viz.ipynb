{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Installing dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T15:39:33.644333Z",
     "iopub.status.busy": "2022-05-06T15:39:33.643975Z",
     "iopub.status.idle": "2022-05-06T15:40:04.276615Z",
     "shell.execute_reply": "2022-05-06T15:40:04.275725Z",
     "shell.execute_reply.started": "2022-05-06T15:39:33.644241Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install wget\n",
    "!pip install --upgrade wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing necessary libraries,packages,etc.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T15:40:04.279243Z",
     "iopub.status.busy": "2022-05-06T15:40:04.278916Z",
     "iopub.status.idle": "2022-05-06T15:40:04.296242Z",
     "shell.execute_reply": "2022-05-06T15:40:04.29532Z",
     "shell.execute_reply.started": "2022-05-06T15:40:04.279199Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import wget\n",
    "import os\n",
    "import tarfile\n",
    "import csv\n",
    "import matplotlib\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T15:40:04.298871Z",
     "iopub.status.busy": "2022-05-06T15:40:04.297517Z",
     "iopub.status.idle": "2022-05-06T15:40:04.393486Z",
     "shell.execute_reply": "2022-05-06T15:40:04.392599Z",
     "shell.execute_reply.started": "2022-05-06T15:40:04.298826Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from ipywidgets import interact, Layout, IntSlider\n",
    "from IPython.display import HTML as html_print\n",
    "from IPython.display import display\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T15:40:04.39541Z",
     "iopub.status.busy": "2022-05-06T15:40:04.395104Z",
     "iopub.status.idle": "2022-05-06T15:40:09.535375Z",
     "shell.execute_reply": "2022-05-06T15:40:09.534508Z",
     "shell.execute_reply.started": "2022-05-06T15:40:04.395371Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T15:40:09.538395Z",
     "iopub.status.busy": "2022-05-06T15:40:09.538109Z",
     "iopub.status.idle": "2022-05-06T15:40:12.840457Z",
     "shell.execute_reply": "2022-05-06T15:40:12.839618Z",
     "shell.execute_reply.started": "2022-05-06T15:40:09.538353Z"
    }
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "wandb.login(key='b44266d937596fcef83bedbe7330d6cee108a277')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T15:40:12.842648Z",
     "iopub.status.busy": "2022-05-06T15:40:12.842363Z",
     "iopub.status.idle": "2022-05-06T15:40:12.851052Z",
     "shell.execute_reply": "2022-05-06T15:40:12.849813Z",
     "shell.execute_reply.started": "2022-05-06T15:40:12.84261Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import SimpleRNN,GRU,LSTM,Embedding,Input,Dense\n",
    "from keras.layers import Layer\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T15:40:12.854959Z",
     "iopub.status.busy": "2022-05-06T15:40:12.854231Z",
     "iopub.status.idle": "2022-05-06T15:40:35.571319Z",
     "shell.execute_reply": "2022-05-06T15:40:35.570375Z",
     "shell.execute_reply.started": "2022-05-06T15:40:12.854917Z"
    }
   },
   "outputs": [],
   "source": [
    "#Dataset downloading and extracting\n",
    "filename = 'dakshina_dataset_v1.0'\n",
    "url = 'https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar'\n",
    "if not os.path.exists(filename+'.tar') and not os.path.exists(filename):\n",
    "    filename_tar = wget.download(url)\n",
    "    file = tarfile.open(filename_tar)\n",
    "    print('\\nExtracting files ....')\n",
    "    file.extractall()\n",
    "    file.close()\n",
    "    print('Done')\n",
    "    os.remove(filename_tar)\n",
    "elif not os.path.exists(filename):\n",
    "    filename_tar = filename + '.tar'\n",
    "    file = tarfile.open(filename_tar)\n",
    "    print('\\nExtracting files ....')\n",
    "    file.extractall()\n",
    "    file.close()\n",
    "    print('Done')\n",
    "    os.remove(filename_tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T15:40:35.580984Z",
     "iopub.status.busy": "2022-05-06T15:40:35.576836Z",
     "iopub.status.idle": "2022-05-06T15:40:35.594407Z",
     "shell.execute_reply": "2022-05-06T15:40:35.593712Z",
     "shell.execute_reply.started": "2022-05-06T15:40:35.580931Z"
    }
   },
   "outputs": [],
   "source": [
    "#Paths\n",
    "lang = 'bn'\n",
    "train_path =  filename+f\"/{lang}/lexicons/{lang}.translit.sampled.train.tsv\"\n",
    "val_path = filename+f\"/{lang}/lexicons/{lang}.translit.sampled.dev.tsv\"\n",
    "test_path = filename+f\"/{lang}/lexicons/{lang}.translit.sampled.test.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T15:40:35.600942Z",
     "iopub.status.busy": "2022-05-06T15:40:35.598625Z",
     "iopub.status.idle": "2022-05-06T15:40:36.191967Z",
     "shell.execute_reply": "2022-05-06T15:40:36.190959Z",
     "shell.execute_reply.started": "2022-05-06T15:40:35.6009Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    df = pd.read_csv(path,header=None,sep='\\t')\n",
    "    df.fillna(\"NaN\",inplace=True)\n",
    "    input_texts,target_texts = df[1].to_list(),df[0].to_list()\n",
    "    return input_texts,target_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T15:40:36.193595Z",
     "iopub.status.busy": "2022-05-06T15:40:36.193282Z",
     "iopub.status.idle": "2022-05-06T15:40:38.481702Z",
     "shell.execute_reply": "2022-05-06T15:40:38.480698Z",
     "shell.execute_reply.started": "2022-05-06T15:40:36.193553Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_text(texts):\n",
    "    characters = set()\n",
    "    for text in texts:\n",
    "        for c in text:\n",
    "            if c not in characters:\n",
    "                characters.add(c)\n",
    "    characters.add(' ')\n",
    "    return sorted(list(characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T15:40:38.483813Z",
     "iopub.status.busy": "2022-05-06T15:40:38.483497Z",
     "iopub.status.idle": "2022-05-06T15:40:38.495971Z",
     "shell.execute_reply": "2022-05-06T15:40:38.49494Z",
     "shell.execute_reply.started": "2022-05-06T15:40:38.483773Z"
    }
   },
   "outputs": [],
   "source": [
    "def start_end_pad(texts):\n",
    "    for i in range(len(texts)):\n",
    "        texts[i] = \"\\t\" + texts[i] + \"\\n\"\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T15:40:38.497776Z",
     "iopub.status.busy": "2022-05-06T15:40:38.497404Z",
     "iopub.status.idle": "2022-05-06T15:40:38.676495Z",
     "shell.execute_reply": "2022-05-06T15:40:38.675629Z",
     "shell.execute_reply.started": "2022-05-06T15:40:38.497731Z"
    }
   },
   "outputs": [],
   "source": [
    "#Train test val dataset import raw\n",
    "train_input_texts,train_target_texts = read_data(train_path)\n",
    "val_input_texts,val_target_texts = read_data(val_path)\n",
    "test_input_texts,test_target_texts = read_data(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T15:40:38.678087Z",
     "iopub.status.busy": "2022-05-06T15:40:38.677803Z",
     "iopub.status.idle": "2022-05-06T15:40:38.722412Z",
     "shell.execute_reply": "2022-05-06T15:40:38.721579Z",
     "shell.execute_reply.started": "2022-05-06T15:40:38.678049Z"
    }
   },
   "outputs": [],
   "source": [
    "#Padding at beginning and end with '\\t' and '\\n' respectively\n",
    "train_target_texts = start_end_pad(train_target_texts)\n",
    "val_target_texts = start_end_pad(val_target_texts)\n",
    "test_target_texts = start_end_pad(test_target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T15:40:38.727597Z",
     "iopub.status.busy": "2022-05-06T15:40:38.727358Z",
     "iopub.status.idle": "2022-05-06T15:40:38.751821Z",
     "shell.execute_reply": "2022-05-06T15:40:38.750813Z",
     "shell.execute_reply.started": "2022-05-06T15:40:38.727568Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class Attention(Layer):\n",
    "    \"\"\"\n",
    "    This Attention layer class code is used from : https://github.com/thushv89/attention_keras/blob/master/src/layers/attention.py\n",
    "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
    "    There are three sets of weights introduced W_a, U_a, and V_a\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        # Create a trainable weight variable for this layer.\n",
    "\n",
    "        self.W_a = self.add_weight(name='W_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.U_a = self.add_weight(name='U_a',\n",
    "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.V_a = self.add_weight(name='V_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "\n",
    "        super(Attention, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, inputs, verbose=False):\n",
    "        \"\"\"\n",
    "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
    "        \"\"\"\n",
    "        assert type(inputs) == list\n",
    "        encoder_out_seq, decoder_out_seq = inputs\n",
    "        if verbose:\n",
    "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
    "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
    "\n",
    "        def energy_step(inputs, states):\n",
    "            \"\"\" Step function for computing energy for a single decoder state\n",
    "            inputs: (batchsize * 1 * de_in_dim)\n",
    "            states: (batchsize * 1 * de_latent_dim)\n",
    "            \"\"\"\n",
    "\n",
    "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
    "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
    "\n",
    "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
    "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
    "            de_hidden = inputs.shape[-1]\n",
    "\n",
    "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
    "            # <= batch size * en_seq_len * latent_dim\n",
    "            W_a_dot_s = K.dot(encoder_out_seq, self.W_a)\n",
    "\n",
    "            \"\"\" Computing hj.Ua \"\"\"\n",
    "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
    "            if verbose:\n",
    "                print('Ua.h>', U_a_dot_h.shape)\n",
    "\n",
    "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n",
    "            if verbose:\n",
    "                print('Ws+Uh>', Ws_plus_Uh.shape)\n",
    "\n",
    "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.softmax(e_i)\n",
    "\n",
    "            if verbose:\n",
    "                print('ei>', e_i.shape)\n",
    "\n",
    "            return e_i, [e_i]\n",
    "\n",
    "        def context_step(inputs, states):\n",
    "            \"\"\" Step function for computing ci using ei \"\"\"\n",
    "\n",
    "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
    "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
    "\n",
    "            # <= batch_size, hidden_size\n",
    "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
    "            if verbose:\n",
    "                print('ci>', c_i.shape)\n",
    "            return c_i, [c_i]\n",
    "\n",
    "        fake_state_c = K.sum(encoder_out_seq, axis=1)\n",
    "        fake_state_e = K.sum(encoder_out_seq, axis=2)  # <= (batch_size, enc_seq_len, latent_dim\n",
    "\n",
    "        \"\"\" Computing energy outputs \"\"\"\n",
    "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
    "        last_out, e_outputs, _ = K.rnn(\n",
    "            energy_step, decoder_out_seq, [fake_state_e],\n",
    "        )\n",
    "\n",
    "        \"\"\" Computing context vectors \"\"\"\n",
    "        last_out, c_outputs, _ = K.rnn(\n",
    "            context_step, e_outputs, [fake_state_c],\n",
    "        )\n",
    "\n",
    "        return c_outputs, e_outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\" Outputs produced by the layer \"\"\"\n",
    "        return [\n",
    "            # (batch_size, decoder_timesteps, decoder_hid_layer_size)\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
    "            # (batch_size, decoder_timesteps, encoder_timesteps)\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T15:40:38.75401Z",
     "iopub.status.busy": "2022-05-06T15:40:38.753607Z",
     "iopub.status.idle": "2022-05-06T15:40:38.766708Z",
     "shell.execute_reply": "2022-05-06T15:40:38.765856Z",
     "shell.execute_reply.started": "2022-05-06T15:40:38.753969Z"
    }
   },
   "outputs": [],
   "source": [
    "def enc_dec_tokens(train_input_texts,train_target_texts,val_input_texts,val_target_texts):\n",
    "    #Returns encoding of characters as integer in two dictionary for input and target characters\n",
    "    #Returns number of tokens in input and output\n",
    "    #Returns the maximum sequence length from input and target texts\n",
    "    input_characters = parse_text(train_input_texts + val_input_texts)\n",
    "    target_characters = parse_text(train_target_texts + val_target_texts)\n",
    "    num_encoder_tokens = len(input_characters)\n",
    "    num_decoder_tokens = len(target_characters)\n",
    "    max_encoder_seq_length = max([len(txt) for txt in train_input_texts + val_input_texts])\n",
    "    max_decoder_seq_length = max([len(txt) for txt in train_target_texts + val_target_texts])\n",
    "\n",
    "    print(\"Number of training samples:\", len(train_input_texts))\n",
    "    print(\"Number of validation samples:\", len(val_input_texts))\n",
    "    print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
    "    print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
    "    print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
    "    print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
    "    \n",
    "    input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "    target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
    "    \n",
    "    return input_token_index,target_token_index,max_encoder_seq_length,max_decoder_seq_length,num_encoder_tokens,num_decoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T15:40:38.76965Z",
     "iopub.status.busy": "2022-05-06T15:40:38.76939Z",
     "iopub.status.idle": "2022-05-06T15:40:38.782133Z",
     "shell.execute_reply": "2022-05-06T15:40:38.781123Z",
     "shell.execute_reply.started": "2022-05-06T15:40:38.769623Z"
    }
   },
   "outputs": [],
   "source": [
    "#Data Preprocessing\n",
    "def data_processing(input_texts,enc_length,input_token_index,num_encoder_tokens, target_texts,dec_length,target_token_index,num_decoder_tokens):\n",
    "         # Returns the input and target data in a form needed by the Keras embedding layer (i.e) \n",
    "        # decoder_input & encoder_input -- (None, timesteps) where each character is encoded by an integer\n",
    "        # decoder_output -- (None, timesteps, vocabulary size) where the last dimension is the one-hot encoding\n",
    "\n",
    "        #  ' ' -- space (equivalent to no meaningful input / blank input)\n",
    "    encoder_input_data = np.zeros(\n",
    "        (len(input_texts), enc_length), dtype=\"float32\"\n",
    "    )\n",
    "    decoder_input_data = np.zeros(\n",
    "            (len(input_texts), dec_length), dtype=\"float32\"\n",
    "        )\n",
    "    decoder_target_data = np.zeros(\n",
    "            (len(input_texts), dec_length, num_decoder_tokens), dtype=\"float32\"\n",
    "        )\n",
    "\n",
    "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "        \n",
    "        for t, char in enumerate(input_text):\n",
    "            encoder_input_data[i, t] = input_token_index[char]\n",
    "        encoder_input_data[i, t + 1 :] = input_token_index[' ']\n",
    "        \n",
    "        for t, char in enumerate(target_text):\n",
    "                # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "            decoder_input_data[i, t] = target_token_index[char]\n",
    "            if t > 0:\n",
    "                    # decoder_target_data will be ahead by one timestep\n",
    "                    # and will not include the start character.\n",
    "                decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
    "        decoder_input_data[i, t + 1 :] = target_token_index[' ']\n",
    "        decoder_target_data[i, t:, target_token_index[' ']] = 1.0\n",
    "    return encoder_input_data,decoder_input_data,decoder_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T15:40:38.784832Z",
     "iopub.status.busy": "2022-05-06T15:40:38.783826Z",
     "iopub.status.idle": "2022-05-06T15:40:38.968121Z",
     "shell.execute_reply": "2022-05-06T15:40:38.967274Z",
     "shell.execute_reply.started": "2022-05-06T15:40:38.784786Z"
    }
   },
   "outputs": [],
   "source": [
    "input_token_index,target_token_index,max_encoder_seq_length,max_decoder_seq_length,num_encoder_tokens,num_decoder_tokens = enc_dec_tokens(train_input_texts,train_target_texts,val_input_texts,val_target_texts)\n",
    "#Dictionary for reverse lookup of character for its integer encode \n",
    "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T15:40:38.970041Z",
     "iopub.status.busy": "2022-05-06T15:40:38.96957Z",
     "iopub.status.idle": "2022-05-06T15:40:41.163639Z",
     "shell.execute_reply": "2022-05-06T15:40:41.162773Z",
     "shell.execute_reply.started": "2022-05-06T15:40:38.97Z"
    }
   },
   "outputs": [],
   "source": [
    "#Preprocessed inputs this will be used in training validation and testing in future\n",
    "train_encoder_input,train_decoder_input,train_decoder_target = data_processing(train_input_texts,max_encoder_seq_length,input_token_index,num_encoder_tokens, train_target_texts,max_decoder_seq_length,target_token_index,num_decoder_tokens)\n",
    "val_encoder_input,val_decoder_input,val_decoder_target = data_processing(val_input_texts,max_encoder_seq_length,input_token_index,num_encoder_tokens, val_target_texts,max_decoder_seq_length,target_token_index,num_decoder_tokens)\n",
    "test_encoder_input,test_decoder_input,test_decoder_target = data_processing(test_input_texts,max_encoder_seq_length,input_token_index,num_encoder_tokens, test_target_texts,max_decoder_seq_length,target_token_index,num_decoder_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T15:40:41.168764Z",
     "iopub.status.busy": "2022-05-06T15:40:41.168285Z",
     "iopub.status.idle": "2022-05-06T15:40:41.218084Z",
     "shell.execute_reply": "2022-05-06T15:40:41.216933Z",
     "shell.execute_reply.started": "2022-05-06T15:40:41.168722Z"
    }
   },
   "outputs": [],
   "source": [
    "#Inference Model for attention\n",
    "def make_inference_model_attn(model):\n",
    "    # Calculating number of layers in encoder and decoder\n",
    "    num_enc_layers, num_dec_layers = 0, 0\n",
    "    for layer in model.layers:\n",
    "        num_enc_layers += layer.name.startswith('encoder')\n",
    "        num_dec_layers += layer.name.startswith('decoder')\n",
    "\n",
    "    # Encoder input\n",
    "    encoder_input = model.input[0]      # Input_1\n",
    "    # Encoder cell final layer\n",
    "    encoder_cell = model.get_layer(\"encoder_\"+str(num_enc_layers))\n",
    "    encoder_type = encoder_cell.__class__.__name__\n",
    "    encoder_sequences, *encoder_state = encoder_cell.output\n",
    "    # Encoder model\n",
    "    encoder_model = keras.Model(encoder_input, encoder_state)\n",
    "\n",
    "    # Decoder input\n",
    "    decoder_input = model.input[1]      # Input_2\n",
    "    decoder_input_embedding = model.get_layer(\"embedding_2\")(decoder_input)\n",
    "    decoder_sequences = decoder_input_embedding\n",
    "    # Inputs to decoder layers' initial states\n",
    "    decoder_states, decoder_state_inputs = [], []\n",
    "    for i in range(1, num_dec_layers+1):\n",
    "        if encoder_type == 'LSTM':\n",
    "            decoder_state_input = [Input(shape=(encoder_state[0].shape[1],), name=\"input_\"+str(2*i+1)), \n",
    "                                   Input(shape=(encoder_state[1].shape[1],), name=\"input_\"+str(2*i+2))]\n",
    "        else:\n",
    "            decoder_state_input = [Input(shape=(encoder_state[0].shape[1],), name=\"input_\"+str(i+2))]\n",
    "\n",
    "        decoder_cell = model.get_layer(\"decoder_\"+str(i))\n",
    "        decoder_sequences, *decoder_state = decoder_cell(decoder_sequences, initial_state=decoder_state_input)\n",
    "        decoder_states += decoder_state\n",
    "        decoder_state_inputs += decoder_state_input\n",
    "    # Attention laye\n",
    "    attention_out,attention_scores = model.get_layer(\"attention_1\")([encoder_sequences,decoder_sequences])\n",
    "    \n",
    "    dense_concat_input = keras.layers.Concatenate(axis=-1,name='concat_layer_1')([decoder_sequences,attention_out])\n",
    "    # Softmax FC layer\n",
    "    decoder_dense = model.get_layer(\"dense_1\")\n",
    "    decoder_dense_output = decoder_dense(dense_concat_input)\n",
    "\n",
    "    # Decoder model\n",
    "    decoder_model = keras.Model(\n",
    "        [encoder_input,decoder_input] + decoder_state_inputs, [attention_scores,decoder_dense_output] + decoder_states\n",
    "    )\n",
    "\n",
    "    return encoder_model, decoder_model, num_enc_layers, num_dec_layers\n",
    "#Inference model without attention\n",
    "def make_inference_model(model):\n",
    "    # Calculating number of layers in encoder and decoder\n",
    "    num_enc_layers, num_dec_layers = 0, 0\n",
    "    for layer in model.layers:\n",
    "        num_enc_layers += layer.name.startswith('encoder')\n",
    "        num_dec_layers += layer.name.startswith('decoder')\n",
    "\n",
    "    # Encoder input\n",
    "    encoder_input = model.input[0]      # Input_1\n",
    "    # Encoder cell final layer\n",
    "    encoder_cell = model.get_layer(\"encoder_\"+str(num_enc_layers))\n",
    "    encoder_type = encoder_cell.__class__.__name__\n",
    "    encoder_seq, *encoder_state = encoder_cell.output\n",
    "    # Encoder model\n",
    "    encoder_model = keras.Model(encoder_input, encoder_state)\n",
    "\n",
    "    # Decoder input\n",
    "    decoder_input = model.input[1]      # Input_2\n",
    "    decoder_input_embedding = model.get_layer(\"embedding_2\")(decoder_input)\n",
    "    decoder_sequences = decoder_input_embedding\n",
    "    # Inputs to decoder layers' initial states\n",
    "    decoder_states, decoder_state_inputs = [], []\n",
    "    for i in range(1, num_dec_layers+1):\n",
    "        if encoder_type == 'LSTM':\n",
    "            decoder_state_input = [Input(shape=(encoder_state[0].shape[1],), name=\"input_\"+str(2*i+1)), \n",
    "                                   Input(shape=(encoder_state[1].shape[1],), name=\"input_\"+str(2*i+2))]\n",
    "        else:\n",
    "            decoder_state_input = [Input(shape=(encoder_state[0].shape[1],), name=\"input_\"+str(i+2))]\n",
    "\n",
    "        decoder_cell = model.get_layer(\"decoder_\"+str(i))\n",
    "        decoder_sequences, *decoder_state = decoder_cell(decoder_sequences, initial_state=decoder_state_input)\n",
    "        decoder_states += decoder_state\n",
    "        decoder_state_inputs += decoder_state_input\n",
    "\n",
    "    # Softmax FC layer\n",
    "    decoder_dense = model.get_layer(\"dense_1\")\n",
    "    decoder_dense_output = decoder_dense(decoder_sequences)\n",
    "\n",
    "    # Decoder model\n",
    "    decoder_model = keras.Model(\n",
    "        [decoder_input] + decoder_state_inputs, [decoder_dense_output] + decoder_states\n",
    "    )\n",
    "\n",
    "    return encoder_model, decoder_model, num_enc_layers, num_dec_layers\n",
    "\n",
    "\n",
    "def num_to_word(num_encoded, token_index, reverse_char_index = None):\n",
    "# Function to return the predictions after cutting the '\\n' and ' ' s at the end.\n",
    "    # If reverse_char_index == None, the predictions are in the form of decoded string, otherwise as list of integers\n",
    "    num_samples = len(num_encoded) if type(num_encoded) is list else num_encoded.shape[0]\n",
    "    predicted_words = ['' for t in range(num_samples)]\n",
    "    for i, encode in enumerate(num_encoded):\n",
    "        for l in encode:\n",
    "            # Stop word : '\\n'\n",
    "            if l == token_index['\\n']:\n",
    "                break\n",
    "            predicted_words[i] += reverse_char_index[l] if reverse_char_index is not None else str(l)\n",
    "    \n",
    "    return predicted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T15:40:41.225246Z",
     "iopub.status.busy": "2022-05-06T15:40:41.224729Z",
     "iopub.status.idle": "2022-05-06T15:40:41.304405Z",
     "shell.execute_reply": "2022-05-06T15:40:41.303606Z",
     "shell.execute_reply.started": "2022-05-06T15:40:41.225198Z"
    }
   },
   "outputs": [],
   "source": [
    "def beam_decoder_util(model,input_sequences,max_decoder_seq_length,B=1,target_sequences=None,start_char=0,batch_size=64,attention=False):\n",
    "    '''\n",
    "    Function to do inference on the model using beam decoder.\n",
    "    Arguments :\n",
    "        model -- (Keras model object) training model\n",
    "        input_sequences -- (numpy ndarray of size : (None, timesteps)) input to encoder\n",
    "        max_decoder_seq_length -- (int) Number of timesteps to infer in decoder\n",
    "        B -- (int, default : 1) beam width of beam decoder\n",
    "        target_sequences -- (numpy ndarray of size : (None, timesteps, num_decoder_tokens), deault : None) expected target.\n",
    "                       If None, cross entropy errors won't be calculated.\n",
    "        start_char -- (int, default : 0) Encoding integer for ' '(start char)\n",
    "        batch_size -- (int, default : 64) batch_size sent to Keras predict\n",
    "    Returns :\n",
    "        final_outputs -- (numpy ndarray of size : (None, B, timesteps)) top B output sequences\n",
    "        final_errors -- (numpy ndarray of size : (None, B)) cross entropy errors for top B output (All zeros if target_seqs == None)\n",
    "        states_values -- (numpy ndarray of size : (, None, timesteps, hidden_layer_size))  hidden states of decoder\n",
    "        final_attn_scores -- (numpy ndarray of size : (None, B, decoder_timesteps(max_decoder_seq_length), encoder_timesteps(max_encoder_seq_length))) attention to all encoder timesteps for a decoder timestep \n",
    "    '''\n",
    "     # Generating output from encoder\n",
    "    if attention:\n",
    "        encoder_model,decoder_model,num_enc_layers,num_dec_layers=make_inference_model_attn(model)\n",
    "    else:\n",
    "        encoder_model,decoder_model,num_enc_layers,num_dec_layers=make_inference_model(model)\n",
    "    encoder_output = encoder_model.predict(input_sequences,batch_size=batch_size)\n",
    "    encoder_output = encoder_output if type(encoder_output) is list else [encoder_output]\n",
    "     # Number of input samples in the data passed\n",
    "    num_samples = input_sequences.shape[0]\n",
    "    # Top B output sequences for each input \n",
    "    outputs_fn = np.zeros((num_samples,B,max_decoder_seq_length),dtype=np.int32)\n",
    "      # Errors for top B output sequences for each input\n",
    "    errors_fn = np.zeros((num_samples,B))\n",
    "    \n",
    "    # decoder input sequence for 1 timestep (for all samples). Initially one choice only there\n",
    "    decoder_b_inputs = np.zeros((num_samples,1,1))\n",
    "    # Populate the input sequence with the start character at the 1st timestep\n",
    "    decoder_b_inputs[:, :, 0] = start_char\n",
    "    \n",
    "    # (log(probability) sequence, decoder output sequence) pairs for all choices and all samples. Probability starts with log(1) = 0\n",
    "    decoder_b_out = [[(0, [])] for t in range(num_samples)]\n",
    "    # Categorical cross entropy error in the sequence for all choice and all samples\n",
    "    errors = [[0] for t in range(num_samples)]\n",
    "    # Output states from decoder for all choices, and all samples\n",
    "    states = [encoder_output*num_dec_layers]\n",
    "    if attention:\n",
    "        # Attention weights output\n",
    "        attn_b_scores = [[None] for t in range(num_samples)]\n",
    "    # Sampling loop\n",
    "    for idx in range(max_decoder_seq_length):\n",
    "        # Storing respective data for all possibilities\n",
    "        all_b_beams = [[] for t in range(num_samples)]\n",
    "        all_decoder_states = [[] for t in range(num_samples)]\n",
    "        all_errors = [[] for t in range(num_samples)]\n",
    "        if attention:\n",
    "            all_attn_scores = [[] for t in range(num_samples)]\n",
    "        for b in range(len(decoder_b_out[0])):\n",
    "            if attention:\n",
    "                attn_scores,decoder_output, *decoder_states = decoder_model.predict([input_sequences,decoder_b_inputs[:,b]] + states[b],batch_size=batch_size)\n",
    "            else:\n",
    "                decoder_output, *decoder_states = decoder_model.predict([decoder_b_inputs[:,b]] + states[b],batch_size=batch_size)\n",
    "            # Top B scores\n",
    "            top_b = np.argsort(decoder_output[:,-1,:],axis=-1)[:,-B:]\n",
    "            for n in range(num_samples):\n",
    "                all_b_beams[n]+= [(decoder_b_out[n][b][0] + np.log(decoder_output[n, -1, top_b[n][i]]),decoder_b_out[n][b][1] + [top_b[n][i]]) for i in range(B)]\n",
    "                if attention:\n",
    "                    all_attn_scores[n] += [attn_scores[n]]*B if attn_b_scores[n][b] is None else [np.concatenate((attn_b_scores[n][b],attn_scores[n]),axis=0)]*B\n",
    "                if target_sequences is not None:\n",
    "                    all_errors[n] += [errors[n][b] - np.log(decoder_output[n,-1,target_sequences[n,idx]])]*B\n",
    "                all_decoder_states[n] += [[decoder_state[n:n+1] for decoder_state in decoder_states]] * B\n",
    "        # Sort and choose top B with max probabilities\n",
    "        sorted_index = list(range(len(all_b_beams[0])))\n",
    "        sorted_index = [sorted(sorted_index,key = lambda ix: all_b_beams[n][ix][0])[-B:][::-1] for n in range(num_samples)]\n",
    "        # Choose the top B decoder output sequences till now\n",
    "        decoder_b_out = [[all_b_beams[n][index] for index in sorted_index[n]] for n in range(num_samples)]\n",
    "        # Update the input sequence for next 1 timestep\n",
    "        decoder_b_inputs = np.array([[all_b_beams[n][index][1][-1] for index in sorted_index[n]] for n in range(num_samples)])\n",
    "        \n",
    "         \n",
    "        # Update states\n",
    "        states = [all_decoder_states[0][index] for index in sorted_index[0]]\n",
    "        \n",
    "        for n in range(1,num_samples):\n",
    "            states = [[np.concatenate((states[i][j],all_decoder_states[n][index][j])) for j in range(len(all_decoder_states[n][index]))] for i,index in  enumerate(sorted_index[n])]\n",
    "            \n",
    "        if attention:\n",
    "            # Update attention scores\n",
    "            attn_b_scores = [[all_attn_scores[n][index] for index in sorted_index[n]] for n in range(num_samples)]    \n",
    "        # Update errors \n",
    "        if target_sequences is not None:\n",
    "            errors = [[all_errors[n][index] for index in sorted_index[n]] for n in range(num_samples)]\n",
    "    \n",
    "           \n",
    "    outputs_fn = np.array([[decoder_b_out[n][i][1] for i in range(B)] for n in range(num_samples)])\n",
    "    if target_sequences is not None:\n",
    "        errors_fn = np.array(errors)/max_decoder_seq_length\n",
    "    if attention:\n",
    "        return outputs_fn,errors_fn,np.array(states),np.array(attn_b_scores)\n",
    "    return outputs_fn,errors_fn,np.array(states)\n",
    "\n",
    "def calc_metrics(b_outputs, target_sequences,token_index,reverse_char_index,b_errors=None,exact_word=True,display=False):\n",
    "        # Calculates the accuracy (and mean error if info provided) for the best of B possible output sequences\n",
    "    # target_sequencess -- Expected output (encoded sequence)\n",
    "    # b_outputs -- b choices of output sequences for each sample\n",
    "    matches = np.mean(b_outputs == target_sequences.reshape((target_sequences.shape[0],1,target_sequences.shape[1])),axis=-1)\n",
    "    best_b = np.argmax(matches,axis=-1)\n",
    "    best_index = (tuple(range(best_b.shape[0])),tuple(best_b))\n",
    "    accuracy = np.mean(matches[best_index])\n",
    "    b_predictions = list()\n",
    "    loss = None\n",
    "    if b_errors is not None:\n",
    "        loss = np.mean(b_errors[best_index])\n",
    "    if exact_word:\n",
    "        equal = [0] * b_outputs.shape[0]\n",
    "        true_out = num_to_word(target_sequences,token_index,reverse_char_index)\n",
    "        for b in range(b_outputs.shape[1]):\n",
    "            pred_out = num_to_word(b_outputs[:,b], token_index,reverse_char_index)\n",
    "            equal = [equal[i] or (pred_out[i] == true_out[i]) for i in range(b_outputs.shape[0])]\n",
    "            if display==True:\n",
    "                b_predictions.append(pred_out)\n",
    "        exact_accuracy = np.mean(equal)\n",
    "        if display==True:\n",
    "            return accuracy,exact_accuracy,loss,true_out,b_predictions\n",
    "        return accuracy,exact_accuracy,loss\n",
    "    return accuracy,loss\n",
    "\n",
    "def beam_decoder(model,input_sequences,target_sequences_onehot,max_decoder_seq_length,token_index,reverse_char_index,B=1,model_batch_size=64,infer_batch_size=512,exact_word=True,attention=False,return_outputs=False,return_states=False,return_attention=False,display=False):\n",
    "        '''\n",
    "    Function to calculate/capture character-wise accuracy, exact-word-match accuracy, and loss for the seq2seq model using a beam decoder.\n",
    "    Arguments :\n",
    "        model -- (Keras model object) model used for training\n",
    "        input_sequences -- (numpy ndarray of size : (None, timesteps)) input to encoder (where characters are encoded as integers)\n",
    "        target_sequences -- (numpy ndarray of size : (None, timesteps, num_decoder_tokens)) expected target in onehot format\n",
    "        max_decoder_seq_length -- (int) Number of timesteps to infer in decoder\n",
    "        token_index -- (dict) target character encoding\n",
    "        reverse_char_index -- (dict) target character decoding\n",
    "        B -- (int, default : 1) beam width to be used in beam decoder\n",
    "        attention -- (bool, defualt : False) whether the model has attention or not\n",
    "        model_batch_size -- (int, default : 64) batch size to be used while evaluating model using Keras\n",
    "        infer_batch_size -- (int, default : 512) number of samples to be sent to beam_decoder_infer() at a time (to avoid RAM memory overshoot problems).\n",
    "                            We have set the default model_batch_size and infer_batch_size such that it takes the least time to run and runs without problems.\n",
    "        exact_word -- (bool, default : True) whether or not exact_accuracy has (If True, will be returned as the next argument after accuracy)\n",
    "        return_outputs -- (bool, default : True) whether or not the outputs predicted need to be returned\n",
    "        return_states -- (bool, default : True) whether or not the decoder hidden states need to be returned (for further training, another sequential model addition, etc)\n",
    "        return_attn_scores -- (bool, default : True) whether or not the attention scores need to be returned\n",
    "    Returns :\n",
    "        accuracy -- (float) the character-wise match accuracy (as calculated by Keras fit)\n",
    "        (If exact_word is True) exact_accuracy -- (float) the exact word match accuracy\n",
    "        loss -- (float) the cross-entropy loss for the top B predictions\n",
    "        (If return_outputs is True) b_outputs -- (numpy ndarray of size : (None, B, timesteps)) top B output sequences\n",
    "        (If return_states is True) b_states -- (numpy ndarray of size : (B, None, timesteps, hidden_layer_size))  hidden states of decoder\n",
    "        (If return_attn_scores is True) b_attn_scores -- (numpy ndarray of size : (None, B, decoder_timesteps, encoder_timesteps)) attention scores\n",
    "    '''\n",
    "    target_sequences = np.argmax(target_sequences_onehot,axis=-1)\n",
    "    if attention:\n",
    "        b_outputs,b_errors,b_states,b_attention=None,None,None,None\n",
    "    else:\n",
    "        b_outputs,b_errors,b_states=None,None,None\n",
    "    for i in range(0,input_sequences.shape[0],infer_batch_size):\n",
    "        if attention:\n",
    "            tmp_b_outputs,tmp_b_errors,tmp_b_states,tmp_b_attention = beam_decoder_util(model,input_sequences[i:i+infer_batch_size],max_decoder_seq_length,B,target_sequences[i:i+infer_batch_size],token_index['\\t'],model_batch_size,attention=True)\n",
    "        else:\n",
    "            tmp_b_outputs,tmp_b_errors,tmp_b_states = beam_decoder_util(model,input_sequences[i:i+infer_batch_size],max_decoder_seq_length,B,target_sequences[i:i+infer_batch_size],token_index['\\t'],model_batch_size,attention=False)\n",
    "        if b_errors is None:\n",
    "            if attention:\n",
    "                b_outputs,b_errors,b_states,b_attention = tmp_b_outputs,tmp_b_errors,tmp_b_states,tmp_b_attention\n",
    "            else:\n",
    "                b_outputs,b_errors,b_states = tmp_b_outputs,tmp_b_errors,tmp_b_states\n",
    "        else:\n",
    "            if attention:\n",
    "                b_outputs = np.concatenate((b_outputs,tmp_b_outputs))\n",
    "                b_errors = np.concatenate((b_errors,tmp_b_errors))\n",
    "                b_states = np.concatenate((b_states,tmp_b_states),axis=2)\n",
    "                b_attention = np.concatenate((b_attention,tmp_b_attention))\n",
    "            else:\n",
    "                b_outputs = np.concatenate((b_outputs,tmp_b_outputs))\n",
    "                b_errors = np.concatenate((b_errors,tmp_b_errors))\n",
    "                b_states = np.concatenate((b_states,tmp_b_states),axis=2) \n",
    "    return_elements = []\n",
    "    if return_outputs:\n",
    "        return_elements += [b_outputs]\n",
    "    if return_states:\n",
    "        return_elements += [b_states]\n",
    "    if return_attention and attention:\n",
    "        return_elements += [b_attention]\n",
    "    if len(return_elements) > 0:\n",
    "        return calc_metrics(b_outputs,target_sequences,token_index,reverse_char_index,b_errors,exact_word,display) + tuple(return_elements)\n",
    "    return calc_metrics(b_outputs,target_sequences,target_token_index,reverse_char_index,b_errors,exact_word,display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T15:40:41.306152Z",
     "iopub.status.busy": "2022-05-06T15:40:41.305749Z",
     "iopub.status.idle": "2022-05-06T15:40:41.319167Z",
     "shell.execute_reply": "2022-05-06T15:40:41.31841Z",
     "shell.execute_reply.started": "2022-05-06T15:40:41.306116Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def levenshtein(s1, s2):\n",
    "    # Function to calculate levenshtein distance between two sequences usign Dynamic Programming\n",
    "    m, n = len(s1)+1, len(s2)+1\n",
    "    # Initialisation\n",
    "    dp = np.zeros((m, n))\n",
    "    # Base case\n",
    "    dp[0,1:] = np.arange(1,n)\n",
    "    dp[1:,0] = np.arange(1,m)\n",
    "\n",
    "    # Recursion\n",
    "    for i in range(1,m):\n",
    "        for j in range(1,n):\n",
    "            if s1[i-1] == s2[j-1]:\n",
    "                dp[i,j] = min(dp[i-1,j-1], dp[i-1,j]+1, dp[i,j-1]+1)\n",
    "            else:\n",
    "                dp[i,j] = min(dp[i,j-1], dp[i-1,j], dp[i-1,j-1]) + 1\n",
    "    \n",
    "    return dp[m-1,n-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T15:40:41.32191Z",
     "iopub.status.busy": "2022-05-06T15:40:41.321601Z",
     "iopub.status.idle": "2022-05-06T15:40:41.350491Z",
     "shell.execute_reply": "2022-05-06T15:40:41.349752Z",
     "shell.execute_reply.started": "2022-05-06T15:40:41.321872Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def test_model(run_id,test_encoder_input,test_decoder_target,max_decoder_seq_length,target_token_index,reverse_target_char_index,attention=False,save_pred=False,test_input_texts=None):\n",
    "    '''\n",
    "    Function to evaluate the model metrics on test data and optionally save the predictions.\n",
    "    Arguments :\n",
    "        run_id -- (string) WANDB run ID for the trained model\n",
    "        test_encoder_input -- (numpy ndarray of size : (None, timesteps)) input to encoder (where characters are encoded as integers)\n",
    "        test_decoder_target -- (numpy ndarray of size : (None, timesteps, decoder_vocab_size)) expected target in onehot format\n",
    "        max_decoder_seq_length -- (int) number of timesteps in the decoder\n",
    "        target_token_index -- (dict) target character encoding\n",
    "        reverse_target_char_index -- (dict) target character decoding\n",
    "        attention -- (bool, default : False) whether or not the model uses attention\n",
    "        save_pred -- (bool, default : False) whether or not to save the predictions in a csv file\n",
    "        test_input_texts -- (list of string : (no_samples, input word), default : None) input as words (needed while saving predictions to file alone)\n",
    "    Returns :\n",
    "        acc -- (float) character-wise match accuracy\n",
    "        exact_B_acc -- (float) exact word match accuracy using the beam width for the model\n",
    "        exact_acc -- (float) exact word match accuracy using the first prediction (which is equivalent to beam width = 1)\n",
    "        loss -- (float) loss value\n",
    "        true_out -- (list of string : (no_samples, word)) true output  \n",
    "        pred_out -- (2D list of string : (no_samples, B, word)) predicted output\n",
    "        pred_scores -- (2D list : (no_samples, B)) levenshtein distance of prediction to true output\n",
    "        (If attention is True) attn_scores -- (numpy ndarray of size : (None, B, decoder_timesteps, encoder_timesteps)) attention scores\n",
    "        model -- (Keras model object) the model obtained from the run\n",
    "    '''\n",
    "    api = wandb.Api()\n",
    "    r = api.run('dlstack/cs6910_assignment_3_attention/'+run_id) if attention else api.run('dlstack/cs6910_assignment_3/'+run_id)\n",
    "    config = r.config['_items'] if '_items' in r.config.keys() else r.config\n",
    "    model_file = r.file('model-best.h5').download(replace=True)\n",
    "    if attention:\n",
    "        model = keras.models.load_model(model_file.name,custom_objects={'Attention':Attention})\n",
    "    else:\n",
    "        model = keras.models.load_model(model_file.name)\n",
    "    \n",
    "    num_samples,batch_size,B = test_encoder_input.shape[0],config['batch_size'],config['beam_width']\n",
    "    if attention:\n",
    "        acc, exact_B_acc, loss, outputs,attention_scores = beam_decoder(model, test_encoder_input, test_decoder_target, max_decoder_seq_length, \n",
    "                                                                target_token_index, reverse_target_char_index,B, batch_size,attention=True,\n",
    "                                                                return_outputs=True,return_attention=True)\n",
    "    else:\n",
    "        acc, exact_B_acc, loss, outputs = beam_decoder(model, test_encoder_input, test_decoder_target, max_decoder_seq_length, \n",
    "                                                                target_token_index, reverse_target_char_index,B, batch_size,attention=False,\n",
    "                                                                return_outputs=True,return_attention=False)    \n",
    "    print(f'Test accuracy (using exact word match with beam width = {B}) : {exact_B_acc*100:.2f}%')\n",
    "    \n",
    "    test_target = np.argmax(test_decoder_target, axis=-1)\n",
    "    true_out = num_to_word(test_target, target_token_index, reverse_target_char_index)\n",
    "    pred_out = [[] for t in range(num_samples)]\n",
    "    pred_scores = [[] for t in range(num_samples)]\n",
    "    for b in range(B):\n",
    "        pred = num_to_word(outputs[:,b], target_token_index, reverse_target_char_index)\n",
    "        pred_out = [pred_out[n] + [pred[n]] for n in range(num_samples)]\n",
    "        pred_scores = [pred_scores[n] + [levenshtein(pred[n], true_out[n])] for n in range(num_samples)]\n",
    "\n",
    "    equal = [pred_out[n][0] == true_out[n] for n in range(num_samples)]\n",
    "    exact_acc = np.mean(equal)\n",
    "\n",
    "    print(f'Test accuracy (using exact word match of the first prediction) : {exact_acc*100:.2f}%')\n",
    "    print('\\n')\n",
    "    save_pred = True\n",
    "    if save_pred:\n",
    "        # We write the input and top K outputs in decreasing order of probabilities to the file\n",
    "        pred_file_name = 'predictions_attention.csv' if attention else 'predictions.csv'\n",
    "        with open(pred_file_name, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Input\"] + [\"Prediction_\"+str(b+1) for b in range(B)])\n",
    "            for n in range(num_samples):\n",
    "                writer.writerow([test_input_texts[n]] + [pred_out[n][b] for b in range(B)])\n",
    "    if attention:\n",
    "        return acc, exact_B_acc, exact_acc, loss, true_out, pred_out, pred_scores, attention_scores, model\n",
    "    return acc, exact_B_acc, exact_acc, loss, true_out, pred_out, pred_scores, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T15:40:41.357056Z",
     "iopub.status.busy": "2022-05-06T15:40:41.353809Z",
     "iopub.status.idle": "2022-05-06T15:40:41.370897Z",
     "shell.execute_reply": "2022-05-06T15:40:41.369878Z",
     "shell.execute_reply.started": "2022-05-06T15:40:41.357012Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_clr(value, cmap=None):\n",
    "    # Function to get appropriate color for a value between 0 and 1 from the default blue to red hard-coded colors or a matplotlib cmap \n",
    "    colors = ['#85c2e1', '#89c4e2', '#95cae5', '#99cce6', '#a1d0e8',\n",
    "        '#b2d9ec', '#baddee', '#c2e1f0', '#eff7fb', '#f9e8e8',\n",
    "        '#f9e8e8', '#f9d4d4', '#f9bdbd', '#f8a8a8', '#f68f8f',\n",
    "        '#f47676', '#f45f5f', '#f34343', '#f33b3b', '#f42e2e']\n",
    "    if cmap is not None:\n",
    "        rgba = matplotlib.cm.get_cmap(cmap)(value,alpha=None,bytes=True)\n",
    "        return 'rgb'+str(rgba[:-1])\n",
    "    value = min(int((value * 100) / 5), 19)\n",
    "    return colors[value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T15:40:41.373419Z",
     "iopub.status.busy": "2022-05-06T15:40:41.372778Z",
     "iopub.status.idle": "2022-05-06T15:40:41.394967Z",
     "shell.execute_reply": "2022-05-06T15:40:41.394018Z",
     "shell.execute_reply.started": "2022-05-06T15:40:41.37338Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def print_samples(inputs, true_output, pred_output, pred_scores, attention=False,wandb_log=False,random_seq=None,cmap=None):\n",
    "    '''\n",
    "    Function to print sample outputs in a neat format\n",
    "    Arguments :\n",
    "        input -- input words\n",
    "        true_output -- true output as words\n",
    "        pred_output -- B predicted output words\n",
    "        pred_scores -- levenshtein distance for the predictions to the true output\n",
    "        attention -- (bool, default : False) whether or not the model uses attention\n",
    "        wandb_log -- (bool, default: False) whether or not to log the predictions to wandb as html file\n",
    "        random_seq -- list of indices from the dataset passed for which the sample outputs are to be printed (If None, random 10 samples will be chosen)\n",
    "    Returns :\n",
    "        random_seq -- the list of indices for which sample outputs are printed\n",
    "    '''\n",
    "    num_samples = len(true_output)\n",
    "    if random_seq is None:\n",
    "        random_seq = random.sample(range(num_samples),10)\n",
    "    if attention:\n",
    "        headline = '-'*20 + f' Top {len(pred_scores[0])} predictions with attention in decreasing order of probabilities for 10 random samples ' + '-'*20\n",
    "    else:\n",
    "        headline = '-'*20 + f' Top {len(pred_scores[0])} predictions in decreasing order of probabilities for 10 random samples ' + '-'*20\n",
    "    print(headline)\n",
    "    print('')\n",
    "    html_body=''\n",
    "    for i in random_seq:\n",
    "        K = len(pred_scores[i])\n",
    "        html_str = '''\n",
    "        <table style=\"border:2px solid black; border-collapse:collapse\">\n",
    "        <caption> <strong>INPUT :</strong> {} &emsp; | &emsp; <strong> TRUE OUTPUT : </strong> {} </caption>\n",
    "        <tr>\n",
    "        <th scope=\"row\" style=\"border:1px solid black;padding:10px;text-align:left\"> Top {} Predictions </th>\n",
    "        '''.format(inputs[i], true_output[i], K)\n",
    "        for k in range(K):\n",
    "            html_str += '''\n",
    "            <td style=\"color:#000;background-color:{};border:1px solid black;padding:10px\"> {} </td>\n",
    "            '''.format(get_clr(pred_scores[i][k]/5,cmap), pred_output[i][k])\n",
    "        html_str += '''\n",
    "        </tr>\n",
    "        <tr>\n",
    "        <th scope=\"row\" style=\"border:1px solid black;padding:10px;text-align:left\"> Levenshtein distance (to true output) &emsp; </th>\n",
    "        '''\n",
    "        for k in range(K):\n",
    "            html_str += '''\n",
    "            <td style=\"border:1px solid black;padding:10px\"> {} </td>\n",
    "            '''.format(pred_scores[i][k])\n",
    "        html_str += '''\n",
    "        </tr>\n",
    "        </table>\n",
    "        '''\n",
    "        html_body+=html_str+'<br>'\n",
    "        display(html_print(html_str))\n",
    "    title = 'with attention' if attention else ''\n",
    "    html_prefix = '''\n",
    "        <!DOCTYPE html>\n",
    "        <html lang=\"en\">\n",
    "        <head>\n",
    "            <meta charset=\"UTF-8\">\n",
    "            <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\n",
    "            <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "            <title>Predictions {}</title>\n",
    "        </head>\n",
    "        <body>\n",
    "        <h4>{}</h4>\n",
    "        '''.format(title,headline)\n",
    "    html_suffix = '''\n",
    "        </body>\n",
    "        </html>\n",
    "        '''\n",
    "    html_out = html_prefix+html_body+html_suffix\n",
    "    fname = \"predictions_attn.html\" if attention else \"predictions.html\"\n",
    "    Func = open(fname,\"w\")\n",
    "    Func.write(html_out)\n",
    "    Func.close()\n",
    "    if wandb_log:\n",
    "        run = wandb.init(project=\"cs6910_assignment3_viz\", entity=\"dlstack\", reinit=True)\n",
    "        wandb.run.name = 'best-model-attn-predictions' if attention else 'best-model-predictions'\n",
    "        wandb.log({f'Predictions {title}':wandb.Html(open(fname))})\n",
    "        run.finish()\n",
    "    print('\\n\\n')\n",
    "    \n",
    "    return random_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T15:40:41.397539Z",
     "iopub.status.busy": "2022-05-06T15:40:41.396997Z",
     "iopub.status.idle": "2022-05-06T15:44:28.919256Z",
     "shell.execute_reply": "2022-05-06T15:44:28.917469Z",
     "shell.execute_reply.started": "2022-05-06T15:40:41.397498Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Best run id of wandb without attention\n",
    "best_run_id = \"nrd2ctiz\"\n",
    "# acc, exact_B_acc, exact_acc, loss, true_out, pred_out, pred_scores,model = test_model(best_run_id,test_encoder_input,test_decoder_target,max_decoder_seq_length,target_token_index,reverse_target_char_index,save_pred=True,test_input_texts=test_input_texts)\n",
    "acc, exact_B_acc, exact_acc, loss, true_out, pred_out, pred_scores,model = test_model(best_run_id,test_encoder_input,test_decoder_target,max_decoder_seq_length,target_token_index,reverse_target_char_index,save_pred=False,test_input_texts=test_input_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T15:44:28.922444Z",
     "iopub.status.busy": "2022-05-06T15:44:28.921883Z",
     "iopub.status.idle": "2022-05-06T15:44:28.951594Z",
     "shell.execute_reply": "2022-05-06T15:44:28.950839Z",
     "shell.execute_reply.started": "2022-05-06T15:44:28.922399Z"
    }
   },
   "outputs": [],
   "source": [
    "# random_samples = print_samples(test_input_texts,true_out,pred_out,pred_scores,wandb_log=True)\n",
    "random_samples = print_samples(test_input_texts,true_out,pred_out,pred_scores,wandb_log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T15:44:28.953564Z",
     "iopub.status.busy": "2022-05-06T15:44:28.953069Z",
     "iopub.status.idle": "2022-05-06T15:44:30.0291Z",
     "shell.execute_reply": "2022-05-06T15:44:30.027384Z",
     "shell.execute_reply.started": "2022-05-06T15:44:28.953521Z"
    }
   },
   "outputs": [],
   "source": [
    "#For plotting model without attention using keras.utils\n",
    "plot_model(model,show_shapes=True,to_file='best_model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T15:44:30.030681Z",
     "iopub.status.busy": "2022-05-06T15:44:30.030446Z",
     "iopub.status.idle": "2022-05-06T15:48:05.648598Z",
     "shell.execute_reply": "2022-05-06T15:48:05.647738Z",
     "shell.execute_reply.started": "2022-05-06T15:44:30.030625Z"
    }
   },
   "outputs": [],
   "source": [
    "#Best run id of wandb with attention\n",
    "best_attn_run_id = \"nkg32le7\"\n",
    "# acc_a, exact_B_acc_a, exact_acc_a, loss_a, true_out_a, pred_out_a, pred_scores_a,attention_scores,model_a = test_model(best_attn_run_id,test_encoder_input,test_decoder_target,max_decoder_seq_length,target_token_index,reverse_target_char_index,attention=True,save_pred=True,test_input_texts=test_input_texts)\n",
    "acc_a, exact_B_acc_a, exact_acc_a, loss_a, true_out_a, pred_out_a, pred_scores_a,attention_scores,model_a = test_model(best_attn_run_id,test_encoder_input,test_decoder_target,max_decoder_seq_length,target_token_index,reverse_target_char_index,attention=True,save_pred=False,test_input_texts=test_input_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T15:48:05.650977Z",
     "iopub.status.busy": "2022-05-06T15:48:05.650402Z",
     "iopub.status.idle": "2022-05-06T15:48:05.680495Z",
     "shell.execute_reply": "2022-05-06T15:48:05.679696Z",
     "shell.execute_reply.started": "2022-05-06T15:48:05.650934Z"
    }
   },
   "outputs": [],
   "source": [
    "# random_samples_attn = print_samples(test_input_texts,true_out_a,pred_out_a,pred_scores_a,attention=True,wandb_log=True,random_seq=random_samples)\n",
    "random_samples_attn = print_samples(test_input_texts,true_out_a,pred_out_a,pred_scores_a,attention=True,wandb_log=False,random_seq=random_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T15:48:05.682949Z",
     "iopub.status.busy": "2022-05-06T15:48:05.682379Z",
     "iopub.status.idle": "2022-05-06T15:48:05.907406Z",
     "shell.execute_reply": "2022-05-06T15:48:05.906494Z",
     "shell.execute_reply.started": "2022-05-06T15:48:05.682906Z"
    }
   },
   "outputs": [],
   "source": [
    "#For plotting model with attention using keras.utils\n",
    "plot_model(model_a,show_shapes=True,to_file='best_model_attn.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T15:48:05.911155Z",
     "iopub.status.busy": "2022-05-06T15:48:05.91002Z",
     "iopub.status.idle": "2022-05-06T15:48:07.933447Z",
     "shell.execute_reply": "2022-05-06T15:48:07.932551Z",
     "shell.execute_reply.started": "2022-05-06T15:48:05.911107Z"
    }
   },
   "outputs": [],
   "source": [
    "#Download font so that matplotlib can display bengali characters\n",
    "filename = 'Hind_Siliguri'\n",
    "url = 'https://fonts.google.com/download?family=Hind%20Siliguri'\n",
    "if not os.path.exists(filename+'.zip') and not os.path.exists(filename):\n",
    "    filename_zip = wget.download(url)\n",
    "    with ZipFile(filename_zip, 'r') as z:\n",
    "        z.printdir()\n",
    "        print('\\nExtracting files ....')\n",
    "        z.extractall()\n",
    "        print('Done')\n",
    "    os.remove(filename_zip)\n",
    "elif not os.path.exists(filename):\n",
    "    filename_zip = filename + '.zip'\n",
    "    with ZipFile(filename_zip, 'r') as z:\n",
    "        z.printdir()\n",
    "        print('\\nExtracting files ....')\n",
    "        z.extractall()\n",
    "        print('Done')\n",
    "    os.remove(filename_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T15:48:07.935229Z",
     "iopub.status.busy": "2022-05-06T15:48:07.934943Z",
     "iopub.status.idle": "2022-05-06T15:48:07.949895Z",
     "shell.execute_reply": "2022-05-06T15:48:07.948964Z",
     "shell.execute_reply.started": "2022-05-06T15:48:07.9352Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_heatmaps(inputs,pred_out,pred_scores,attn_scores,wandb_log=False,random_seq=None,cmap='magma'):\n",
    "    '''\n",
    "    Function to generate attention heatmaps for 9 samples in a 3 x 3 grid\n",
    "    Arguments :\n",
    "        inputs -- input words\n",
    "        pred_out -- B predicted output words\n",
    "        pred_scores -- levenshtein distance for the predictions to the true output\n",
    "        attn_scores -- attention scores\n",
    "        wandb_log -- (bool, default : False) whether or not to log the image generated to WANDB\n",
    "        rand_seq -- list of indices from the dataset passed for which the sample outputs are to be printed (If None, random 9 samples will be chosen)\n",
    "                    (The length of list passed should be >= 9)\n",
    "    Returns :\n",
    "        rand_seq -- the list of indices for which sample outputs are printed\n",
    "    '''\n",
    "    num_samples = len(pred_out)\n",
    "    if random_seq is None:\n",
    "        random_seq = random.sample(range(num_samples),9)\n",
    "    random_seq = random_seq[:9]\n",
    "    \n",
    "    plt.close('all')\n",
    "    fig = plt.figure(figsize=(15,15))\n",
    "    fig,axes = plt.subplots(3,3, figsize=(15, 15),constrained_layout=True)\n",
    "    plt.suptitle('Attention Heatmaps',fontsize='x-large')\n",
    "    for i,ax in zip(random_seq,axes.flat):\n",
    "        K = len(pred_scores[i])\n",
    "        k = np.argmin(pred_scores[i])\n",
    "        X = attn_scores[i,k,:len(pred_out[i][k])+1,:len(inputs[i])+1].T\n",
    "        im = ax.imshow(X,vmin=0,vmax=1,cmap=cmap)\n",
    "        ax.set_xticks(range(len(pred_out[i][k])+1))\n",
    "        ax.set_xticklabels(list(pred_out[i][k])+['<end>'],fontproperties=FontProperties(fname='HindSiliguri-Medium.ttf'))\n",
    "        ax.set_yticks(range(len(inputs[i])+1))\n",
    "        ax.set_yticklabels(list(inputs[i])+['<end>'])\n",
    "        ax.set_ylabel(f'Encoder Input:{inputs[i]}')\n",
    "        ax.set_xlabel(f'Decoder Output:{pred_out[i][k]}',fontproperties=FontProperties(fname='HindSiliguri-Medium.ttf'))\n",
    "        ax.set_title(str(i) + r'$^{th}$ example of Test Set')\n",
    "        ax.set_aspect(\"equal\")\n",
    "        ax.grid(False)\n",
    "    fig.colorbar(im,ax=axes.ravel().tolist(),shrink=0.7)\n",
    "    if wandb_log:\n",
    "        run = wandb.init(project=\"cs6910_assignment3_viz\", entity=\"dlstack\", reinit=True)\n",
    "        wandb.run.name = 'attn_heat_maps'\n",
    "        wandb.log({'attention_heatmaps':fig})\n",
    "        run.finish()\n",
    "    plt.show()\n",
    "    return random_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T15:48:07.952051Z",
     "iopub.status.busy": "2022-05-06T15:48:07.951531Z",
     "iopub.status.idle": "2022-05-06T15:48:10.354865Z",
     "shell.execute_reply": "2022-05-06T15:48:10.35407Z",
     "shell.execute_reply.started": "2022-05-06T15:48:07.952007Z"
    }
   },
   "outputs": [],
   "source": [
    "_ = plot_heatmaps(test_input_texts,pred_out_a,pred_scores_a,attention_scores,wandb_log=False,random_seq=random_samples_attn)\n",
    "# _ = plot_heatmaps(test_input_texts,pred_out_a,pred_scores_a,attention_scores,wandb_log=True,random_seq=random_samples_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T15:48:10.357011Z",
     "iopub.status.busy": "2022-05-06T15:48:10.35632Z",
     "iopub.status.idle": "2022-05-06T15:48:10.363242Z",
     "shell.execute_reply": "2022-05-06T15:48:10.362235Z",
     "shell.execute_reply.started": "2022-05-06T15:48:10.356966Z"
    }
   },
   "outputs": [],
   "source": [
    "def cstr(s, color=None):\n",
    "      # Function to get text html element\n",
    "    if color is None:\n",
    "        return '''<text style=\"padding:2px; color:#C0C0C0\"> {} </text>'''.format(s)\n",
    "    return '''<text style=\"color:#000;background-color:{}; padding:2px; color:#FF6699\"> {} </text>'''.format(color, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T15:48:10.365366Z",
     "iopub.status.busy": "2022-05-06T15:48:10.365061Z",
     "iopub.status.idle": "2022-05-06T15:48:10.380864Z",
     "shell.execute_reply": "2022-05-06T15:48:10.379754Z",
     "shell.execute_reply.started": "2022-05-06T15:48:10.36532Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_connectivity(inputs, pred_out, pred_scores, attn_scores, dec_char_ind=0):\n",
    "    '''\n",
    "    Function to visualize attention for one index of decoder output of one sample\n",
    "    Arguments :\n",
    "        input -- sample input word\n",
    "        pred_out -- K predicted output words for the sample\n",
    "        pred_scores -- levenshtein distance for the predictions to the true output\n",
    "        attn_scores -- attention scores\n",
    "        dec_char_ind -- (default : 0) index of the character in decoder for which the visuzalization is to be done\n",
    "    Returns :\n",
    "        -- None --\n",
    "    '''\n",
    "    K = len(pred_scores)\n",
    "    print('-'*20 + f' Visualizing attention for Top {K} predictions (in decreasing order of probabilities) ' + '-'*20)\n",
    "    print('')\n",
    "    html_str = '''\n",
    "    <table style=\"border:2px solid black; border-collapse:collapse; font-size:1.5em\">\n",
    "    <caption> <strong>INPUT : </strong> {} </caption>\n",
    "    <tr>\n",
    "    <th style=\"border:1px solid black;padding:10px;text-align:center\"> Character in Prediction Focussed </th>\n",
    "    <th style=\"border:1px solid black;padding:10px;text-align:center\"> Attention Visualization </th>\n",
    "    </tr>\n",
    "    '''.format(inputs)\n",
    "    for k in range(K):  \n",
    "        char = pred_out[k][dec_char_ind] if dec_char_ind < len(pred_out[k]) else '&lt end &gt' if dec_char_ind == len(pred_out[k]) else '&lt blank &gt'\n",
    "        middle_char = pred_out[k][dec_char_ind] if dec_char_ind < len(pred_out[k]) else ''\n",
    "        end_str = pred_out[k][dec_char_ind+1:] if dec_char_ind < len(pred_out[k])-1 else ''\n",
    "        html_str += '''\n",
    "        <tr>\n",
    "        <td style=\"border:1px solid black;padding:10px;text-align:center\"> character at index {} of {}<span style=\"color: #FF1493\">{}</span>{} <br/> ({}) </td>\n",
    "        <td style=\"border:1px solid black;padding:10px;text-align:center\">\n",
    "        '''.format(dec_char_ind, pred_out[k][:dec_char_ind], middle_char, end_str, char)\n",
    "        for i,c in enumerate(inputs):\n",
    "            html_str += '''\n",
    "            {}\n",
    "            '''.format(cstr(c, get_clr(attn_scores[k,dec_char_ind,i], 'YlGnBu')))\n",
    "        html_str += '''\n",
    "        </td>\n",
    "        </tr>\n",
    "        '''\n",
    "    html_str += '''\n",
    "    </table>\n",
    "    '''\n",
    "    display(html_print(html_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T15:48:10.384492Z",
     "iopub.status.busy": "2022-05-06T15:48:10.383963Z",
     "iopub.status.idle": "2022-05-06T15:48:10.393976Z",
     "shell.execute_reply": "2022-05-06T15:48:10.393114Z",
     "shell.execute_reply.started": "2022-05-06T15:48:10.38445Z"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_attention(sample_ind=0, dec_char_ind=0):\n",
    "    # Function to visualize the importance of encoder input characters to the (dec_char_ind)th character of the output,\n",
    "    # for the (sample_ind)th sample in the test data\n",
    "    print_connectivity(test_input_texts[sample_ind], pred_out_a[sample_ind], pred_scores_a[sample_ind], attention_scores[sample_ind], dec_char_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T15:48:10.396267Z",
     "iopub.status.busy": "2022-05-06T15:48:10.395747Z",
     "iopub.status.idle": "2022-05-06T15:48:10.483513Z",
     "shell.execute_reply": "2022-05-06T15:48:10.482717Z",
     "shell.execute_reply.started": "2022-05-06T15:48:10.396221Z"
    }
   },
   "outputs": [],
   "source": [
    "# Question 6 - visualizing attention\n",
    "@interact(sample_ind = IntSlider(min=0, max=len(test_input_texts)-1, step=1, value=10, layout=Layout(width='800px')))\n",
    "def f(sample_ind):\n",
    "    print(f'Input : {test_input_texts[sample_ind]}')\n",
    "    print(f'Top {len(pred_out_a[sample_ind])} predictions : ')\n",
    "    mx_len = 0\n",
    "    for pred in pred_out_a[sample_ind]:\n",
    "        print(pred)\n",
    "        mx_len = max(mx_len, len(pred))\n",
    "    \n",
    "    @interact(character_ind = IntSlider(min=0, max=mx_len-1, step=1, value=0))\n",
    "    def g(character_ind):\n",
    "        visualize_attention(sample_ind, character_ind)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
